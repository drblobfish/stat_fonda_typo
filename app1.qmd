# Algèbre linéaire {#sec-al}

## Multiplication matricielle

La pratique des régressions linéaires nécessite une certaine familiarité avec la multiplication des matrices. On rappelle que si $A$ est une matrice à $\ell$ lignes et $m$ colonnes, et que $B$ est une matrice à $m$ lignes et $n$ colonnes, alors il est possible de les multiplier entre elles. Il en résulte une matrice $AB$ avec $\ell$ lignes et $n$ colonnes, dont le terme $i,j$ est égal à 
$$\sum_{k=1}^m A_{i,k}B_{k,j}. $$
Ce terme peut aussi être vu comme $\langle A_{i, \cdot}, B_{\cdot, j}\rangle$, le produit scalaire entre la $i$-ème ligne de $A$ et la $j$-ème colonne de $B$. 

De façon générale, le produit scalaire entre deux vecteurs de même taille, $\langle x, y\rangle$, est donc égal à la multiplication matricielle entre le vecteur ligne $x^\top$ et le vecteur colonne $y$. 

Il est aussi possible de multiplier un vecteur ligne $x$ de taille $n$ et un vecteur colonne $y^\top$ de taille $m$, mais ici on n'a plus besoin que $n$ et $m$ soient égaux. Il en résulte une matrice de taille $n\times m$, 
$$xy^\top = [x_i y_j]_{\substack{i=1, \dotsc, n\\ j=1,\dotsc, m}}.$$
Si, comme tout à l'heure, $A$ est une matrice $\ell,n$ et $B$ une matrice $m,n$, notons $a_i$ les *colonnes* de $A$ (vecteurs colonnes) et $b_i$ les *lignes* de $B$ (vecteurs lignes). Alors, on peut écrire 
$$ AB = \sum_{i=1}^m a_i b_i. $$
En particulier, pour n'importe quelle matrice $X$ de taille $n,d$ dont les lignes sont $\bx_i$ (et donc, les colonnes de $X^\top$ sont les $\bx_i^\top$), alors on peut écrire
$$X^\top X = \sum_{i=1}^n \bx_i^\top \bx_i.$$

## Le théorème spectral

Grâce aux manipulations ci-dessus, le théorème de décomposition en vecteurs propres prend une forme légèrement différente. Ce théorème dit habituellement que toute matrice $M$ symétrique réelle peut s'écrire $UDU^\top$, avec $U$ la matrice de passage dans la base des vecteurs propres et $D = \mathrm{diag}(\lambda_i)$ la matrice diagonale des valeurs propres. C'est donc la même chose que l'énoncé suivant. 

:::{#thm-spectral}

Soit $M$ une matrice symétrique réelle. Il existe une base orthonormale de vecteurs $u_1, \dotsc, u_n$ et des nombres réels $\lambda_1, \dotsc
, \lambda_n$ tels que 
$$ M = \sum_{i=1}^n \lambda_i u_i u_i^\top.$$

:::

## Projections orthogonales

Soit $v$ un vecteur non nul de $\mathbb{R}^n$. L'espace vectoriel engendré par $v$ est l'ensemble $\mathscr{V}=\{tv : t \in \mathbb{R}\}$, et son orthogonal est l'hyperplan $\mathscr{V}^\perp = \{x : \langle x, v \rangle = 0\}$. Les résultats élémentaires d'algèbre linéaire disent que tout vecteur $x$ se décompose *de façon unique* sous la forme 
$$ x = y + z$$
avec $y$ dans $\mathscr{V}$ et $z$ dans $\mathscr{V}^\top$. En particulier, il existe un $t$ tel que $y = tv$. 

Considérons maintenant la matrice 
$$P = \frac{1}{|v|^2}vv^\top \in \mathscr{M}_{n,n}. $$
Appliquons cette matrice à $x$. Par linéarité, $Px = Px + Pz$. Calculons ces deux termes. 

1. $Pz = |v|^{-1}v v^\top z = |v|^{-1}v \langle v, z\rangle$. Comme $z$ est orthogonal à $v$, cela vaut 0. 
2. $Py = tPv$. Par définition de $P$, ceci est donc égal à  $t|v|^{-2}v v^\top v = t|v|^{-2}v |v|^2 = tv$, c'est-à-dire $y$. 
   
Nous avons montré plusieurs choses. D'abord, l'application qui à $x$ associe $y$ est effectivement linéaire, et une de ses matrices est $P$. On dit que $P$ est la matrice de projection sur $\mathscr{V}$. De même, comme $(I - P)x = y+z - y = z$, la matrice $I-P$ est la matrice de projection sur $\mathscr{V}^\perp$. 

Le cas d'un sous-espace vectoriel généré par *plusieurs* vecteurs $v_1, \dotsc, v_d$ linéairement indépendants se traite de la même façon. Soit $V = [v_1, \dotsc, v_d]$ la matrice $n \times d$ dont les colonnes sont les $v_i$. Tout à l'heure, $|v|^{-2}$ aurait pu s'écrire $(v^\top v)^{-1}$. L'analogue avec $V$ est donc naturellement $(V^\top V)^{-1}$, donnant naissance au théorème suivant. 

:::{#thm-projortho}

Soient $v_1, \dotsc, v_d$ des vecteurs non-colinéaires de $\mathbb{R}^n$, et soit $V = [v_1, \dotsc, v_d]$ la matrice $n \times d$ dont les colonnes sont les $v_i$. La matrice de taille $n\times n$
$$P_V = V (V^\top V)^{-1}V^\top $$
est la matrice de projection orthogonale sur le sous-espace $\mathscr{V}$ engendré par les $v_i$. De plus, la matrice $I - P_V$ est la matrice de projection orthogonale sur le sous-espace $\mathscr{V}^\perp$. 

:::

:::{.proof}
Si $x=y+z$ est la décomposition de $x$ en somme d'un élément $y\in\mathscr{V}$ et d'un élément $z \in \mathscr{V}^\perp$, alors $Px = Py + Pz$ et 
$$ Pz = V(V^\top V)^{-1}V^\top z.$$
Or, les $d$ lignes de $V^\top z$ sont les produits scalaires $\langle v_i, z\rangle$, qui sont tous nuls car $z$ est orthogonal à tous les $v_i$. Ainsi, $Pz = 0$. 

D'autre part, comme $y$ est dans l'espace engendré par les $v_i$, il s'écrit sous la forme $t_1 v_1 + \dotsc + t_d v_d$. Cela peut se récrire en disant que $y = V t$, où $t$ est le vecteur colonne des $t_i$. Mais alors, 
$$ Py = V(V^\top V)^{-1}V^\top V t = Vt = y.$$
On conclut comme dans le cas $d=1$ exposé ci-dessus. Il reste cependant un point de détail : nous devons nous assurer que $V^\top V$ est effectivement inversible ! C'est le cas, je le jure. 

:::


## Matrices positives {#sec-defpos}

Une matrice symétrique réelle est *positive* lorsque toutes ses valeurs propres sont positives ou nulles, et *définie positive* lorsqu'elles sont toutes strictement positives. 

:::{#prp-dp}

Une matrice $A$ est positive si et seulement si $\langle x, Ax\rangle$ est un nombre positif ou nul pour tout $x$. 

:::

:::{.proof}

Décomposer $x$ dans une base orthonormale $u_1, \dotsc, u_n$ de vecteurs propres de $A$ afin d'écrire $\langle x, Ax\rangle$ sous la forme $\sum_{i=1}^n \lambda_i \langle x, u_i\rangle^2$. 
L'équivalence est alors évidente. 

:::

:::{#def-domination}

On dit que $A$ est dominée par $B$ lorsque $B-A$ est une matrice positive. On note cela $A \preceq B$. 

:::

La proposition précédente montre immédiatement que c'est équivalent à ce que $\langle x, Ax\rangle \leqslant \langle x, Bx\rangle$ pour tout $x$. 