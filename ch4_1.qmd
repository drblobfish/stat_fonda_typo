# Théorie des tests simples

\newcommand{\dtv}{\mathrm{d}_{\mathrm{TV}}}

## La distance en variation totale

Lorsqu'on cherche à tester une hypothèse de type $\text{loi} = P$ contre une hypothèse de type $\mathrm{loi} = Q$ (c'est-à-dire, deux hypothèses simples), on en revient à chercher un événément très improbable sous la loi $P$, et très probable sous la loi $Q$. On peut se demander en toute généralité quels sont les événements pour lesquels ces probabilités diffèrent le plus, c'est-à-dire les événements $A$ qui maximisent $P(A) - Q(A)$. Cela mène directement à la définition de la *variation totale*.   



:::{#def-dtv}


## distance en variation totale

Soient $P,Q$ deux mesures de probabilité sur un même espace $(\mathcal{X}, \mathscr{F})$. Leur distance en variation totale est 
$$ \dtv(P,Q) = \sup_{A \in \mathscr{F}}P(A) - Q(A). $$

:::


La distance en variation totale est un objet important en probabilités, qui possède de nombreuses propriétés. Parmi elles, voici les plus importantes. 

1. C'est une distance sur l'espace des mesures de probabilité. 
2. Elle génère une topologie plus fine que celle de la convergence en loi ; autrement dit, si $\dtv(P_n, Q) \to 0$ alors $P_n$ converge en loi vers $Q$ mais l'inverse n'est pas vrai. 

:::{#prp-dtv}

Soit $\nu$ une mesure telle que $P$ et $Q$ sont absolument continues par rapport à $\nu$, de densités respectives $p$ et $q$ par rapport à $\nu$. Alors, $\dtv(P,Q)$ est égale à chacune des quantités suivantes : 

$$\int_{\mathcal{X}} (p(x) - q(x))_+d\nu$$
$$ \frac{1}{2}\int_{\mathcal{X}} |p(x) - q(x)|d\nu.$$ {#eq-dtvformule}

De plus, notons $E$ l'ensemble mesurable $\{x \in \mathcal{X} : p(x)>q(x)\}$. Alors, 
$$\dtv(P,Q) = P(E) - Q(E).$${#eq-dtv-2}
:::

:::{.proof}

Pour tout événement $A \in \mathscr{F}$, la différence $P(A) - Q(A)$ est égale à $\int_A p(x) - q(x) d\nu$, qui peut elle-même s'écrire sous la forme $$\int_{A \cap E} (p - q) d\nu + \int_{A \cap \bar{E}} (p - q) d\nu.$$
Le second terme est négatif, puisque si $x \notin E$ alors $p(x)\leqslant q(x)$. Ainsi, $P(A) - Q(A)$ est plus petit que le premier terme, lequel est à son tour plus petit que $\int_E (p-q)d\nu = P(E) - Q(E)$. Cela montre directement @eq-dtv-2. Au passage, il est évident que 
$$\int_E (p(x)-q(x))d\nu = \int_{\mathcal{X}}(p(x) - q(x))_+ d\nu,  $$
ce qui montre la première égalité de @eq-dtvformule. La seconde égalité résulte de la première, puisque comme $p$ et $q$ sont des densités de probabilité, on a forcément $\int (p-q)_+ = \int(p-q)_-$. 

:::

Dans la suite, on supposera toujours que les diverses lois possèdent toutes une densité par rapport à une mesure de référence $\nu$. C'est le cas dans de très nombreux modèles --- pas tous, hélas. Les lettres majuscules désigneront les mesures, tandis que les lettres minuscules désigneront leurs densités. 

## Test optimal au sens de l'affinité


L'affinité d'un test est la somme de ses erreurs de première et seconde espèce : c'est la probabilité de « se tromper » en général, quelle que soit l'hypothèse. 

\newcommand{\reject}{\mathsf{rejeter}}

:::{#thm-affinity}

Soit $\mathfrak{T}$ l'ensemble des tests possibles de l'hypothèse $H_0 : P = P_0$ contre l'hypothèse alternative $H_1 : P = P_1$. Alors, le test possédant la meilleure affinité possible parmi tous les tests possibles vérifie
$$\inf_{T \in \mathfrak{T}}~\{\alpha_T + \beta_T \} = 1 - \dtv(P_0, P_1). $$
En particulier, le test optimal pour l'affinité est donné par la région de rejet
$$ \reject_\star = \{p(x) < q(x)\}.$$
:::

:::{.proof}
Soit $T$ n'importe quel test. Son affinité est $P_1(\{T=0\}) + P_0(\{T=1\})$. En passant au complémentaire dans le second terme, on obtient $$1 - (P_0(\{T=0\}) - P_1(\{T=0\})). $$
Cette quantité est forcément plus petite que $1 - \dtv(P_0, P_1)$ par la définition même de la variation totale. De plus, cette borne est atteinte en choisissant le test $T$ donné dans l'énoncé, d'où l'égalité. 
:::

**Commentaire.** Le théorème précédent semble donner au problème de la construction de tests une réponse définitive : il donne le test optimal au sens de l'affinité, test qui est élémentaire et intuitif. En effet, si $P_0, P_1$ sont les deux lois et si $(x_1, \dotsc, x_n)$ est l'échantillon observé, alors on rejette l'hypothèse nulle si la probabilité de cette observation est plus grande sous $P_1$ que sous $P_0$ : autrement dit, si 
$$ \frac{P_1(x_1, \dotsc, x_n)}{P_0(x_1, \dotsc, x_n)}>1. $$
Le terme de droite s'appelle **rapport de vraisemblance**. 
Pourtant, ce test ne permet pas de contrôler l'erreur de première espèce. Il peut tout à fait exister d'autres tests qui ont un niveau plus élevé. Il est donc naturel de se demander si, parmi les tests ayant un niveau fixé $1-\alpha$, il existe un autre critère d'optimalité. 

## Théorème de Neyman-Pearson

On se place toujours dans un cadre où les deux lois $P_0$ et $P_1$ possèdent deux densités (disons, $p_0, p_1$) par rapport à une mesure commune $\nu$. 

:::{#def-testRV}

Un *test du rapport de vraisemblance* est un test dont la région de rejet est de la forme
$$\reject = \left\lbrace \frac{p_1(x)}{p_0(x)} > z \right\rbrace $${#eq-rejetMV}
pour un certain $z>0$. 

:::

Le test optimal au sens de l'affinité est un test de rapport de vraisemblance ($z=1$). 

:::{#thm-jerzy}

## Théorème de Neyman-Pearson

Tout test de même niveau qu'un test du rapport de vraisemblance est moins puissant que celui-ci. 

:::

:::{.proof}
On suppose que la région de rejet de $T_\star$ est de la forme @eq-rejetMV. Soit $T$ un autre test de même niveau que $T_\star$. La quantité
$$ \int_{\mathcal{X}} (T(x) - T_\star(x))(p_1(x) - z p_0(x))d\nu $$
est forcément négative ou nulle : en effet, si $T_\star(x)=1$, alors $T(x)-T_\star(x) = T(x)-1 \leqslant 0$,  mais $p_1(x)$ est plus grand que $zp_0(x)$, donc $(p_1(x) - zp_0(x))\geqslant 0$. De même, si $T(x) = 0$, alors cette fois ce terme est négatif. Dans les deux cas, la fonction dans l'intégrale est toujours le produit de deux nombres de signes opposés : elle est donc négative. Or, en développant cette intégrale, on constate qu'elle vaut aussi
$$P_1(T=1) - P_1(T_\star=1) - zP_0(T=1)+zP_0(T_\star=1). $$ 
Tout ceci n'est rien d'autre que $\beta_\star-\beta - z(\alpha-\alpha_\star)$, 
où $\alpha, \beta$ désignent les deux types d'erreurs du test $T$ et $\alpha_\star, \beta_\star$ celles de $T_\star$. Mais nous avons supposé que $\alpha = \alpha_\star$ : des deux termes ci-dessus, ne reste que le premier, à savoir $\beta_\star - \beta$, qui est bien négatif comme demandé. 
:::


## Bornes sur la variation totale

La construction du test optimal au sens de l'affinité nécessite le calcul de la distance en variation totale, laquelle peut être notoirement difficile : 
- d'abord, parce que la formule @eq-dtvformule peut être impossible à calculer même si $P$ et $Q$ sont connues ; 
- ensuite, parce que $Q$ elle-même peut parfois être très difficile à calculer (le calcul peut être de complexité exponentielle).

En pratique, on peut chercher à *borner* cette distance par d'autres quantités plus faciles à calculer. Parmi ces quantités, la *divergence de Kullback-Leibler* joue un rôle extrêmement important, notamment pour son lien avec le maximum de vraisemblance que nous verrons plus tard. 

\newcommand{\dkl}{\mathrm{d}_{\mathrm{KL}}}

:::{#def-dkl}

Soient $P$ et $Q$ deux mesures, $P$ étant absolument continue par rapport à $Q$. Alors, 

$$ \dkl(P \mid Q) = \int \ln \left(\frac{\mathrm{d}P}{\mathrm{d}Q}\right)dP. $$

:::

Cette quantité n'est pas une distance, et c'est pour cela qu'on l'appelle *divergence* et qu'on la note avec une barre plutôt qu'une virgule : elle n'est pas symétrique en général. Cependant, elle est toujours positive, et n'est nulle que si $P=Q$. 

:::{#thm-BH}

## Borne de Bretagnole-Huber-Pinsker

$$ \dtv(P,Q) \leqslant \sqrt{1 - e^{-\dkl(P \mid Q)}}.$${#eq-bh}

:::

Il est très facile de vérifier que $\sqrt{1-e^{-x}}\leqslant \sqrt{x}$ lorsque $x>0$. Ainsi, @eq-bh entraîne la borne plus simple $\dtv \leqslant \sqrt{\dkl}$. La borne *classique* de Pinsker dit qu'en fait, on a légèrement mieux, puisqu'en toute généralité $\dtv \leqslant \sqrt{\dkl/2}$. 

:::{.proof} 

On pose $r = p/q$, $v = 1+(r-1)_+$ et $w = 1 - (r-1)_-$. 

:::