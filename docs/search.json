[
  {
    "objectID": "content/ch7_ex.html",
    "href": "content/ch7_ex.html",
    "title": "Exercices",
    "section": "",
    "text": "$$\n\\newcommand{\\bx}{\\boldsymbol{x}}\n\\newcommand{\\bt}{\\boldsymbol{\\theta}}\n\\newcommand{\\dkl}{\\mathrm{d}_{\\mathrm{KL}}}\n\\newcommand{\\dtv}{\\mathrm{d}_{\\mathrm{TV}}}\n\\newcommand{\\emv}{\\hat{\\theta}_{\\mathrm{emv}}}\n\\newcommand{\\ent}{\\mathrm{Ent}}\n$$\n\n\n\n\n\nRien pour l’instant."
  },
  {
    "objectID": "content/ch2.html#précision-dun-estimateur",
    "href": "content/ch2.html#précision-dun-estimateur",
    "title": "2  Estimation de paramètre",
    "section": "2.1 Précision d’un estimateur",
    "text": "2.1 Précision d’un estimateur\n\nDéfinition 2.1 (Biais , risque quadratique)  \n\nLe biais de \\(\\hat{\\theta}\\) est la quantité \\(\\mathbb{E}_\\theta[\\hat\\theta - \\theta]\\). L’estimateur est dit sans biais s’il est de biais nul.\nLe risque quadratique de \\(\\hat\\theta\\) est la quantité \\(\\mathbb{E}_{\\theta}[ |\\hat{\\theta}- \\theta|^2]\\).\n\n\nEn pratique, on peut vouloir estimer non pas \\(\\theta\\) lui-même, mais un paramètre \\(\\psi = \\psi_\\theta\\) qui dépend de \\(\\theta\\), comme \\(\\cos(\\theta)\\) ou \\(|\\theta|\\) par exemple. Dans ce cas, si \\(\\hat{\\psi}\\) est un estimateur de \\(\\psi\\) alors le biais est défini par \\(\\mathbb{E}_\\theta[\\hat{\\psi} - \\psi_\\theta]\\) et le risque quadratique par \\(\\mathbb{E}_\\theta [ |\\hat\\psi - \\psi_\\theta|^2]\\).\n\nThéorème 2.1 (Décomposition biais-variance) \\[\n\\mathbb{E}_{\\theta} [|\\hat{\\theta}-\\theta|^2]\n= \\underbrace{\\operatorname{Var}_{\\theta} (\\hat{\\theta})}_{\\text{variance}} +\n\\underbrace{\\mathbb{E}_{\\theta}[\\hat{\\theta}-\\theta]^2}_{\\text{carré du biais}} \\, .\n\\]\n\n\nPreuve. En notant \\(x\\) l’espérance de \\(\\hat{\\theta}\\), on voit que le risque quadratique est égal à \\(\\mathbb{E}[|\\hat{\\theta} - x - (\\theta - x)|^2]\\). Le carré se développe en trois termes : le premier, \\(\\mathbb{E}[|\\hat{\\theta} - x|^2]\\), est la variance de \\(\\hat{\\theta}\\). Le second, \\(-2\\mathbb{E}[(\\hat{\\theta} - x)(\\theta - x)]\\), est égal à \\(-2(\\theta - x)\\mathbb{E}[\\hat{\\theta} - x]\\), c’est-à-dire 0. Le dernier, \\(\\mathbb{E}[(\\theta - x)^2]\\), est égal à \\((\\theta - x)^2\\), c’est-à-dire \\((\\theta - \\mathbb{E}[\\hat{\\theta}])^2\\) : c’est bien le carré du biais."
  },
  {
    "objectID": "content/ch2.html#convergence",
    "href": "content/ch2.html#convergence",
    "title": "2  Estimation de paramètre",
    "section": "2.2 Convergence",
    "text": "2.2 Convergence\nLa dépendance du biais ou du RQ (ou d’autres indicateurs) vis à vis de la taille de l’échantillon est une question importante : pour une suite d’expériences donnée, on veut que ces indicateurs tendent vite vers zéro. Quelles sont les meilleures vitesses envisageables, et comment les obtenir ?\nRappelons brièvement deux notions de convergence des variables aléatoires. Une suite de variables aléatoires \\(X_n\\) à valeurs dans \\(\\mathbb{R}^d\\) converge en probabilité vers une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb{R}^d\\) si pour tout \\(\\varepsilon &gt;0\\), \\[\n\\lim_{n\\to\\infty} \\mathbb{P} (| X_n -X|&gt; \\varepsilon ) = 0 \\, .\n\\]\n\nDéfinition 2.2 (convergence d’un estimateur) Une suite d’estimateurs \\((\\widehat{\\theta}_n)\\) est convergente pour l’estimation de \\(\\theta\\) lorsque, pour tout \\(\\theta \\in \\Theta\\), sous \\(P_\\theta\\), la suite \\((\\hat{\\theta}_n)\\) converge en probabilité vers \\(\\theta\\) ; autrement dit, lorsque \\[ \\forall \\varepsilon&gt;0, \\qquad \\lim_n     P_\\theta ( | \\widehat{\\theta}_n-\\theta| &gt; \\varepsilon ) =0.\n\\] La suite est fortement convergente si, pour tout \\(\\theta\\), la convergence a lieu \\(P_\\theta\\)-presque sûrement.\n\nOn voit parfois le mot consistant utilisé au lieu de convergent. Je pense que c’est un anglicisme."
  },
  {
    "objectID": "content/ch2.html#normalité-asymptotique",
    "href": "content/ch2.html#normalité-asymptotique",
    "title": "2  Estimation de paramètre",
    "section": "2.3 Normalité asymptotique",
    "text": "2.3 Normalité asymptotique\nLorsqu’un estimateur est convergent, on peut se demander à quoi ressemblent ses fluctuations autour de sa valeur limite. Le théorème central limite indique que le comportement asymptotique gaussiens est relativement fréquent, et beaucoup d’estimateurs sont des sommes de réalisations de variables indépendantes.\n\nDéfinition 2.3 (normalité asymptotique) Soit \\(\\theta\\) un paramètre à estimer, et \\(\\hat{\\theta}_n\\) une suite d’estimateurs de \\(\\theta\\). On dit que ces estimateurs sont asymptotiquement gaussiens (ou normaux) si, après les avoir renormalisés convenablement, ils convergent en loi vers une loi gaussienne. Autrement dit, s’il existe une suite \\(a_n\\) de nombres réels tels que \\[ a_n(\\hat{\\theta}_n - \\theta) \\xrightarrow[n\\to \\infty]{\\text{loi}} N(0,\\Sigma)\\] où \\(\\Sigma\\) est une matrice de covariance qui dépend peut-être de \\(\\theta\\) — pour éviter les cas dégénérés, on demande à ce que \\(\\Sigma\\) soit non-nulle."
  },
  {
    "objectID": "content/ch2.html#trois-outils-sur-la-normalité-asymptotique",
    "href": "content/ch2.html#trois-outils-sur-la-normalité-asymptotique",
    "title": "2  Estimation de paramètre",
    "section": "2.4 Trois outils sur la normalité asymptotique",
    "text": "2.4 Trois outils sur la normalité asymptotique\nLa normalité asymptotique n’est pas intéressante en elle-même ; l’idée est plutôt de chercher le comportement asymptotique de la statistique recentrée pour pouvoir en déduire des garanties en terme de risque asymptotique ou d’intervalle de confiance. Nous le ferons souvent par la suite : la normalité asymptotique sera par exemple utilisée pour construire des intervalles de confiance. Aussi, prouver que des estimateurs sont asymptotiquement normaux est une tache importante, qui est grandement simplifiée par les deux outils suivants. On commence par rappeler le théorème centra-limite.\n\nThéorème 2.2 (Théorème Central-Limite) Soit \\((X_i)\\) une suite de variables aléatoires réelles, indépendantes et identiquement distribuées. On suppose que ces variables ont une variance \\(\\sigma^2\\) finie. Alors, la variable aléatoire \\[ \\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i - \\mathbb{E}[X]\\right)\\] converge en loi vers une loi \\(N(0,\\sigma^2)\\).\n\nLe Chapitre 21 de l’appendice revient sur différentes formes du TCL.\nUn autre lemme, dit « Lemme de Slutsky », sera fréquemment utilisé pour combiner convergence en loi et convergence en probabilité.\n\nThéorème 2.3 (Lemme de Slutsky) Soit \\((X_n)\\) une suite de variables aléatoire qui converge en loi vers \\(X\\) et \\((Y_n)\\) une suite de variables aléatoires qui converge en probabilité (ou en loi) vers une constante \\(c\\). Alors, le couple \\((X_n, Y_n)\\) converge en loi vers \\((X,c)\\) ; autrement dit, pour toute fonction continue bornée \\(\\varphi\\), \\[\\mathbb{E}[\\varphi(X_n, Y_n)] \\to \\mathbb{E}[\\varphi(X,c)].\\]\n\n\nPreuve. Fixons une fonction test \\(\\varphi\\) continue à support compact, donc bornée par un certain \\(M\\). Il faut montrer que \\(\\mathbb{E}[\\varphi(X_n, Y_n) - \\varphi(X,c)]\\) tend vers zéro. L’intégrande est égal à la somme de \\(A = \\varphi(X_n, Y_n) - \\varphi(X_n, c)\\) et de \\(B=\\varphi(X_n, c) - \\varphi(X, c)\\).\nComme \\(X_n\\) tend en loi vers \\(X\\) et que \\(t\\to \\varphi(t,c)\\) est continue bornée, l’espérance de \\(B\\) tend vers zéro. Il faut donc montrer que l’espérance de \\(A\\) tend vers zéro. On fixe un \\(\\varepsilon&gt;0\\).\n\nPar le théorème de Heine, \\(\\varphi\\) est uniformément continue : il existe \\(\\delta&gt;0\\) tel que \\(|(x,y) - (x', y')|&lt;\\delta\\) entraîne que \\(|\\varphi(x,y) - \\varphi(x', y')|&lt; \\varepsilon/2\\).\nOn introduit l’événement \\(\\{|Y_n - c|\\leqslant \\delta\\}\\). Par le point précédent, sur cet événement on a \\(|A| &lt; \\varepsilon/2\\). Hors de cet événement, on peut toujours borner \\(|A|\\) par \\(2M\\). Le terme \\(|\\mathbb{E}A|\\) est donc plus petit que \\[\\mathbb{P}(|Y_n - c|\\leqslant \\delta)\\varepsilon/2 +  \\mathbb{P}(|Y_n - c| &gt; \\delta)2M.\\]\nComme \\(Y_n\\) converge en probabilité vers \\(c\\), lorsque \\(n\\) est assez grand on a \\(\\mathbb{P}(|Y_n - c| &gt; \\delta) &lt; \\varepsilon/4M\\).\nEn regroupant tout ce qui a été dit, on obtient bien \\(|\\mathbb{E}A| \\leqslant \\varepsilon\\) dès que \\(n\\) est assez grand, ce qui montre que \\(\\mathbb{E}A \\to 0\\).\n\n\n\nOn termine par la « delta-méthode » : si une suite d’estimateurs est asymptotiquement normale, leur image par n’importe quelle fonction lisse \\(g\\) l’est encore, et on sait calculer la moyenne et la variance limites.\n\n\nThéorème 2.4 (Delta-méthode) Soit \\((X_n)\\) une suite de variables aléatoires réelles telle que \\(\\sqrt{n}(X_n - \\alpha)\\) converge en loi vers \\(N(0,\\sigma^2)\\). Pour toute fonction \\(g : \\mathbb{R} \\to \\mathbb{R}\\) dérivable en \\(\\alpha\\) (de dérivée non nulle en \\(\\alpha\\)), on a \\[ \\sqrt{n}(g(X_n) - g(\\alpha)) \\xrightarrow[n \\to \\infty]{\\text{loi}} N(0, g'(\\alpha)^2 \\sigma^2).\\]\n\nPlus généralement, si les \\(X_n\\) sont à valeurs dans \\(\\mathbb{R}^d\\) et que \\(\\sqrt{n}(X_n - \\alpha) \\to N(0,\\Sigma)\\), alors pour toute application \\(g:\\mathbb{R}^d \\to \\mathbb{R}^k\\), la suite \\(\\sqrt{n}(g(X_n) - g(\\alpha))\\) converge en loi vers \\[N(0, Dg(\\alpha)\\Sigma Dg(\\alpha)^\\top)\\] où \\(Dg(x)\\) est la matrice jacobienne de \\(g\\) en \\(x\\).\n\nPreuve. L’approximation au premier ordre de \\(g\\) au point \\(\\alpha\\) dit que \\(\\sqrt{n}(g(X_n) - g(\\alpha))\\) est à peu près égal à \\(\\sqrt{n}(X_n - \\alpha)g'(\\alpha)\\), et comme \\(g'(\\alpha)\\) est une constante, ce terme converge bien en loi vers \\(N(0,\\sigma^2 g'(\\alpha)^2)\\). Il suffit donc de montrer que le terme de reste \\(r_n\\) qui complète le « à peu près » de la phrase précédente tend lui-même vers 0. On va montrer qu’il tend bien vers 0 en probabilité : l’application du Lemme de Slutsky ci-dessus permettra de conclure.\nLa formule de Taylor-Lagrange montre qu’il y a un nombre (aléatoire) \\(\\xi_n\\) entre \\(X_n\\) et \\(\\alpha\\) tel que le reste \\(r_n\\) est égal à \\[\\sqrt{n}\\frac{(X_n - \\alpha)^2}{2}g''(\\xi_n) = \\frac{[\\sqrt{n}(X_n - \\alpha)]^2}{2\\sqrt{n}}g''(\\xi_n). \\]\nOn introduit l’événement \\(E_n(b) = \\{|\\sqrt{n}(X_n - \\alpha)| &lt; b\\}\\), où \\(b\\) est une constante arbitraire ici. Sur cet événement, \\(|X_n - \\alpha|\\) est plus petit que \\(b/\\sqrt{n}\\) donc \\(X_n\\) et \\(\\xi_n\\) tendent vers \\(\\alpha\\), et donc \\(g''(\\xi_n)\\) est bornée : ainsi, sur cet événement, \\(r_n\\) tend vers zéro. Pour tout \\(\\varepsilon &gt;0\\), on a donc \\[ \\limsup_n \\mathbb{P}(|r_n|&gt;\\varepsilon) \\leqslant \\limsup_n \\mathbb{P}(\\overline{E_n(b)})\\] et le terme de droite, par convergence en loi, est égale à \\(\\mathbb{P}(|N(0,\\sigma^2)|&gt;b)\\). Ce nombre peut être rendu arbitrairement petit par le choix de \\(b\\), la limite ci-dessus est donc nulle et on a bien \\(r_n \\to 0\\) en probabilité."
  },
  {
    "objectID": "content/ch2.html#deux-estimateurs-importants",
    "href": "content/ch2.html#deux-estimateurs-importants",
    "title": "2  Estimation de paramètre",
    "section": "2.5 Deux estimateurs importants",
    "text": "2.5 Deux estimateurs importants\nDeux estimateurs sont omniprésents en statistique : la moyenne empirique et la variance empirique. Ils sont pertinents dans n’importe quel modèle où les observations sont des réalisations de variables iid possédant une moyenne \\(\\mu\\) et une variance \\(\\sigma^2\\).\nLa moyenne empirique est définie par \\[ \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\\] Il est évident que \\(\\mathbb{E}[\\bar{X}_n] = \\mathbb{E}[X] = \\mu\\). Cet estimateur est donc toujours sans biais, et son risque quadratique est égal à sa variance, c’est-à-dire \\(\\frac{\\sigma^2}{n}\\).\nL’estimateur de la variance empirique est défini comme \\[\\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2.\\]\n\nThéorème 2.5 Si les \\(X_i\\) sont indépendants (ou simplement décorrélés), l’estimateur \\(\\hat{\\sigma}_n^2\\) est sans biais.\n\n\nPreuve. Il suffit de calculer l’espérance de \\((n-1)\\hat{\\sigma}^2_n\\), ce qui revient à calculer la somme des \\(\\mathbb{E}[X_i^2 - 2X_i \\bar{X}_n + \\bar{X}_n^2]\\). Il y a trois éléments dans cette expression : \\(\\mathbb{E}[X_i^2], -2\\mathbb{E}[X_i \\bar{X}_n]\\) et \\(\\mathbb{E}[\\bar{X}_n^2]\\).\nLe premier terme est égal à \\(\\sigma^2\\). Le second terme vaut \\(-2\\sum_j \\mathbb{E}[X_i X_j]/n\\), et tous les termes avec \\(i\\neq j\\) sont nuls car les \\(X_k\\) sont décorrélés. Il ne reste que le terme \\(j=i\\), à savoir \\(-2 \\sigma^2 / n\\). Enfin, \\(\\mathbb{E}[\\bar{X}_n^2]\\) est la variance de \\(\\bar{X}_n\\), c’est-à-dire \\(\\sigma^2/n\\).\nEn additionnant tout, on obtient \\(\\sigma^2 - \\sigma^2 / n\\) et comme il y avait \\(n\\) termes dans la somme initiale, on obtient \\((n-1)\\mathbb{E}[\\hat{\\sigma}_n^2] = n\\sigma^2 - \\sigma^2\\) et donc \\(\\mathbb{E}[\\hat{\\sigma}_n^2] = \\sigma^2\\)."
  }
]