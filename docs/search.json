[
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "Statistiques Fondamentales",
    "section": "Organisation",
    "text": "Organisation\n\nLes CM ont lieu les jeudi à (8h30 - 10h30), et les vendredi (10h45 - 12h45) sauf le premier cours qui a lieu lundi 8 janvier à 10h45-12h45.\nLes TD ont lieu lundi (13h45 - 16h45) et vendredi (13h30 - 15h30), de lundi 8 janvier à vendredi 16 février.\nIl y aura deux contrôles de 2h, le vendredi 26 janvier et lundi 12 février.\nL’examen a lieu le 1er mars de 13h30 à 16h30.\nIl y aura une interro de 5 minutes chaque semaine le jeudi."
  },
  {
    "objectID": "index.html#utiliser-ce-site",
    "href": "index.html#utiliser-ce-site",
    "title": "Statistiques Fondamentales",
    "section": "Utiliser ce site",
    "text": "Utiliser ce site\nChaque chapitre de ce livre contient une page dédiée au cours théorique, et contiendra dans un futur proche une page d’exercices.\nLa saveur du cours est essentiellement mathématique et nous n’aurons pas de TP d’info ; cependant, je vous recommande vraiment d’essayer d’appliquer tout ça via votre langage de programmation favori, c’est-à-dire Python R SAS C++ Julia. J’essaierai autant que possible de fournir des mini-jeux de données avec des petits challenges pour appliquer ce que vous apprenez en cours.\nCes notes sont mises en lignes et totalement accessibles via Quarto. Si vous savez comment utiliser git, n’hésitez pas à corriger toutes les erreurs que vous pourriez voir (et Dieu sait qu’elles seront nombreuses) via des pull requests."
  },
  {
    "objectID": "ch1.html#un-exemple-pour-fixer-les-idées",
    "href": "ch1.html#un-exemple-pour-fixer-les-idées",
    "title": "1  Introduction",
    "section": "1.1 Un exemple pour fixer les idées",
    "text": "1.1 Un exemple pour fixer les idées\nUne grande enseigne de distribution possède \\(n=100\\) magasins identiques, qui génèrent chaque année un chiffre d’affaire annuel (CA, en millions d’euros). Ce chiffre oscille autour d’une valeur de référence \\(\\mu\\). Cette valeur n’est pas observée ; ce qui est observé, ce sont tous les chiffres d’affaires des \\(n\\) magasins, qui fluctuent tous autour de la vraie valeur \\(\\mu\\). Ces fluctuations proviennent de nombreuses sources : erreurs comptables, perturbations des ventes dues aux fournisseurs ou aux prix, etc. Ce qu’on observe, c’est donc des chiffres \\(x_1, \\dotsc, x_n\\) qui ne sont pas tous égaux ; comment avoir une idée de la véritable valeur de \\(\\mu\\) ?\nEstimation. Évidemment, la moyenne empirique \\[\\bar x_n = \\frac{x_1+\\dotsb + x_n}{n}\\] vient naturellement à l’esprit. En faisant le calcul, on trouve \\(\\bar{x}_n \\approx 21,6\\). Cette valeur est une estimation du CA moyen \\(\\mu\\). Ce chiffre peut être utilisé par l’enseigne, par exemple pour jauger la rentabilité d’un possible plan d’ouverture de nouveaux magasins.\nPrécision. On pourrait se demander à quel point cette estimation est précise ou, disons, essayer de quantifier l’erreur possible qu’on fait si l’on dit que \\(\\mu\\) est égal à 21,6 millions d’euros. Cela nécessite de faire quelques hypothèses sur le hasard qui génère les fluctuations des \\(x_i\\) autour de \\(\\mu\\). Ces fluctuations observées au cours de l’année proviennent de l’agrégation de toutes les fluctuations quotidiennes, lesquelles sont à peu près indépendantes, et pour cette raison on peut supposer (pour commencer) que ces fluctuations sont gaussiennes et ont à peu près la même variance, disons \\(\\sigma^2=1\\). Comme on a supposé que les \\(x_i\\) sont des réalisations d’une loi gaussienne \\(N(\\mu, 1)\\), alors on sait que \\(\\bar{x}_n\\) est la réalisation d’une loi \\(N(\\mu, 1/n)\\), ou encore que \\(\\bar{x}_n - \\mu\\) est la réalisation d’une gaussienne centrée de variance \\(1/n\\). Les lois gaussiennes sont bien connues ; par exemple, avec probabilité supérieure à 99%, une gaussienne \\(N(0, \\sigma^2)\\) est comprise entre les valeurs \\(-2,96\\sigma\\) et \\(2,96\\sigma\\). Autrement dit, il y a 99% de chances pour que le nombre \\(|\\bar{x}-\\mu|\\), qui représente l’erreur d’estimation, soit plus petite que \\(2,96/\\sqrt{n} = 2,96/10 \\approx 0,3\\).\nCe dernier raisonnement peut être vu d’une autre façon. Dire que \\(\\bar{x}_n\\) et \\(\\mu\\) ne diffèrent pas de plus de \\(0,3\\), c’est équivalent à dire que \\(\\mu\\) appartient à l’intervalle \\([\\bar{x} - 0,3, \\bar{x} +0,3]\\). En d’autres termes, avec une probabilité supérieure à 99%, le vrai CA \\(\\mu\\) de chaque magasin se situe entre \\(21,3\\) et \\(21,9\\). Cela laisse tout de même une chance de 1% que le paramètre \\(\\mu\\) ne soit pas dans cette région.\nTests. Il existe encore un autre point de vue sur ce problème. Par exemple, le conseil d’administration de la firme veut s’assurer que le dirigeant a bien tenu sa promesse selon laquelle le CA de chaque magasin était supérieur à 21 millions d’euros. La valeur exacte de \\(\\mu\\) n’est pas le plus important : ce qui nous intéresse maintenant, c’est plutôt d’être sûrs que \\(\\mu\\) n’est pas inférieur au seuil de 21. Le dirigeant, fin statisticien, effectue alors un raisonnement par l’absurde en probabilité. Supposons que le CA \\(\\mu\\) soit effectivement égal à 21 (ou même, inférieur). Alors, par les mêmes calculs que ci-dessus, cela voudrait dire qu’avec 99% de chances, \\(\\bar{x}_n\\) et \\(21\\) ne devraient pas différer de plus de \\(0,3\\) ; autrement dit, que \\(\\bar{x}_n\\) devrait se situer entre \\(20,7\\) et \\(21,3\\). Ce n’est pas le cas, puisque \\(\\bar{x}_n = 21,6\\). Si \\(\\mu\\) est réellement plus petit que 21, alors ce qu’on a observé est extrêmement peu probable. Par contraposée probabiliste, il est raisonnable de rejeter l’hypothèse selon laquelle \\(\\mu\\) est inférieur à 21.\n\nLes trois points de vue donnés ci-dessus sont en quelque sorte les piliers de l’analyse statistique. L’estimation consiste à deviner une valeur cachée dans du bruit ; les intervalles de confiance consistent à donner une région dans laquelle se trouve cette valeur ; les tests d’hypothèse permettent de raisonner de façon logique sur cette valeur.\n\nL’objectif du cours de statistiques de quantifier l’incertitude liée au hasard dans chacun de ces objectifs. Comme dans les exemples donnés ci-dessus, c’est un ensemble de méthodes scientifiques qui s’appuient sur la théorie des probabilités ; dans ce cours, on fera des hypothèses sur le hasard qui est en jeu, et on en tirera des conséquences probables sur le modèle sous-jacent. En théorie des probabilités, le jeu est plutôt inverse : partant d’un modèle probabiliste fixé, on essaie de déterminer quel sera le comportement des réalisations de ce modèle. Il semble difficile de faire l’un sans l’autre."
  },
  {
    "objectID": "ch1.html#quest-ce-quun-problème-statistique",
    "href": "ch1.html#quest-ce-quun-problème-statistique",
    "title": "1  Introduction",
    "section": "1.2 Qu’est-ce qu’un problème statistique ?",
    "text": "1.2 Qu’est-ce qu’un problème statistique ?\nIl n’y aurait pas de statistiques s’il n’y avait pas de monde réel, et comme chacun sait, le monde réel est principalement composé de quantités aléatoires.\nUn problème statistique tire donc toujours sa source d’un ensemble d’observations, disons \\(n\\) observations notées \\(x_1, \\dotsc, x_n\\) ; cet ensemble d’observations est appelé un échantillon. L’hypothèse de base de tout travail statistique consiste à supposer que cet échantillon suit une certaine loi de probabilité ; l’objectif est de trouver laquelle. Évidemment, on ne va pas partir de rien : il faut bien faire des hypothèse minimales sur cette loi. Ce qu’on appelle un modèle statistique est le choix d’une famille de lois de probabilités que l’on suppose pertinentes.\n\nDéfinition 1.1 Formellement, choisir un modèle statistique revient à choisir trois choses : \n\n\\(\\mathcal{X}\\), l’espace dans lequel vit notre échantillon ; \n\\(\\mathscr{F}\\), une tribu sur \\(\\mathcal{X}\\), pour donner du sens à ce qui est observable ou non ;\n\\((P_\\theta)_{\\theta \\in \\Theta}\\), une famille de mesures de probabilités sur \\(\\mathcal{X}\\) indexée par \\(\\theta \\in \\Theta\\), où \\(\\Theta\\) est appelé espace des paramètres. On écrira fréquemment \\(\\mathbb{E}_\\theta\\) ou \\(\\mathrm{Var}_\\theta\\) pour désigner des espérances, variances, etc., calculées avec la loi \\(P_\\theta\\).\n\n\nEn pratique, dans ce cours, on aura toujours un échantillon \\((x_1, \\dotsc, x_n)\\) où les \\(x_i\\) vivent dans un même espace, disons \\(\\mathbb{R}^d\\) pour simplifier. On devrait donc écrire \\(\\mathcal{X} = \\mathbb{R}^{d\\times n}\\) ; et l’on fera toujours l’hypothèse que ces observations sont indépendantes les unes des autres, et que ces observations ont la même loi de probabilité. Autrement dit, on se donnera toujours une mesure \\(p_\\theta\\) sur \\(\\mathbb{R}^d\\) et on supposera que la loi de notre échantillon est \\(P_\\theta = p_\\theta^{\\otimes n}\\). Dans ce cadre, les observations \\(x_i\\) sont des réalisations de variables aléatoires \\(X_i\\) iid de loi \\(p_\\theta\\).\nIl faut prendre garde à distinguer les variables aléatoires \\(X_i\\), qui sont des objets théoriques, de leurs réalisations \\(x_i\\), qui, elles, sont bel et bien observées.\n\nDéfinition 1.2 On dit qu’un modèle statistique est identifiable si \\(\\theta \\neq \\theta'\\) entraîne \\(P_\\theta \\neq P_{\\theta'}\\).\n\nSi l’on a bien choisi notre modèle statistique, alors il existe un « vrai » paramètre, disons \\(\\theta_\\star\\), tel que les observations \\(x_1, \\dotsc, x_n\\) sont des réalisations de loi \\(p_{\\theta_\\star}\\). L’objectif est alors de trouver \\(\\theta_\\star\\) ou quelque information que ce soit le concernant.\nDans un modèle identifiable, la statistique inférentielle (classique) permet de faire trois choses :\n\nTrouver une valeur approchée du vrai paramètre \\(\\theta_\\star\\) (estimation ponctuelle).\nDonner une zone de \\(\\Theta\\) dans laquelle le vrai paramètre \\(\\theta_\\star\\) a des chances de se trouver (intervalle de confiance).\nRépondre à des questions binaires sur \\(\\theta_\\star\\), par exemple « \\(\\theta_\\star\\) est-il positif ? »."
  },
  {
    "objectID": "ch1.html#quest-ce-quun-estimateur",
    "href": "ch1.html#quest-ce-quun-estimateur",
    "title": "1  Introduction",
    "section": "1.3 Qu’est-ce qu’un estimateur ?",
    "text": "1.3 Qu’est-ce qu’un estimateur ?\n\nDéfinition 1.3 Une statistique est une fonction mesurable des observations. Plus formellement, si le modèle statistique fixé est \\((\\mathcal{X}, \\mathscr{F}, P)\\), alors une statistique est n’importe quelle fonction mesurable de \\((\\mathcal{X}, \\mathscr{F})\\).\n\n\nLe premier point important est qu’une statistique ne peut pas prendre \\(\\theta\\) en argument. Ses valeurs ne doivent dépendre du paramètre \\(\\theta\\) qu’au travers de \\(P_\\theta\\).\nLe second point important est que, si \\(X\\) est une variable aléatoire et \\(T\\) une statistique, alors \\(T(X)\\) est une variable aléatoire. On peut donc définir des quantités théoriques liées à \\(T\\): typiquement, si \\(X\\) a pour loi \\(P_\\theta\\), on peut définir la valeur moyenne de \\(T\\) sous le modèle \\(P_\\theta\\) comme \\[\\mathbb{E}_\\theta[T(X)] = \\int_{\\mathcal{X}} T(x) P_\\theta(dx)\\] ou encore sa variance \\(\\mathbb{E}_\\theta[T(X)^2] - (\\mathbb{E}_\\theta[T(X)])^2\\), etc. On peut aussi calculer la valeur de cette statistique sur l’échantillon dont on dispose, c’est-à-dire \\(T(x_1, \\dotsc, x_n)\\). Par exemple, la moyenne empirique d’un \\(n\\)-échantillon réel est la fonction \\(T : (a_1, \\dots, a_n) \\to n^{-1}(a_1+\\dotsb + a_n)\\). Si les \\(x_i\\) sont des réalisations des variables aléatoires \\(X_i\\), alors \\(T(x_1, \\dotsc, x_n)\\) est une réalisation de la variable aléatoire \\(T(X_1, \\dotsc, X_n)\\).\nCe qui ne se voit pas dans la définition, c’est qu’une bonne statistique devrait être facilement calculable ; à la place de statistique, on peut penser à algorithme : une bonne statistique doit pouvoir être calculée facilement par un algorithme ne prenant en entrée que les échantillons \\(x_i\\).\n\nSi le but est de deviner la valeur de \\(\\theta\\) à partir des observations, il est naturel de considérer des statistiques à valeurs dans \\(\\Theta\\). C’est précisément la définition d’un estimateur.\n\nDéfinition 1.4 Dans le modèle \\((\\mathcal{X},\\mathcal{A}, (P_\\theta)_{\\theta \\in \\Theta})\\), un estimateur de \\(\\theta\\) est une statistique à valeurs dans \\(\\Theta\\).\n\nEn fait, on n’est pas obligés de vouloir estimer précisément \\(\\theta\\). Peut-être qu’on veut estimer quelque chose qui dépend de \\(\\theta\\), mais qui n’est pas \\(\\theta\\) ; disons, une fonction \\(\\varphi(\\theta)\\). Dans ce cas, un estimateur de \\(\\varphi(\\theta)\\) sera simplement une statistique à valeurs dans l’espace où vit \\(\\varphi(\\theta)\\)."
  },
  {
    "objectID": "ch1.html#points-de-vue",
    "href": "ch1.html#points-de-vue",
    "title": "1  Introduction",
    "section": "1.4 Points de vue",
    "text": "1.4 Points de vue\nInférence paramétrique. La plupart des expériences/modèles statistiques que nous rencontrerons dans ce cours, seront de nature dite paramétrique, autrement dit indexés par des parties de \\(\\mathbb{R}^d\\). Le mot “paramètre” est en lui-même trompeur : on parle souvent de paramètre d’une distribution pour désigner ce qui devrait plutôt s’appeler une fonctionnelle. Par exemple, la moyenne, la covariance d’une distribution sur \\(\\mathbb{R}^d\\) sont des paramètres de cette distribution. Les quantiles, l’asymétrie, la kurtosis sont d’autres paramètres.\nStatistique non paramétrique. Tous les modèles ne sont pas paramétriques au sens ci-dessus : dans de nombreux développements des statistiques, par exemple en estimation de densité, on travaille sur des modèles plus riches qui n’admettent pas de paramétrisation naturelle par une partie d’un espace euclidien de dimension finie. C’est ce qu’on appelle l’ estimation non-paramétrique. Nous y reviendrons au dernier chapitre.\nStatistique bayésienne. En statistique paramétrique, les paramètres \\(\\theta\\) déterminent le hasard qui génère les observations \\(x_i\\). La statistique bayésienne consiste à renverser le point de vue, et à rendre le paramètre \\(\\theta\\) lui-même aléatoire ; sa loi, appelée prior, mesure “le degré de connaissance a priori” qu’on en a. La règle de Bayes explique comment cette loi est modifiée par les observations. C’est un point de vue qui ne sera pas abordé dans ce cours."
  },
  {
    "objectID": "ch2.html#précision-dun-estimateur",
    "href": "ch2.html#précision-dun-estimateur",
    "title": "2  Estimation de paramètre",
    "section": "2.1 Précision d’un estimateur",
    "text": "2.1 Précision d’un estimateur\n\nDéfinition 2.1 (Biais , risque quadratique)  \n\nLe biais de \\(\\hat{\\theta}\\) est la quantité \\(\\mathbb{E}_\\theta[\\hat\\theta - \\theta]\\). L’estimateur est dit sans biais s’il est de biais nul.\nLe risque quadratique de \\(\\hat\\theta\\) est la quantité \\(\\mathbb{E}_{\\theta}[ |\\hat{\\theta}- \\theta|^2]\\).\n\n\nEn pratique, on peut vouloir estimer non pas \\(\\theta\\) lui-même, mais un paramètre \\(\\psi = \\psi_\\theta\\) qui dépend de \\(\\theta\\), comme \\(\\cos(\\theta)\\) ou \\(|\\theta|\\) par exemple. Dans ce cas, si \\(\\hat{\\psi}\\) est un estimateur de \\(\\psi\\) alors le biais est défini par \\(\\mathbb{E}_\\theta[\\hat{\\psi} - \\psi_\\theta]\\) et le risque quadratique par \\(\\mathbb{E}_\\theta [ |\\hat\\psi^2 - \\psi_\\theta|^2]\\).\nLa dépendance du risque quadratique vis à vis de la taille de l’échantillon est une question importante en statistique mathématique. Elle concerne la vitesse d’estimation (pour une suite d’expériences donnée, quelles sont les meilleures vitesses envisageables, et comment les obtenir ?).\n\nThéorème 2.1 (Décomposition biais-variance) \\[\n\\mathbb{E}_{\\theta} [|\\hat{\\theta}-\\theta|^2]\n= \\underbrace{\\operatorname{Var}_{\\theta} (\\hat{\\theta})}_{\\text{variance}} +\n\\underbrace{\\mathbb{E}_{\\theta}[\\hat{\\theta}-\\theta]^2}_{\\text{carré du biais}} \\, .\n\\]"
  },
  {
    "objectID": "ch2.html#consistances",
    "href": "ch2.html#consistances",
    "title": "2  Estimation de paramètre",
    "section": "2.2 Consistances",
    "text": "2.2 Consistances\nPour introduire la notion de consistance d’une suite d’estimateurs, nous aurons besoin des notions de convergence en probabilité et de convergence presque sûre.\n\nDéfinition 2.2 Une suite de variables aléatoires \\(X_n\\) à valeurs dans \\(\\mathbb{R}^k\\) converge en probabilité vers une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb{R}^k\\), vivant sur cet espace probabilisé si et seulement si, pour tout \\(\\epsilon &gt;0\\) \\[\n\\lim_{n\\to\\infty} \\mathbb{P} (| X_n -X|&gt; \\epsilon ) = 0 \\, .\n\\]\n\n\nDéfinition 2.3 (consistance d’un estimateur) Une suite d’estimateurs \\((\\widehat{\\theta}_n)\\) est consistante pour l’estimation de \\(\\theta\\) lorsque, pour tout \\(\\theta \\in \\Theta\\), \\[ \\forall \\varepsilon&gt;0, \\qquad \\lim_n     P_\\theta ( | \\widehat{\\theta}_n-\\theta| &gt; \\varepsilon ) =0.\n\\] La suite est fortement consistante si, pour tout \\(\\theta \\in \\Theta\\), \\[\n\\hat{\\theta}_n \\to \\theta \\quad P_\\theta-\\text{p.s.}\n\\]"
  },
  {
    "objectID": "ch2.html#normalité-asymptotique",
    "href": "ch2.html#normalité-asymptotique",
    "title": "2  Estimation de paramètre",
    "section": "2.3 Normalité asymptotique",
    "text": "2.3 Normalité asymptotique\nLorsqu’un estimateur est consistant, on peut se demander à quoi ressemblent ses fluctuations autour de sa valeur limite. Le théorème central limite indique que le comportement asymptotique gaussiens est relativement fréquent, et beaucoup d’estimateurs sont des sommes de réalisations de variables indépendantes.\n\nDéfinition 2.4 (normalité asymptotique) Soit \\(\\theta\\) un paramètre à estimer, et \\(\\hat{\\theta}_n\\) une suite d’estimateurs de \\(\\theta\\). On dit que ces estimateurs sont asymptotiquement gaussiens (ou normaux) si, après les avoir renormalisés convenablement, ils convergent en loi vers une loi gaussienne. Autrement dit, s’il existe une suite \\(a_n\\) de nombres réels tels que \\[ a_n(\\hat{\\theta}_n - \\theta) \\xrightarrow[n\\to \\infty]{\\text{loi}} N(0,\\Sigma)\\] où \\(\\Sigma\\) est une matrice de covariance qui dépend peut-être de \\(\\theta\\) — pour éviter les cas dégénérés, on demande à ce que \\(\\Sigma\\) soit non-nulle."
  },
  {
    "objectID": "ch2.html#deux-outils-sur-la-normalité-asymptotique",
    "href": "ch2.html#deux-outils-sur-la-normalité-asymptotique",
    "title": "2  Estimation de paramètre",
    "section": "2.4 Deux outils sur la normalité asymptotique",
    "text": "2.4 Deux outils sur la normalité asymptotique\nLa normalité asymptotique n’est pas intéressante en elle-même ; l’idée est plutôt de chercher le comportement asymptotique de la statistique recentrée pour pouvoir en déduire des garanties en terme de risque asymptotique ou d’intervalle de confiance. Nous utiliserons cela de nombreuses fois dans la suite ; la normalité asymptotique sera par exemple la clé de la construction de nombreux intervalles de confiance. Aussi, prouver que des estimateurs sont asymptotiquement normaux est une tache importante, qui est grandement simplifiée par les deux outils suivants.\n\nThéorème 2.2 (Lemme de Slutsky) Soit \\((X_n)\\) une suite de variables aléatoire qui converge en loi vers \\(X\\) et \\((Y_n)\\) une suite de variables aléatoires qui converge en probabilité (ou en loi) vers une constante \\(c\\). Alors, le couple \\((X_n, Y_n)\\) converge en loi vers \\((X,c)\\) ; autrement dit, pour toute fonction continue bornée \\(\\varphi\\), \\[\\mathbb{E}[\\varphi(X_n, Y_n)] \\to \\mathbb{E}[\\varphi(X,c)].\\]\n\n\nPreuve. Fixons une fonction test \\(\\varphi\\) continue à support compact, donc bornée par un certain \\(M\\). Il faut montrer que \\(\\mathbb{E}[\\varphi(X_n, Y_n) - \\varphi(X,c)]\\) tend vers zéro. L’intégrande est égal à la somme de \\(A = \\varphi(X_n, Y_n) - \\varphi(X_n, c)\\) et de \\(B=\\varphi(X_n, c) - \\varphi(X, c)\\).\nComme \\(X_n\\) tend en loi vers \\(X\\) et que \\(t\\to \\varphi(t,c)\\) est continue bornée, l’espérance de \\(B\\) tend vers zéro. Il faut donc montrer que l’espérance de \\(A\\) tend vers zéro. On fixe un \\(\\varepsilon&gt;0\\).\n\nPar le théorème de Heine, \\(\\varphi\\) est uniformément continue : il existe \\(\\delta&gt;0\\) tel que \\(|(x,y) - (x', y')|&lt;\\delta\\) entraîne que \\(|\\varphi(x,y) - \\varphi(x', y')|&lt; \\varepsilon/2\\).\nOn introduit l’événement \\(\\{|Y_n - c|\\leqslant \\delta\\}\\). Par le point précédent, sur cet événement on a \\(|A| &lt; \\varepsilon/2\\). Hors de cet événement, on peut toujours borner \\(|A|\\) par \\(2M\\). On a donc \\[|\\mathbb{E}A| \\leqslant \\mathbb{P}(|Y_n - c|\\leqslant \\delta)\\varepsilon/2 +  \\mathbb{P}(|Y_n - c| &gt; \\delta)2M.\\]\nComme \\(Y_n\\) converge en probabilité vers \\(c\\), lorsque \\(n\\) est assez grand on a \\(\\mathbb{P}(|Y_n - c| &gt; \\delta) &lt; \\varepsilon/4M\\).\nEn regroupant tout ce qui a été dit, on obtient bien \\(|\\mathbb{E}A| \\leqslant \\varepsilon\\) dès que \\(n\\) est assez grand, ce qui montre bien que \\(\\mathbb{E}A \\to 0\\).\n\n\n\n\nThéorème 2.3 (Delta-méthode) Soit \\((X_n)\\) une suite de variables aléatoires réelles telle que \\(\\sqrt{n}(X_n - \\alpha)\\) converge en loi vers \\(N(0,\\sigma^2)\\). Pour toute fonction \\(g : \\mathbb{R} \\to \\mathbb{R}\\) dérivable en \\(\\alpha\\) (de dérivée non nulle en \\(\\alpha\\)), on a \\[ \\sqrt{n}(g(X_n) - g(\\alpha)) \\xrightarrow[n \\to \\infty]{\\text{loi}} N(0, g'(\\alpha)^2 \\sigma^2).\\]\n\nPlus généralement, si les \\(X_n\\) sont à valeurs dans \\(\\mathbb{R}^d\\) et que \\(\\sqrt{n}(X_n - \\alpha) \\to N(0,\\Sigma)\\), alors pour toute application \\(g:\\mathbb{R}^d \\to \\mathbb{R}^k\\), la suite \\(\\sqrt{n}(g(X_n) - g(\\alpha))\\) converge en loi vers \\[N(0, Dg(\\alpha)\\Sigma Dg(\\alpha)^\\top)\\] où \\(Dg(x)\\) est la matrice jacobienne de \\(g\\) en \\(x\\).\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch2.html#la-méthode-des-moments",
    "href": "ch2.html#la-méthode-des-moments",
    "title": "2  Estimation de paramètre",
    "section": "2.5 La méthode des moments",
    "text": "2.5 La méthode des moments\nIl existe plusieurs techniques générales pour construire des estimateurs. La méthode des moments est la plus naturelle, et donne beaucoup des estimateurs avec de bonnes propriétés.\nDans un modèle statistique, supposons qu’on dispose d’une statistiques intégrable \\(T\\) (pas forcément réelle), dont la moyenne n’est pas le paramètre \\(\\theta\\) lui-même, mais plutôt une fonction de \\(\\theta\\) :\n\\[\\mathbb{E}_\\theta[T(X)] = \\varphi(\\theta).\\] C’est cette fonction \\(\\varphi\\) qu’on appelle moment. Typiquement,\n\nla moyenne d’une loi \\(\\mathscr{E}(\\theta)\\) n’est pas \\(\\theta\\) mais \\(1/\\theta\\).\nla moyenne d’une loi log-normale de paramètres \\((0, \\sigma^2)\\) est \\(e^{\\sigma^2/2}\\).\n\nPrenons la moyenne empirique associée à cet estimateur, \\(\\bar{T}_n\\). Par la loi des grands nombres, \\[\\bar{T}_n = \\frac{1}{n}\\sum_{i=1}^n T(X_i) \\to \\varphi(\\theta) \\qquad P_\\theta-ps, \\] ce qui permet d’estimer \\(\\varphi(\\theta)\\). Peut-on alors estimer \\(\\theta\\) ? Si la fonction \\(\\varphi\\) est inversible et si \\(\\bar{T}_n\\) appartient presque sûrement à l’ensemble de définition de \\(\\varphi^{-1}\\), alors \\(\\varphi^{-1}(\\bar{T_n})\\) est bien définie. Pour qu’en plus cette quantité converge presque sûrement vers \\(\\theta\\), il faut s’assurer que \\(\\varphi^{-1}\\) est continue. C’est par exemple le cas lorsque l’ensemble des paramètres \\(\\Theta\\) est un ouvert, et que \\(\\varphi\\) est un difféomorphisme sur son image — une situation si fréquente qu’elle mérite son propre théorème, et si agréable qu’elle garantit que l’estimateur associé est asymptotiquement normal.\n\nThéorème 2.4 (Estimation par moments) Sous l’hypothèse mentionnée ci-dessus (la fonction \\(\\varphi\\) est un difféomorphisme), l’estimateur \\[\\hat{\\theta}_n = \\varphi^{-1}(\\bar{T}_n)\\] est presque sûrement bien défini pour tout \\(n\\) suffisamment grand ; il est également consistant pour l’estimation de \\(\\theta\\). En outre, si \\(T\\) est de carré intégrable, cet estimateur est asymptotiquement normal, au sens où \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta)\\) converge en loi vers une gaussienne centrée de matrice de covariance \\[ (D\\varphi(\\theta))^{-1}\\mathrm{Cov}_\\theta(T)(D\\varphi(\\theta)^\\top)^{-1}.\\]\n\n\nPreuve. La première partie a essentiellement été démontrée un peu plus haut. Pour la seconde, il faut d’abord remarquer que si \\(T\\) est de carré intégrable, alors \\(\\sqrt{n}(\\bar{T}_n - \\varphi(\\theta))\\) converge vers une loi \\(N(0, \\mathrm{Cov}_\\theta(T))\\) par le TCL. Une simple application de la delta-méthode (Théorème 2.3) donne alors le résultat, puisque la matrice jacobienne de \\(\\varphi^{-1}\\) en \\(\\varphi(\\theta)\\) n’est autre que l’inverse de la matrice jacobienne de \\(\\varphi\\) en \\(\\theta\\)."
  },
  {
    "objectID": "ch2.html#deux-estimateurs-importants",
    "href": "ch2.html#deux-estimateurs-importants",
    "title": "2  Estimation de paramètre",
    "section": "2.6 Deux estimateurs importants",
    "text": "2.6 Deux estimateurs importants\nDeux estimateurs sont omniprésents en statistique : la moyenne empirique et la variance empirique. Ils sont pertinents dans n’importe quel modèle où les observations sont des réalisations de variables iid possédant une moyenne \\(\\mu\\) et une variance \\(\\sigma^2\\).\nLa moyenne empirique est définie par \\[ \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\\] Il est évident que \\(\\mathbb{E}[\\bar{X}_n] = \\mathbb{E}[X] = \\mu\\). Cet estimateur est donc toujours sans biais, et son risque quadratique est égal à sa variance, c’est-à-dire \\(\\frac{\\sigma^2}{n}\\).\nL’estimateur de la variance empirique est défini comme \\[\\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2.\\]\n\nThéorème 2.5 L’estimateur \\(\\hat{\\sigma}_n^2\\) est sans biais.\n\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch2_ex.html#questions",
    "href": "ch2_ex.html#questions",
    "title": "3  Exercices",
    "section": "3.1 Questions",
    "text": "3.1 Questions\n\nMontrer que la convergence en loi vers une constante implique la convergence en probabilité.\nMontrer que, si un modèle statistique n’est pas identifiable, alors il ne peut exister aucun estimateur convergent.\nTrouver un couple de variables aléatoires \\((X_n, Y_n)\\) tel que \\(X_n\\) converge en loi et \\(Y_n\\) converge en loi, mais le couple ne converge pas en loi.\nOn observe un échantillon de lois de Poisson de paramètre \\(\\lambda\\), que l’on estime par la moyenne empirique. Calculer le risque quadratique de cet estimateur.\nQuelle est la loi d’une somme de lois de Bernoulli indépendantes ? L’écart-type ?"
  },
  {
    "objectID": "ch2_ex.html#exercices",
    "href": "ch2_ex.html#exercices",
    "title": "3  Exercices",
    "section": "3.2 Exercices",
    "text": "3.2 Exercices\n\nExercice 3.1 (Variance empirique) On se donne \\(Y_1, \\dots, Y_n\\), i.i.d de moyenne \\(\\mu\\) et variance \\(\\sigma^2\\).\n\nOn suppose \\(\\mu\\) connu. Donner un estimateur non biaisé de \\(\\sigma^2\\).\nOn suppose \\(\\mu\\) inconnu. Calculer l’espérance de \\(\\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2\\). En déduire un estimateur non biaisé de \\(\\sigma^2\\).\n\n\n\nExercice 3.2 (Estimation de masse) Au cours de la seconde guerre mondiale, l’armée alliée notait les numéros de série \\(X_1, \\dots, X_n\\) de tous les tanks nazis capturés ou détruits, afin d’obtenir un estimateur du nombre total \\(N\\) de tanks produits.\n\nProposer un modèle pour le tirage de \\(X_1, \\dots, X_n\\).\nCalculer l’espérance de \\(\\bar X_n\\). En déduire un estimateur non biaisé de \\(N\\). Indication: la loi de \\(n\\) tirages sans remise est échangeable.\nÉtudier la loi de \\(X_{(n)}\\) et en déduire un estimateur non biaisé de \\(N\\).\nProposer deux intervalles de confiance de niveau \\(1-\\alpha\\) de la forme \\([aS, bS]\\) avec \\(a, b\\in\\mathbb{R}\\) et \\(S\\) une statistique. On pourra utiliser le fait que l’inégalité de Hoeffding s’applique également aux tirages sans remise.\n\nSelon Ruggles et Broodie (1947, JASA), la méthode statistique a fourni comme estimation une production moyenne de 246 tanks/mois entre juin 1940 et septembre 1942. Des méthodes d’espionnage traditionnelles donnaient une estimation de 1400 tanks/mois. Les chiffres officiels du ministère nazi des Armements ont montré après la guerre que la production moyenne était de 245 tanks/mois.\n\n\nExercice 3.3 (Lois uniformes (1)) On considère \\((X_1, \\dots, X_n)\\) un échantillon de loi uniforme sur \\(]\\theta, \\theta+1[\\).\n\nDonner la densité de la loi de la variable \\(R_n=X_{(n)} -X_{(1)}\\), où \\(X_{(1)}=\\min(X_1, \\dots, X_n)\\) et \\(X_{(n)}=\\max(X_1, \\dots, X_n)\\).\nÉtudier les différents modes de convergence de \\(R_n\\) quand \\(n\\to\\infty\\).\nÉtudier le comportement en loi de \\(n(1-R_n)\\) quand \\(n\\to\\infty\\).\n\n\n\nExercice 3.4 (Lois uniformes (2)) Soit \\(X_1,\\dots,X_n\\) un échantillon de loi \\(\\mathscr{U}([0,\\theta])\\), avec \\(\\theta &gt;0\\). On veut estimer \\(\\theta\\).\n\nDéterminer un estimateur de \\(\\theta\\) à partir de \\(\\bar{X}_n\\).\nOn considère l’estimateur \\(X_{(n)}= {\\max}_{1\\leq i \\leq n}X_i\\). Déterminer les propriétés asymotptiques de ces estimateurs.\nComparer les performances des deux estimateurs.\n\n\n\nExercice 3.5 (Lois Gamma) La loi Gamma \\(\\Gamma(\\alpha, \\beta)\\) de paramètres \\(\\alpha, \\beta&gt;0\\) a pour densité \\[ x\\mapsto \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x), \\quad x&gt;0.\\] On se donne un échantillon \\((X_1,\\dots,X_n)\\) de loi \\(\\Gamma(\\alpha, \\beta)\\) et on chercche à estimer les paramètres.\n\nOn suppose le paramètre \\(\\beta\\) connu. Proposer un estimateur de \\(\\alpha\\) par la méthode des moments.\nOn suppose à présent que les deux paramètres \\(\\alpha, \\beta\\) sont inconnus. Proposer un estimateur de \\((\\alpha,\\beta)\\) par la méthode des moments.\n\n\n\nExercice 3.6 (Lois de Gumbel) La loi de Gumbel (centrale) de paramètre \\(\\beta\\) a pour fonction de répartition \\(F(x)= e^{-e^{-x/\\beta}}\\). On observe un échantillon de lois de Gumbel et l’on cherche à estimer \\(\\beta\\).\n\nCalculer la densité des lois de Gumbel, ainsi que leur moyenne et variance [indice : \\(0.57721…\\)]\nEn déduire un estimateur convergent dont on calculera le risque quadratique et les propriétés asymptotiques.\n\n\n\nExercice 3.7 (Lois de Yule-Simon) Une variable aléatoire \\(X\\) suit la loi de Yule-Simon de paramètre \\(\\rho&gt;0\\) lorsque \\(\\mathbb{P}(X = n) = \\rho B(n, 1+\\rho)\\), où \\(n\\geqslant 1\\) et \\(B\\) est la fonction beta.\n\nMontrer que si \\(\\rho&gt;1\\), alors \\(\\mathbb{E}[X] = \\rho/(\\rho-1)\\).\nTrouver un estimateur de \\(\\rho\\) et donner ses propriétés."
  },
  {
    "objectID": "ch3.html#principe",
    "href": "ch3.html#principe",
    "title": "4  Intervalles de confiance",
    "section": "4.1 Principe",
    "text": "4.1 Principe\nDans un modèle statistique, l’estimation du paramètre d’intérêt \\(\\theta\\) par intervalles de confiance consiste à spécifier un intervalle calculable à partir des données, et qui contient \\(\\theta\\) avec grande probabilité : en d’autres termes, une région de confiance pour \\(\\theta\\).\nPour simplifier, on supposera d’abord que \\(\\theta\\) est un paramètre réel.\n\nDéfinition 4.1 (intervalle de confiance) Un intervalle de confiance de niveau \\(1-\\alpha\\) est un intervalle \\(I = [A,B]\\) dont les bornes \\(A,B\\) sont des statistiques, et tel que pour tout \\(\\theta\\), \\[P_\\theta(\\theta \\in I) \\geqslant 1 - \\alpha.\\] Un intervalle de confiance de niveau asymptotique \\(1-\\alpha\\) est une suite d’intervalles \\(I_n = [A_n,B_n]\\) dont les bornes \\(A_n,B_n\\) sont des statistiques, et tels que pour tout \\(n\\), \\[ P_\\theta(\\theta \\in I_n) \\geqslant 1 - \\alpha.\\]\n\nLe terme « niveau » désigne \\(1-\\alpha\\) ; la vocation de ce nombre est d’être proche de 1, typiquement 99%. Le nombre \\(\\alpha\\) est parfois appelé « erreur », « marge d’erreur » ou encore « niveau de risque » ; la vocation de ce nombre est d’être proche de zéro, typiquement 1%.\nIl n’y a rien d’autre à savoir sur les intervalles de confiance ; tout l’art de la chose consiste à savoir les construire. Commençons par des exemples essentiels à plusieurs titres : le cas d’un échantillon gaussien, et le cas de lois de Bernoulli."
  },
  {
    "objectID": "ch3.html#exemples-gaussiens",
    "href": "ch3.html#exemples-gaussiens",
    "title": "4  Intervalles de confiance",
    "section": "4.2 Exemples gaussiens",
    "text": "4.2 Exemples gaussiens\nOn dispose de variables aléatoires \\(X_1, \\dotsc, X_n\\) de loi \\(N(\\mu, \\sigma^2)\\). On va donner des intervalles de confiance pour l’estimation des paramètres \\(\\mu\\) et \\(\\sigma\\) dans plusieurs cas de figure.\n\n4.2.1 Estimation de \\(\\mu\\)\nLorsque \\(\\sigma\\) est connue. \nNous avons déjà vu que la moyenne empirique \\(\\bar{X}_n\\) est un estimateur sans biais de \\(\\mu\\). Or, nous savons aussi la loi exacte de \\(\\bar{X}_n\\), qui est \\(N(\\mu, \\sigma^2/n)\\). Autrement dit, \\[\\frac{\\sqrt{n}}{\\sigma}(\\bar{X}_n - \\mu) \\sim N(0,1). \\tag{4.1}\\]\nDans cette équation, on a trouvé une variable aléatoire dont la loi ne dépend plus de \\(\\mu\\). Il est donc possible de déterminer un intervalle dans lequel elle fluctue à l’aide des quantiles de la loi normale, qui sont étudiés dans Section 5.1. Si l’on se donne une marge d’erreur \\(\\alpha = 1\\%\\), alors \\[ \\mathbb{P}( (\\sqrt{n}/\\sigma)|\\bar{X}_n - \\mu| &gt; z_{0.99}) = 1\\%\\] où \\(z_{0.99} \\approx 2.32\\). Or, \\[ \\frac{\\sqrt{n}}{\\sigma}|\\bar{X}_n - \\mu| &gt; z_{0.99} \\tag{4.2}\\] est équivalent à \\[ \\bar{X}_n - \\frac{z_{0.99}\\sigma}{\\sqrt{n}} \\leqslant \\mu \\leqslant \\bar{X}_n + \\frac{z_{0.99}\\sigma}{\\sqrt{n}}. \\tag{4.3}\\] Le passage de Équation 4.2 à Équation 4.3 est souvent appelé pivot et sert à passer d’un intervalle de fluctuation à un intervalle de confiance.\nNous avons donc les deux bornes de notre intervalle de confiance : \\[ A = \\bar{X}_n - \\frac{z_{0.99}\\sigma}{\\sqrt{n}}\\] \\[ B = \\bar{X}_n + \\frac{z_{0.99}\\sigma}{\\sqrt{n}} .\\] Ces deux quantités sont bien des statistiques, car \\(\\sigma\\) est connu. De plus, nous venons de montrer que \\(P_\\mu(\\mu \\in [A,B]) = 99\\%\\). Ici, le choix de la marge d’erreur \\(\\alpha = 1\\%\\) ne jouait aucun rôle particulier ; ainsi, un intervalle de confiance de niveau \\(1-\\alpha\\) pour l’estimation de \\(\\mu\\) est donné par \\[\\left[\\bar{X}_n - \\frac{z_{1-\\alpha}\\sigma}{\\sqrt{n}}~~;~~\\bar{X}_n + \\frac{z_{1-\\alpha}\\sigma}{\\sqrt{n}} \\right]. \\tag{4.4}\\]\nLorsque \\(\\sigma\\) est inconnue. \nLorsque \\(\\sigma\\) n’est pas connu, les bornes \\(A,B\\) ci-dessus ne sont pas des statistiques, car elles dépendent de \\(\\sigma\\). Heureusement, on peut estimer \\(\\sigma\\) sans biais via l’estimateur \\[ \\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2.\\] Que se passe-t-il si, dans la statistique Équation 4.1, on remplace \\(\\sigma\\) par son estimation \\(\\hat{\\sigma}_n^2\\) ? On obtient la statistique dite de Student, \\[T_n = \\frac{\\sqrt{n}}{\\sqrt{\\hat{\\sigma}_n^2}}(\\bar{X}_n - \\mu). \\tag{4.5}\\] Sa loi n’est plus une loi gaussienne, mais une loi de Student à \\(n-1\\) paramètres de liberté \\(\\mathscr{T}(n-1)\\): le calcul de la densité est fait en détails dans Section 5.2.3 - Section 5.2.4. Les quantiles des lois de Student ont été calculés avec précision. On notera \\(t_{k,\\alpha}\\) le quantile symétrique de niveau \\(\\alpha\\) de \\(\\mathscr{T}(k)\\). Alors, \\[ P_{\\mu, \\sigma^2}(|T_n|&gt; t_{n-1,\\alpha})\\leqslant \\alpha .\\] Par le même raisonnement que tout à l’heure, l’inégalité \\[ \\left|\\frac{\\sqrt{n}}{\\hat{\\sigma}^2_n}(\\bar{X}_n - \\mu)\\right| &gt; t_{n-1,\\alpha}\\] est équivalente à \\[ \\bar{X}_n - \\frac{t_{n-1,\\alpha}\\hat{\\sigma}^2_n}{\\sqrt{n}} \\leqslant \\mu \\leqslant \\bar{X}_n + \\frac{t_{n-1,\\alpha}\\hat{\\sigma}^2_n}{\\sqrt{n}}.\\] et les deux côtés de ces inégalités sont des statistiques; en les notant \\(A,B\\), on a bien trouvé un intervalle de confiance de niveau \\(\\alpha\\), c’est-à-dire tel que \\(P_{\\mu,\\sigma^2}(\\mu \\in [A,B]) = \\alpha\\). Cet intervalle de confiance est d’une grande importance en pratique et mérite son propre théorème. Il est dû à William Gosset.\n\nThéorème 4.1 (Intervalle de Student) Un intervalle de confiance de niveau \\(1-\\alpha\\) pour l’estimation de \\(\\mu\\) lorsque \\(\\sigma\\) n’est pas connue est donné par\n\\[\\left[\\bar{X}_n - \\frac{t_{n-1, 1-\\alpha}\\hat{\\sigma}_n}{\\sqrt{n}}~~;~~\\bar{X}_n + \\frac{t_{n-1, 1-\\alpha}\\hat{\\sigma}_n}{\\sqrt{n}} \\right].\\]\n\n\n\n4.2.2 Estimation de \\(\\sigma\\)\nSupposons maintenant qu’on désire estimer la variance \\(\\sigma^2\\).\nLorsque \\(\\mu\\) est connue.\nEn supposant que \\(\\mu\\) est connue, l’estimateur des moments le plus naturel pour estimer \\(\\sigma^2\\) est évidemment \\[ \\tilde{\\sigma}^2_n = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2.\\] Comme les \\((X_i - \\mu)/\\sigma\\) sont des variables aléatoires gaussiennes centrées réduites, l’estimateur \\(\\tilde{\\sigma}^2_n \\times (n/ \\sigma^2)\\) est une somme de \\(n\\) gaussiennes standard indépendantes. La loi de cette statistique est connue : c’est une loi du chi-deux à \\(n\\) paramètres de liberté comme démontré dans Section 5.2.2. Cette loi n’est pas symétrique, puisqu’elle est supportée sur \\([0,\\infty[\\). On note souvent \\(k^-_{n,\\alpha}\\) et \\(k^+_{n,\\alpha}\\) les nombres les plus éloignées possible (ils exisent) tels que \\(\\mathbb{P}(k^-_{n,\\alpha}&lt; \\chi^2(n)&lt;k^+_{n,\\alpha}) = 1-\\alpha\\). Ainsi, \\[P_{\\sigma^2}(k^-_{n,\\alpha}&lt; \\frac{n \\tilde{\\sigma}^2_n}{\\sigma^2} &lt; k^+_{n,\\alpha}) = \\alpha.\\] En pivotant comme dans les exemples précédents, on obtient que l’intervalle \\[\\left[\\frac{n\\tilde{\\sigma}_n^2}{k^{+}_{n,\\alpha}} ~~;~~ \\frac{n\\tilde{\\sigma}_n^2}{k^-_{n,\\alpha}} \\right] \\] est un intervalle de confiance de niveau \\(\\alpha\\) pour \\(\\sigma^2\\).\nLorsque \\(\\mu\\) est inconnue.\nCette fois, on utilise l’estimateur déjà évoqué plus tôt, à savoir \\[ \\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2.\\] La loi de \\((n-1)\\hat{\\sigma}^2_n / \\sigma^2\\) est encore une loi du chi-deux, mais à \\(n-1\\) paramètres de liberté. Ainsi, le même raisonnement que ci-dessus donne l’intervalle de confiance de niveau \\(\\alpha\\) suivant :  \\[\\left[\\frac{(n-1)\\hat{\\sigma}_n^2}{k^+_{n-1,\\alpha}} ~~;~~ \\frac{(n-1)\\hat{\\sigma}_n^2}{k^-_{n-1,\\alpha}} \\right]. \\]"
  },
  {
    "objectID": "ch3.html#exemples-asymptotiques",
    "href": "ch3.html#exemples-asymptotiques",
    "title": "4  Intervalles de confiance",
    "section": "4.3 Exemples asymptotiques",
    "text": "4.3 Exemples asymptotiques\n\n4.3.1 Estimation du paramètre \\(p\\) dans un modèle de Bernoulli.\nSoient \\(X_1, \\dotsc, X_n\\) des variables indépendantes de loi \\(\\mathscr{B}(p)\\), dont on cherche à estimer le paramètre \\(p\\in ]0,1[\\). Un estimateur naturel est donné par la moyenne empirique, \\(\\hat{p}_n = (X_1 + \\dotsb + X_n)/n\\). Cet estimateur est non biaisé et son risque quadratique est égal à \\(p(1-p)/n\\). De plus, la loi de \\(\\hat{p}_n\\) est connue : \\(n\\hat{p}_n \\sim \\mathrm{Bin}(n,p)\\). Par conséquent, si l’on connaît les quantiles de \\(\\mathscr{Bin}(n,p)-p\\), on pourra construire des intervalles de confiance de niveau \\(1-\\alpha\\). Ces quantiles peuvent être calculés par des méthodes numériques, mais il existe des façons plus simples de faire.\nInégalité BT.  L’inégalité de Bienaymé-Tchebychev dit que \\[P_p(|\\hat{p}_n - p|&gt;t)\\leqslant \\frac{p(1-p)}{nt^2}.  \\tag{4.6}\\] Si l’on choisit \\[t = \\sqrt{\\frac{p(1-p)}{n\\alpha}},\\] cette probabilité est plus petite que \\(\\alpha\\). En pivotant, on en déduit que l’intervalle \\([\\hat{p_n} \\pm \\sqrt{p(1-p)/n\\alpha}]\\) contient \\(p\\) avec une probabilité supérieure à \\(1-\\alpha\\). Mais les bornes de cet intervalle ne sont pas des statistiques, car elles dépendent de \\(p\\) ! Fort heureusement, on sait que \\(p\\) est entre \\(0\\) et \\(1\\), ce qui entraîne que \\(p(1-p)\\) est plus petit que \\(1/4\\), donc l’intervalle ci-dessus est contenu dans l’intervalle plus grand \\[ \\left[\\hat{p}_n \\pm \\frac{1}{2\\sqrt{n\\alpha}}\\right]. \\] Ce dernier est bien un intervalle de confiance de niveau \\(1-\\alpha\\) pour l’estimation de \\(p\\).\nTCL.  On a mentionné que les quantiles des lois binomiales pourraient être calculés ; or, ils peuvent également être approchés grâce au théorème central-limite. Celui-ci dit que \\[ \\frac{\\sqrt{n}(\\hat{p}_n - p)}{\\sqrt{p(1-p)}} \\to N(0,1). \\tag{4.7}\\] Si \\(z_\\alpha\\) est le quantile symétrique d’ordre \\(\\alpha\\) de \\(N(0,1)\\), alors on en déduit que \\[\\mathbb{P}\\left(\\left|\\frac{\\sqrt{n}(\\hat{p}_n - p)}{\\sqrt{p(1-p)}} \\right|&gt;z_\\alpha \\right) \\to \\alpha. \\] En pivotant, on voit alors que l’intervalle \\[\\left[\\hat{p}_n \\pm z_\\alpha \\sqrt{p(1-p)/n}\\right] \\] contient \\(p\\) avec une probabilité qui tend lorsque \\(n\\to\\infty\\) vers \\(1-\\alpha\\). Là encore, cet intervalle n’est pas un intervalle de confiance. On pourrait utiliser deux techniques.\n\nComme tout à l’heure, l’intervalle ci-dessus est contenu dans l’intervalle plus grand \\([\\hat{p}_n \\pm z_\\alpha/2\\sqrt{n}]\\) qui est un intervalle de confiance asymptotique de niveau \\(1-\\alpha\\).\nIl y a plus fin. Nous savons par la loi des grands nombres que \\(\\hat{p}_n \\to p\\) en probabilité. Ainsi, \\(\\sqrt{\\hat{p}_n(1-\\hat{p}_n)} \\to \\sqrt{p(1-p)}\\) en probabilité. Le lemme de Slutsky nous assure alors que dans Équation 4.8, on peut remplacer le dénominateur par \\(\\sqrt{\\hat{p}_n (1-\\hat{p}_n)}\\) pour obtenir \\[ \\frac{\\sqrt{n}(\\hat{p}_n - p)}{\\sqrt{\\hat{p}_n(1-\\hat{p}_n)}} \\to N(0,1). \\tag{4.8}\\] Le reste du raisonnement est identique, et l’on obtient l’intervalle de confiance asymptotique de niveau \\(1-\\alpha\\) suivant : \\[\\left[\\hat{p}_n \\pm z_\\alpha \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}\\right] \\]\n\nHoeffding. L’inégalité de Bienaymé-Tchebychev n’est pas très fine. Il existe de nombreuses autres inégalités de concentration : l’inégalité de Hoeffding (Théorème 5.4) concerne les variables bornées, comme ici où les \\(X_i\\) sont dans \\([0,1]\\) . Cette inégalité dit que \\[\\mathbb{P}(|\\hat{p}_n - p|&gt;t)\\leqslant 2 e^{-2nt^2}. \\] Le choix \\[ t = \\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{1}{\\alpha}\\right)}\\] donne une probabilité inférieure à \\(\\alpha\\), et fournit donc l’intervalle de confiance non-asymptotique de niveau \\(1-\\alpha\\) suivant :  \\[ \\left[\\bar{X}_n \\pm \\frac{\\ln(1/\\alpha)}{\\sqrt{2n}}\\right].\\]\n\n\n4.3.2 Estimation de moyenne dans un modèle non-gaussien.\nLes deux techniques ci-dessus n’ont rien de spécifique au cas de variables de Bernoulli. En fait, elles s’appliquent à tout modèle statistique iid dont on cherche à estimer la moyenne \\(\\mu\\), pourvu que la variance existe.\nLa première méthode utilisant Bienaymé-Tchebychev nécessite de borner la variance. Cela peut se faire dans certains cas, mais pas dans tous.\nLa seconde méthode s’applique systématiquement en utilisant l’estimateur de la variance empirique \\(\\hat{\\sigma}_n^2\\). En effet, la convergence \\[\\frac{\\sqrt{n}}{\\hat{\\sigma}_n}(\\bar{X}_n - \\mu) \\to N(0,1)\\] est toujours vraie d’après le théorème de Slutsky.\n\nThéorème 4.2 Soient \\(X_1, \\dotsc, X_n\\) des variables iid possédant une variance. L’intervalle \\[ \\left[ \\bar{X}_n \\pm \\frac{z_\\alpha \\hat{\\sigma}_n}{\\sqrt{n}} \\right]\\] est un intervalle de confiance asymptotique de niveau \\(\\alpha\\) pour l’estimation de la moyenne des \\(X_i\\)."
  },
  {
    "objectID": "ch31_outils.html#sec-quantiles",
    "href": "ch31_outils.html#sec-quantiles",
    "title": "5  Outils pour construire des intervalles de confiance",
    "section": "5.1 Quantiles",
    "text": "5.1 Quantiles\nSi \\(X\\) est une variable aléatoire sur \\(\\mathbb{R}\\), un quantile d’ordre \\(\\beta \\in ]0,1[\\), noté \\(q_\\beta\\), est un nombre tel que \\(\\mathbb{P}(X \\leqslant q_\\beta) = \\beta\\). Lorsque \\(X\\) est continue, un tel nombre existe forcément, car la fonction de répartition \\(F(x) = \\mathbb{P}(X\\leqslant x)\\) est une surjection continue. Les quantiles symétriques \\(z_\\beta\\) sont, eux, définis par \\(\\mathbb{P}(|X|\\leqslant z_\\beta) = \\beta\\).\nSi la loi de \\(X\\) est de surcroît symétrique, les quantiles symétriques s’expriment facilement en fonction des quantiles classiques. En effet, \\(\\mathbb{P}(|X|\\leqslant z)\\) est égal à \\(\\mathbb{P}(X \\leqslant z) - \\mathbb{P}(X \\leqslant -z)\\). Or, si la loi de \\(X\\) est symétrique, alors \\(\\mathbb{P}(X \\leqslant -z) = 1 - \\mathbb{P}(X \\leqslant z)\\), et donc \\[ \\mathbb{P}(|X|\\leqslant z) = 2\\mathbb{P}(X \\leqslant z) - 1.\\] Il suffit alors de choisir pour \\(z\\) le quantile \\(q_{\\frac{1+\\beta}{2}}\\) pour obtenir \\(\\mathbb{P}(|X|\\leqslant z) = \\beta\\). Lorsque \\(\\beta\\) est de la forme \\(1-\\alpha\\) avec \\(\\alpha\\) petit (comme les niveaux des intervalles de confiance), on trouve alors \\(z_{1-\\alpha} = q_{1 - \\alpha/2}\\).\nLes quantiles s’obtiennent en inversant la fonction de répartition : lorsque celle-ci est une bijection sur \\(]0,1[\\), alors \\(q_\\beta = F^{-1}(\\beta)\\). En règle générale, il n’y a pas de forme fermée. Par exemple, pour une loi gaussienne standard, \\[F(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-u^2/2}du\\] qui elle-même n’a pas d’écriture plus simple. Fort heureusement, les outils de calcul numérique permettent d’effectuer ces calculs avec une grande précision. La table suivante donne les quantiles symétriques de la gaussienne.\n\n\n\n\\(\\beta\\)\n90%\n95%\n98%\n99%\n99.9%\n99.99999%\n\n\n\n\n\\(z_\\beta\\)\n1.64\n1.96\n2.32\n2.57\n3.2\n5.32\n\n\n\nVoir aussi la règle 1-2-3. Il existe de nombreuses tables de quantiles pour les lois usuelles.\n\nThéorème 5.1 (Queues de distribution de la gaussienne) Si \\(x\\) est plus grand que 1, \\[  \\left(\\frac{1}{x} - \\frac{1}{x^3}\\right) \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}}\\leqslant \\mathbb{P}(X &gt; x) \\leqslant \\frac{1}{x}\\frac{e^{-x^2/2}}{\\sqrt{2\\pi}} \\] En particulier, si \\(x\\) est grand, \\(\\mathbb{P}(X \\geqslant x) \\sim e^{-x^2/2}/x\\sqrt{2\\pi}\\) avec une erreur d’ordre \\(O(e^{-x^2/2}/x^3)\\).\n\nÀ titre d’exemple, pour \\(x=2.32\\) cette approximation donne 98.83%, ce qui est remarquablement proche de 98%. Pour \\(x = 2.57\\) on trouve 99.42%.\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch31_outils.html#calculs-de-lois",
    "href": "ch31_outils.html#calculs-de-lois",
    "title": "5  Outils pour construire des intervalles de confiance",
    "section": "5.2 Calculs de lois",
    "text": "5.2 Calculs de lois\n\n5.2.1 Lois Gamma\nUne variable aléatoire suit une loi Gamma de paramètres \\(\\lambda&gt;0, \\alpha&gt;0\\) lorsque sa densité est donnée par \\[\\gamma_{r,\\alpha}(x) =  \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}e^{-\\lambda x}x^{\\alpha -1}\\mathbf{1}_{x&gt;0}.\\] Les lois Gamma rassemblent les lois exponentielles (\\(\\Gamma(\\lambda, 1) = \\mathscr{E}(\\lambda)\\)) et les lois du chi-deux qu’on verra ci-dessous \\((\\Gamma(1/2, n/2) = \\chi_2(n)\\)). La transformée de Fourier \\(\\varphi_{\\lambda, \\alpha}\\) d’une loi \\(\\Gamma(\\lambda, \\alpha)\\) se calcule facilement par un changement de variables : \\[\\varphi_{\\lambda, \\alpha}(t) = \\left(1 - \\frac{it}{\\lambda}\\right)^{-\\alpha}. \\tag{5.1}\\] Cette identité montre également que si \\(X_1, \\dotsc, X_n\\) sont des variables indépendantes de loi \\(\\Gamma(\\lambda, \\alpha_i)\\), alors leur somme est une variable de loi \\(\\Gamma(\\lambda, \\alpha_1 + \\dotsc + \\alpha_n)\\).\n\n\n5.2.2 Loi du chi-deux\nSoit \\(X\\) une loi gaussienne standard. Calculons la densité de \\(X^2\\) ; pour toute fonction-test \\(\\varphi\\), \\(\\mathbb{E}[\\varphi(X^2)]\\) est donné par \\[\\frac{1}{\\sqrt{2\\pi}}\\int e^{-x^2/2}\\varphi(x^2)dx.\\] Cette intégrale est symétrique, donc on peut ajouter un facteur 2 et intégrer sur \\([0,\\infty[\\). En posant \\(u=x^2\\), on obtient alors la valeur \\[ \\frac{2}{\\sqrt{2\\pi}}\\int_0^\\infty e^{-u/2}\\varphi(u)\\frac{1}{2\\sqrt{u}}du.\\] On reconnaît la densité d’une loi Gamma de paramètres \\((1/2, 1/2)\\). Cette loi s’appelle loi du chi-deux et on la note \\(\\chi_2(1)\\). Sa tranformée de Fourier est donnée par \\[\\mathbb{E}[e^{itX^2}] = \\frac{1}{\\sqrt{1 - 2it}}. \\]\nSoient maintenant \\(X_1,\\dotsc, X_n\\) des variables de loi \\(N(0,1)\\) indépendantes. Chaque \\(X_i^2\\) est une \\(\\chi_2(1)\\) ; leur somme a pour loi la convolée \\(n\\) fois de \\(\\chi_2(1)\\). Calculons sa tranformée de Fourier :  \\[\\begin{align}\\mathbb{E}[e^{it(X_1^2 + \\dotsb + X_n^2)}] &= \\mathbb{E}[e^{itX_1^2}]^n \\\\ &= (1-2it)^{-\\frac{n}{2}} .\\end{align}\\] On reconnaît la transformée de Fourier d’une loi \\(\\Gamma(n/2, 1/2)\\) ; cette loi s’appelle loi du chi-deux à \\(n\\) paramètres de liberté et elle est notée \\(\\chi_2(n)\\). Sa densité est donnée par \\[ \\frac{1}{2^{n/2}\\Gamma(n/2)}e^{-x/2}x^{n/2 - 1}\\mathbf{1}_{x&gt;0}. \\tag{5.2}\\]\n\n\n5.2.3 Loi de Student\nSoit \\(X\\) une variable de loi \\(N(0,1)\\) et \\(Y_n\\) une variable de loi \\(\\chi_2(n)\\) indépendante de \\(X\\). On va calculer la loi de \\(T_n = X/\\sqrt{Y_n/n}\\). Soit \\(\\varphi\\) une fonction test ; l’espérance \\(\\mathbb{E}[\\varphi(T_n)]\\) est égale à \\[\\frac{1}{Z_n\\sqrt{2\\pi}}\\int_0^\\infty \\int_{-\\infty}^{\\infty}  \\varphi\\left(\\frac{x}{\\sqrt{y/n}}\\right) e^{-\\frac{x^2}{2}}e^{-\\frac{y}{2}}y^{\\frac{n}{2} - 1}dxdy \\] où \\(Z_n = 2^{n/2}\\Gamma(n/2)\\). Dans l’intégrale en \\(x\\), on effectue le changement de variable \\(u = x/\\sqrt{y/n}\\) afin d’obtenir \\[\\frac{1}{Z_n\\sqrt{2\\pi}}\\int_0^\\infty \\int_{-\\infty}^{\\infty}  \\varphi(u) e^{-\\frac{yu^2}{2n}}e^{-\\frac{y}{2}}y^{\\frac{n}{2} - 1}\\sqrt{\\frac{y}{n}} dxdy. \\] La densité de \\(T_n\\) est donc donnée par \\[t_n(u)= \\frac{1}{Z_n\\sqrt{2\\pi n}}\\int_0^\\infty  e^{-\\frac{yu^2}{2n}-\\frac{y}{2}}y^{\\frac{n+1}{2}-1} dy. \\] Le changement de variables \\(z = y(1+u^2/n)/2\\) nous ramène à \\[t_n(u) = \\frac{1}{Z_n\\sqrt{2\\pi n}}\\left(\\frac{2}{1+\\frac{u^2}{n}}\\right)^{\\frac{n+1}{2}}\\int_0^\\infty  e^{-z}z^{\\frac{n+1}{2}- 1} dz.\\] On reconnaît \\(\\Gamma((n+1)/2)\\) à droite. La densité \\(t_n(x)\\) est donc \\[t_n(x) = \\frac{1}{\\sqrt{n\\pi}}\\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)}\\left(\\frac{1}{1 + \\frac{x^2}{n}}\\right)^{\\frac{n+1}{2}}.\\]\nCette loi s’appelle loi de Student de paramètre \\(n\\) ; on dit parfois à \\(n\\) degrés de liberté. Elle est notée \\(\\mathscr{T}(n)\\). La loi de Student de paramètre \\(n=1\\) est tout simplement une loi de Cauchy.\n\n\n5.2.4 Loi de la statistique de Student\nSoient \\(X_1, \\dotsc, X_n\\) des variables gaussiennes \\(N(\\mu, \\sigma^2)\\) indépendantes, et soit \\(T_n = (\\bar{X}_n-\\mu)/\\sqrt{\\hat{\\sigma}^2_n}\\), où \\[\\hat{\\sigma}^2_n = \\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}. \\]\n\nThéorème 5.2 \\[T_n \\sim \\mathscr{T}(n-1).\\]\n\n\\(~~\\)\n\nPreuve. On va montrer 1° que \\(\\bar{X}_n\\) et \\(\\sqrt{\\hat{\\sigma}^2_n / \\sigma^2}\\) sont indépendantes, et 2° que \\(\\sqrt{\\hat{\\sigma}^2_n / \\sigma^2}\\) a bien la même loi que \\(\\sqrt{Y_{n-1}/(n-1)}\\) où \\(Y_{n-1}\\) est une \\(\\chi_2(n-1)\\). Dans la suite, on supposera que \\(\\mu=0\\) et \\(\\sigma=1\\), ce qui n’enlève rien en généralité.\nPremier point.  Le vecteur \\(X=(X_1, \\dotsc, X_n)\\) est gaussien. Posons \\(Z = (X_1 - \\bar{X}_n, \\dotsc, X_n - \\bar{X}_n)\\). Le couple \\((\\bar{X}_n, Z_n)\\) est linéaire en \\(X\\), donc ce couple est aussi un vecteur gaussien. Or, la covariance de ses deux éléments est nulle. Par exemple, \\(\\mathrm{Cov}(\\bar{X}_n, Z_1)\\) est égale à \\(\\mathrm{Cov}(\\bar{X}_n, X_1) - \\mathrm{Var}(\\bar{X}_n)\\), ce qui par linéarité donne \\(1/n - 1/n = 0\\). Ainsi, \\(\\bar{X}_n\\) et \\(Z\\) sont deux variables conjointement gaussiennes et décorrélées : elles sont donc indépendantes. Comme \\(\\hat{\\sigma}_n\\) est une fonction de \\(Z\\), elle est aussi indépendante de \\(\\bar{X}_n\\).\nSecond point.  \\(Z\\) est la projection orthogonale de \\(X\\) sur le sous-espace vectoriel \\(\\mathscr{V}=\\{x \\in \\mathbb{R}^n : x_1 + \\dotsc + x_n = 0\\}\\). Soit \\((f_i)_{i=2, \\dotsc, n}\\) une base orthonormale de \\(\\mathscr{V}\\), de sorte que \\(Z = \\sum_{i=2}^n \\langle f_i, X\\rangle f_i\\). Par l’identité de Parseval, \\[|Z|^2 = \\sum_{i=2}^n |\\langle f_i, X \\rangle|^2.\\] Or, les \\(n-1\\) variables aléatoires \\(G_i = \\langle f_i, X\\rangle\\) sont des gaussiennes standard iid. En effet, on vérifie facilement que \\(\\mathrm{Cov}(G_i, G_j) = \\langle f_i, f_j\\rangle = \\delta_{i,j}\\). On en déduit donc que \\(|Z|^2\\) suit une loi \\(\\chi_2(n-1)\\).\n\nLa seconde partie de la démonstration est un cas particulier du théorème de Cochran, que nous verrons dans le chapitre sur la régression linéaire."
  },
  {
    "objectID": "ch31_outils.html#inégalités-de-concentration",
    "href": "ch31_outils.html#inégalités-de-concentration",
    "title": "5  Outils pour construire des intervalles de confiance",
    "section": "5.3 Inégalités de concentration",
    "text": "5.3 Inégalités de concentration\nLes outils de base pour construire des intervalles de confiance dans des circonstances générales (non gaussiennes) sont les inégalités de concentration. Une inégalité de concentration pour une variable aléatoire intégrable \\(X\\) consiste à borner \\(\\mathbb{P}(|X - \\mathbb{E}[X]|&gt;x)\\) par quelque chose de petit quand x est grand : on cherche à contrôler la probabilité pour que les réalisations de la variable aléatoire \\(X\\) soient éloignées de leur valeur moyenne \\(\\mathbb{E}[X]\\) de plus de \\(x\\)."
  },
  {
    "objectID": "ch31_outils.html#inégalité-de-bienaymé-tchebychev",
    "href": "ch31_outils.html#inégalité-de-bienaymé-tchebychev",
    "title": "5  Outils pour construire des intervalles de confiance",
    "section": "5.4 Inégalité de Bienaymé-Tchebychev",
    "text": "5.4 Inégalité de Bienaymé-Tchebychev\n\nThéorème 5.3 Soit \\(X\\) une variable aléatoire de carré intégrable. Alors, \\[ \\mathbb{P}(|X - \\mathbb{E}[X]|\\geqslant x)\\leqslant \\frac{\\mathrm{Var}(X)}{x^2}.\\]\n\n\nPreuve. Élever au carré les deux membres de l’inégalité dans \\(\\mathbb{P}\\), puis appliquer l’inégalité de Markov à la variable aléatoire positive \\(|X - \\mathbb{E}X|^2\\) dont l’espérance est \\(\\mathrm{Var}(X)\\)."
  },
  {
    "objectID": "ch31_outils.html#inégalité-de-hoeffding",
    "href": "ch31_outils.html#inégalité-de-hoeffding",
    "title": "5  Outils pour construire des intervalles de confiance",
    "section": "5.5 Inégalité de Hoeffding",
    "text": "5.5 Inégalité de Hoeffding\n\nThéorème 5.4 (Inégalité de Hoeffding) Soient \\(X_1, \\dotsc, X_n\\) des variables aléatoires indépendantes, pas forcément de même loi. On suppose que chaque \\(X_i\\) est à valeurs dans un intervalle borné \\([a_i, b_i]\\) et on pose \\(S_n = X_1 + \\dotsc + X_n\\). Pour tout \\(t&gt;0\\),\n\\[\\mathbb{P}(S_n - \\mathbb{E}[S_n] \\geqslant t) \\leqslant e^{-\\frac{2t^2}{\\sum_{i=1}^n(b_i - a_i)^2}} \\tag{5.3}\\] et \\[\\mathbb{P}(|S_n - \\mathbb{E}[S_n]| \\geqslant t) \\leqslant 2e^{-\\frac{2t^2}{\\sum_{i=1}^n(b_i - a_i)^2}}.  \\tag{5.4}\\]\n\nLa démonstration se fonde sur le lemme suivant.\n\nLemme 5.1 (lemme de Hoeffding) Soit \\(X\\) une variable aléatoire à valeurs dans \\([a,b]\\). Pour tout \\(t\\),\n\\[\\mathbb{E}[e^{t(X-\\mathbb{E}[X]}] \\leqslant e^{\\frac{t^2(b-a)^2}{8}}. \\tag{5.5}\\]\n\n\nPreuve. Soit \\(X\\) une variable aléatoire, que par simplicité on supposera centrée et à valeurs dans l’intervalle \\([a,b]\\) (\\(a\\) est forcément négatif). En écrivant \\[x = a\\times \\frac{b-x}{b-a} + b\\times \\left(1 - \\frac{b-x}{b-a}\\right)\\] et en utilisant la convexité de la fonction \\(x \\mapsto e^{tx}\\), on obtient \\(e^{tX}\\leqslant (b-X)e^{ta}/(b-a) + (1 - (b-x)/(b-a)) e^{bt})\\), puis en prenant l’espérance et le fait que \\(X\\) est centrée et en simplifiant, \\[\\mathbb{E}[e^{tX}]\\leqslant \\frac{be^{ta} - ae^{tb}}{b-a}.\\] Notons \\(f(t)\\) le terme à droite ; pour montrer Équation 5.5, il suffit de montrer que \\(\\ln f(t) \\leqslant t^2(b-a)^2/8\\). La formule de Taylor dit que \\[ \\ln f(t) = \\ln f(0) + t (\\ln f)'(0) + \\frac{t^2}{2}(\\ln f)''(\\xi)\\] pour un certain \\(\\xi\\). Or, \\(\\ln f(0) = \\ln 1 = 0\\), \\((\\ln f)'(0) = f'(0)/f(0) = 0\\), et il suffit donc de montrer que \\((\\ln f)''(t)\\) est toujours plus petit que \\((b-a)^2/4\\) pour conclure. Un simple calcul montre que \\(\\ln f(t) = \\ln(b/(b-a)) + ta + \\ln(1 - ae^{t(b-a)} / b)\\), et donc \\[ (\\ln f)''(t) = \\frac{(a/b)(b-a)e^{t(b-a)}}{(1 - ae^{t(b-a)}/b)^2}.\\] L’inégalité \\(uv/(u-v)^2 \\leqslant 1/4\\) appliquée à \\(u = a/b\\) et \\(v = e^{t(b-a)}\\) permet alors de conclure.\n\nPreuve de l’inégalité de Hoeffding. En remplaçant \\(X_k\\) par \\(X_k - \\mathbb{E}[X_k]\\), on peut supposer que tous les \\(X_i\\) sont centrés et étudier seulement \\(\\mathbb{P}(S_n &gt;t)\\). Écrivons \\(\\mathbb{P}(S_n &gt; t) = \\mathbb{P}(e^{\\lambda S_n} &gt; e^{\\lambda t})\\), où \\(\\lambda\\) est un nombre positif que l’on choisira plus tard. L’inégalité de Markov borne cette probabilité par \\(\\mathbb{E}[e^{\\lambda S_n}]e^{-\\lambda t}\\). Comme les \\(X_i\\) sont indépendantes, \\(\\mathbb{E}[e^{tS_n}]\\) est le produit des \\(e^{ \\varphi_k(\\lambda)}\\) où \\(\\varphi_k(t) = \\ln \\mathbb{E}[e^{itX_k}]\\). En appliquant le lemme de Hoeffding à chaque \\(\\varphi_k\\), on borne \\(\\mathbb{P}(S_n &gt;t)\\) par \\[ \\exp\\left(\\sum_{i=1}^n \\frac{(b_i - a_i)^2 \\lambda^2}{8} - t\\lambda\\right).\\] Le minimum en \\(\\lambda\\) du terme dans l’exponentielle est atteint au point \\(4t / \\sum (a_i - b_i)^2\\) et la valeur du minimum est le terme dans l’exponentielle de Équation 5.3. On déduit Équation 5.4 par une simple borne de l’union.\nLa démonstration de l’inégalité de Hoeffding ne dépend pas directement du fait que \\(X\\) est bornée, mais plutôt de Équation 5.5. Toutes les variables aléatoires qui vérifient une inégalité de type \\(\\mathbb{E}[e^{tX}]\\leqslant e^{c t^2}\\) pour une constante \\(c\\) peuvent donc avoir leur propre inégalité de Hoeffding."
  },
  {
    "objectID": "ch3_ex.html#questions",
    "href": "ch3_ex.html#questions",
    "title": "6  Exercices",
    "section": "6.1 Questions",
    "text": "6.1 Questions\n\nSoit \\(X_n\\) une variable aléatoire de loi de Student de paramètre \\(n\\). Montrer que \\(X_n\\) converge en loi vers \\(N(0,1)\\).\nSoit \\(X_n \\sim \\chi_2(n)\\). La suite \\((X_n)\\) est-elle asymptotiquement normale ?\nDonner un intervalle de confiance de la forme \\([A,+\\infty[\\) pour la moyenne d’un échantillon gaussien.\nMême question pour la variance dans un modèle gaussien centré.\nDans l’estimation de la moyenne \\(\\mu\\) d’un modèle gaussien où la variance \\(\\sigma^2\\) est connue, montrer que l’intervalle de confiance obtenu (Équation 4.4) est le plus grand possible de niveau \\(1-\\alpha\\).\nDémontrer le théorème Théorème 5.1 sur l’asymptotique des queues de distribution de la loi gaussienne.\nMontrer la borne suivante sur les quantiles de loi gaussienne standard: \\(q_\\beta &lt; \\sqrt{\\ln\\frac{1}{\\beta\\sqrt{2\\pi}}}\\) (pour tout \\(1/2&lt;\\beta&lt;1\\)).\n\nComparer les queues de distribution des lois \\(N(0,1), \\chi_2(n)\\) et \\(\\mathscr{T}(n)\\).\nExpliquer à votre grand-mère la différence entre un intervalle de fluctuation et un intervalle de confiance.\nL’intervalle de confiance de niveau \\(1-\\alpha\\) pour la moyenne d’un modèle \\(N(\\mu, 1)\\) avec \\(n\\) observations est \\(I_n = [\\bar{X}_n \\pm z_\\alpha /\\sqrt{n}]\\). Supposons qu’on obtienne une nouvelle observation indépendante des autres, disons \\(Z\\). La probabilité \\(\\mathbb{P}(Z \\in I_n)\\) est-elle plus grande ou plus petite que \\(1-\\alpha\\) ?\nComparer la longueur des intervalles de confiance obtenus par les différentes méthodes de la section Section 4.3.1."
  },
  {
    "objectID": "ch3_ex.html#exercices",
    "href": "ch3_ex.html#exercices",
    "title": "6  Exercices",
    "section": "6.2 Exercices",
    "text": "6.2 Exercices\n\nExercice 6.1 (Lois de Poisson) On suppose que l’on observe \\(X_1, \\dots, X_n\\) i.i.d de loi \\(\\mathscr{P}(\\theta)\\).\n\nÉtudier \\(\\bar{X}_n\\).\nMontrer que \\(\\sqrt{\\bar{X}_n} \\underset{n \\rightarrow \\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}} \\sqrt{\\theta}\\).\nDonner deux intervalles de confiance au niveau \\(98 \\%\\) pour \\(\\sqrt{\\theta}\\), et les comparer.\n\n\n\nExercice 6.2 (Lois uniformes) Soit \\(X_1, \\dotsc, X_n\\) des variables aléatoires iid de loi \\(\\mathscr{U}[0,\\theta]\\). Donner un intervalle de confiance non asymptotique pour \\(\\theta\\) en utilisant l’estimateur \\(\\hat{\\theta}_n = \\max_{i=1,\\dotsc, n}X_i\\).\n\n\nExercice 6.3 (Lois exponentielles décalées) Soit \\(X_1, \\dotsc, X_n\\) des variables aléatoires iid de densité \\(e^{\\theta-x} \\mathbf{1}_{x&gt;\\theta}\\), où \\(\\theta &gt;0\\).\n\nCalculer \\(\\mathbb{E}_\\theta\\left[X_1\\right]\\) et en déduire un estimateur de \\(\\theta\\) que l’on notera \\(\\hat\\theta_n\\). Étudier ses propriétés (risque quadratique, convergence) et l’utiliser pour construire un premier intervalle de confiance \\(I_1(\\alpha)\\) non-asymptotique pour \\(\\theta\\) de niveau \\(1-\\alpha\\).\nConstruire un intervalle de confiance asymptotique \\(I_2(\\alpha)\\) pour \\(\\theta\\) à partir de \\(\\hat{\\theta}_n\\).\nMontrer que l’estimateur \\(\\theta_n^\\star := \\min_{1 \\leq i \\leq n} X_i\\) est meilleur que \\(\\hat \\theta_n\\) au sens du risque quadratique, puis l’utiliser pour construire un intervalle de confiance \\(I_3(\\alpha)\\) de niveau \\(1-\\alpha\\).\nComparer les longueurs de tous ces différents intervalles de confiance.\n\n\n\nExercice 6.4 (Lois exponentielles) Soit \\(X_1, \\dotsc, X_n\\) des variables aléatoires iid exponentielles de paramètre \\(\\lambda&gt;0\\).\n\nQuelle est la loi de \\(S_n = X_1 + \\dotsb + X_n\\) ?\nConstruire un intervalle de confiance de niveau \\(1-\\alpha\\) pour \\(\\lambda\\).\n\n\n\nExercice 6.5 (Inégalité d’Azuma) Montrer que l’inégalité de Hoeffding reste valable lorsque les \\(X_i\\) ne sont plus supposés indépendants, mais que la suite \\(S_k = X_1 + \\dotsb + X_k\\) est une martingale. Indice : \\(\\mathbb{E}[e^{\\lambda S_{n+1}}] = \\mathbb{E}[e^{\\lambda S_n}\\mathbb{E}[e^{\\lambda X_{n+1}}|S_n]]\\).\nCe raffinement s’appelle inégalite de Hoeffding-Azuma. C’est celui que nous avons utilisé dans l’exercice (ex-tanks?), lorsque les \\(X_1, \\dotsc, X_n\\) sont des tirages sans remise dans une urne à \\(N\\) éléments."
  },
  {
    "objectID": "ch4.html#exemples-de-tests-gaussiens",
    "href": "ch4.html#exemples-de-tests-gaussiens",
    "title": "7  Test d’hypothèse",
    "section": "7.1 Exemples de tests gaussiens",
    "text": "7.1 Exemples de tests gaussiens\nOn se place dans un modèle où \\(X_1, \\dotsc, X_n\\) sont des gaussiennes \\(N(\\mu, \\sigma^2)\\). Nous avons déjà vu plusieurs fois que \\(\\bar{X}_n \\sim N(\\mu, \\sigma^2)\\).\n\n7.1.1 Construction du test\nOn cherche à réfuter l’hypothèse selon laquelle ces variables aléatoires sont centrées ; autrement dit, on posera \\(H_0 = \\{0\\}\\). Sous cette hypothèse, nos variables aléatoires sont donc des variables \\(N(0,\\sigma^2)\\).\nSupposons dans un premier temps que \\(\\sigma^2\\) est connue. Sous \\(H_0\\), on a donc \\[ \\frac{\\sqrt{n}\\bar{X}_n}{\\sigma} \\sim N(0,1)\\] et par conséquent, \\(P_0(|\\bar{X}_n| &lt; z_\\alpha \\sigma / \\sqrt{n}) = 1-\\alpha\\). Autrement dit, sous l’hypothèse \\(\\mu = 0\\), on devrait observer l’événement \\[ \\bar{X}_n \\in \\left[ \\pm \\frac{z_\\alpha \\sigma}{\\sqrt{n}}\\right]\\] avec probabilité élevée \\(1-\\alpha\\). Si cet événement n’est pas observé, il est alors très douteux que \\(\\mu\\) soit effectivement égal à zéro ! On pose donc \\[ \\mathsf{rejeter}_\\alpha = \\{|\\bar{X}_n | &gt; z_\\alpha \\sigma / \\sqrt{n}\\}.\\] Le niveau de ce test est bien \\(1-\\alpha\\) : nous l’avons construit pour cela.\nSupposons maintenant que \\(\\sigma\\) n’est pas connue. En l’estimant via \\(\\hat{\\sigma}_n\\), nous savons que (toujours sous l’hypothèse selon laquelle \\(\\mu=0\\)) \\[ \\frac{\\sqrt{n}\\bar{X}_n}{\\hat{\\sigma}_n} \\sim \\mathscr{T}(n-1).\\] On reproduit alors le raisonnement ci-dessus : comme \\(\\mathbb{P}(|\\bar{X}_n| &lt; t_{n-1, 1-\\alpha}\\sqrt{\\sigma}_n / \\sqrt{n}) = \\alpha\\) où \\(t_{n-1,1-\\alpha}\\) est le quantile symétrique de \\(\\mathscr{T}(n-1)\\), on voit que l’événement \\[ \\mathsf{rejeter}_\\alpha = \\{|\\bar{X}_n| &gt; t_{n-1,1-\\alpha}\\hat{\\sigma}_n / \\sqrt{n}\\}\\] est bien un test de niveau \\(1-\\alpha\\).\n\n\n7.1.2 Calcul de la puissance et hypothèse alternative\nNous n’avons pas encore eu besoin de spécifier une hypothèse alternative, mais nous allons en avoir besoin pour calculer la puissance du test. Pour commencer, on va supposer que, si \\(\\mu\\) n’est pas nulle, alors elle ne peut être égale qu’à 1. Autrement dit, \\(H_1 = \\{1\\}\\). Ce genre d’hypothèse alternative ne peut évidemment avoir de pertinence qu’en fonction du problème réel sous-jacent !\nSous l’hypothèse alternative, donc, nous savons que \\(\\bar{X}_n \\sim N(1, \\sigma^2)\\). La puissance du test est définie par \\(1-\\beta\\) où \\(\\beta=P_1(\\mathsf{accepter}_\\alpha)\\) c’est-à-dire \\[\\begin{align}\\beta &= P_1(|\\bar{X}_n|\\leqslant z_\\alpha \\sigma / \\sqrt{n}) \\\\\n&= P_1 \\left(-\\frac{z_\\alpha \\sigma}{\\sqrt{n}}\\leqslant \\bar{X}_n \\leqslant \\frac{z_\\alpha \\sigma}{\\sqrt{n}} \\right) \\\\\n&= P_1 \\left(-\\frac{z_\\alpha \\sigma}{\\sqrt{n}}-1\\leqslant \\bar{X}_n - 1 \\leqslant \\frac{z_\\alpha \\sigma}{\\sqrt{n}} -1 \\right)\\\\\n&= \\Phi(-\\sqrt{n}/\\sigma + z_\\alpha) - \\Phi(-\\sqrt{n}/\\sigma + z_\\alpha).\n\\end{align}\\] où \\(\\Phi(x) = \\mathbb{P}(N(0,1)\\leqslant x)\\). Cette expression ne peut pas plus se simplifier, mais on peut quand même la borner par \\(F(-\\sqrt{n}/\\sigma + z_\\alpha)\\). Lorsque \\(x\\) est grand, nous avons vu (Théorème 5.1) que \\(F(x) &lt; e^{-x^2}/|x|\\sqrt{2\\pi}\\). Ainsi, l’erreur de première espèce est bornée par \\(O(e^{-n/\\sigma^2/2} / \\sqrt{n})\\). Cela tend extrêmement vite vers 0 ; en fait, dès que \\(n\\) est plus grand que 10 et \\(\\sigma=1\\), cette erreur est inférieure à 0.1%, donc dans ce cas le test aura une puissance supérieure à \\(99.9\\%\\).\nQue se serait-il passé si notre hypothèse alternative n’avait pas été \\(\\mu=1\\) mais \\(\\mu = m\\) pour n’importe quel \\(m\\neq 0\\) ? Dans ce cas, on aurait eu \\(H_1 = \\mathbb{R}\\setminus \\{0\\}\\). L’erreur de première espèce aurait alors été $= _{m}_m $ où \\[ \\beta_m = P_m(\\mathsf{accepter}_\\alpha).\\] On revoyant les calculs ci-dessus, on voit que \\[\\beta_m = \\Phi(-m\\sqrt{n}/\\sigma + z_\\alpha) - \\Phi(-m\\sqrt{n}/\\sigma + z_\\alpha).\\] Il est évident \\(\\lim_{m\\to 0}\\beta_m =1\\), par continuité de \\(\\Phi\\). Ainsi, \\(\\beta=1\\) et \\(1-\\beta = 0\\) : pour cette seconde hypothèse alternative, la puissance de notre test… est nulle.\nCela vient du fait que notre hypothèse alternative contient des situations quasiment indiscernables de notre hypothèse nulle. Par exemple, il est quasiment impossible de distinguer \\(\\mu = 0\\) de \\(\\mu = 10^{-100}\\) par exemple. Cet exemple illustre la dissymétrie entre \\(H_0\\) et \\(H_1\\)."
  },
  {
    "objectID": "ch4.html#la-notion-de-p-valeur",
    "href": "ch4.html#la-notion-de-p-valeur",
    "title": "7  Test d’hypothèse",
    "section": "7.2 La notion de \\(p\\)-valeur",
    "text": "7.2 La notion de \\(p\\)-valeur\nLa construction d’un test dépend du niveau de risque \\(\\alpha\\). Si le niveau de risque acceptable est de plus en petit, alors l’événement \\(\\mathsf{rejeter}_\\alpha\\) devrait être de moins en moins probable. D’ailleurs, \\(\\mathsf{rejeter}_0 = \\varnothing\\) et \\(\\mathsf{accepter}_0 = \\Omega\\) : si l’on ne tolère aucun niveau de risque de première espèce, c’est qu’on ne veut pas rejeter l’hypothèse nulle.\nTrès souvent, si \\(\\alpha&lt;\\beta\\), on a même \\[\\mathsf{rejeter}_\\alpha \\subset \\mathsf{rejeter}_\\beta.  \\]\n\nDéfinition 7.2 La \\(p\\)-valeur d’une famille croissante de tests est le plus petit niveau de risque qui nous amène à rejeter l’hypothèse nulle compte tenu des observations. Formellement, \\[ p = \\inf\\{\\alpha&gt;0 : \\mathsf{rejeter}_\\alpha\\}.\\]\nC’est donc une statistique."
  },
  {
    "objectID": "ch5.html#ajustement-affine-en-une-dimension.",
    "href": "ch5.html#ajustement-affine-en-une-dimension.",
    "title": "8  Modèle linéaire",
    "section": "8.1 Ajustement affine en une dimension.",
    "text": "8.1 Ajustement affine en une dimension.\nOn dispose de variables \\(x_i\\), dites explicatives, et de variables \\(y_i\\), dites à expliquer. On suppose qu’il existe une relation, peut-être imparfaite, de la forme \\[ y_i \\approx \\alpha + \\beta x_i\\] où \\(\\alpha, \\beta\\) sont deux nombres réels. Pour trouver les meilleurs \\(\\alpha, \\beta\\) possibles, on calcule la distance entre le nuage de points \\((x_i, y_i)\\) et la droite d’équation \\(y = \\alpha + \\beta x\\). Cette distance au carré est donnée par \\[ \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2.\\]\nOn cherchera donc à minimiser la fonction de deux variables \\[(\\alpha, \\beta) \\mapsto \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2.\\] Appelons cette fonction \\(L\\). C’est manifestement une fonction quadratique qui tend vers \\(+\\infty\\) lorsque \\((\\alpha, \\beta) \\to \\infty\\), par conséquent cette fonction possède un unique minimiseur \\((\\hat{\\alpha}, \\hat{\\beta})\\), et ce minimiseur est le seul point en lequel les dérivées partielles s’annulent (conditions de premier ordre) : \\(\\partial_\\alpha L(\\hat{\\alpha}, \\hat{\\beta}) = 0\\) et \\(\\partial_\\beta L(\\hat{\\alpha}, \\hat{\\beta})=0\\). Or, \\[\\partial_\\alpha L(\\alpha, \\beta)  = \\sum_{i=1}^n (\\alpha + \\beta x_i - y_i)\\] \\[ \\partial_\\beta L(\\alpha, \\beta) = \\sum_{i=1}^n x_i(\\alpha + \\beta x_i - y_i).\\] Les conditions de premier ordre deviennent donc \\(n\\alpha + \\beta (x_1 + \\dotsc + x_n) - (y_1 + \\dotsb + y_n) =0\\) soit encore \\(\\alpha + \\beta \\bar{x} - \\bar{y}=0\\), et d’autre part \\(\\alpha (x_1 + \\dotsb + x_n) + \\beta(x_1^2 + \\dotsb + x_n^2) - (x_1y_1 + \\dotsb + x_ny_n) = 0\\), soit \\(\\alpha \\bar{x}+ \\beta \\overline{xx} - \\overline{xy} = 0\\), où \\(\\overline{xx}\\) est la moyenne des carrés des \\(x_i\\) et \\(\\overline{xy}\\) la moyenne des \\(x_iy_i\\). En résolvant ces équations, on trouve d’abord \\(\\alpha\\) puis \\(\\beta\\) :  \\[\\beta = \\frac{\\overline{xy}-\\bar{x}\\bar{y}}{\\overline{xx} - \\bar{x}\\bar{x}} ,~~\\quad\\alpha = \\bar{y} - \\hat{\\beta}\\bar{x}.\\] Le coefficient \\(\\beta\\) n’est rien d’autre que la covariance empirique des \\(x_i\\) et des \\(y_i\\), normalisé par la variance empirique des \\(x_i\\).\nL’inégalité de Cauchy-Schwartz dit que \\(\\left|\\overline{xy} - \\bar{x}{\\bar{y}}\\right| \\leqslant \\tilde{\\sigma}_x \\tilde{\\sigma}_y\\), où l’on a noté \\[\\tilde{\\sigma}_x^2 = \\frac{1}{n}\\sum_{i=1}^n x_i^2 - \\left(\\frac{1}{n}\\sum_{i=1}^n x_i\\right)^2\\] l’estimateur naïf de la variance1. L’inégalité n’est une égalité que si \\(x\\) et \\(y\\) sont effectivement colinéaires, c’est-à-dire si \\(y_i = \\hat{\\alpha} + x_i \\hat{\\beta}\\) pour tous les \\(i\\). La qualité de l’ajustement affine est donc bien mesurée par la quantité \\[ r^2 = \\frac{\\overline{xy}-\\bar{x}\\bar{x}}{\\tilde{\\sigma}_x \\tilde{\\sigma}_y}.\\]"
  },
  {
    "objectID": "ch5.html#cadre-général",
    "href": "ch5.html#cadre-général",
    "title": "8  Modèle linéaire",
    "section": "8.2 Cadre général",
    "text": "8.2 Cadre général\nDans le cadre général, les variables explicatives ne sont pas de dimension 1 mais \\(d\\). On notera \\(\\boldsymbol{x}= (x_1, \\dotsc, x_d)\\) un élément de \\(\\mathbb{R}^d\\) ; les variables explicatives seront alors \\(\\boldsymbol{x}_1,\\dotsc, \\boldsymbol{x}_n\\). On cherchera des nombres \\(\\theta_i\\) tels que \\(y_i\\) est aussi proche que possible de \\[\\theta_1 \\boldsymbol{x}_{i,1} + \\dotsb + \\theta_d \\boldsymbol{x}_{i,d} = \\langle \\boldsymbol{\\theta}, \\boldsymbol{x}\\rangle = \\boldsymbol{x}_i \\theta^\\top \\] On pose \\(X\\) la matrice \\(n\\times d\\) dont la \\(i\\)-ème ligne est \\(\\boldsymbol{x}_i\\). On cherche à trouver le \\(\\boldsymbol{\\theta}\\) qui minimise \\(|Y - X\\theta|^2\\), autrement dit\n\\[\\hat{\\boldsymbol{\\theta}} = \\arg \\min_{\\theta} |Y - X\\theta|^2. \\]\n\nThéorème 8.1 Si \\(d\\leqslant n\\) et si \\(X\\) est de rang \\(d\\), alors\n\\[\\hat{\\boldsymbol{\\theta}} = (X^\\top X)^{-1}X^\\top Y. \\tag{8.1}\\]\n\n\nPreuve. La projection orthogonale sur le sous-espace vectoriel engendré par les colonnes d’une matrice \\(X\\) est la matrice \\(X(X^\\top X)^{-1}X^\\top\\). Ainsi, la projection de \\(Y\\) sur ce sous-espace est \\(X(X^\\top X)^{-1}X^\\top Y\\), et c’est aussi (par définition de l’argmin) \\(X \\hat{\\boldsymbol{\\theta}}\\). Comme \\(X\\) est injective en vertu du théorème du rang, on en déduit le résultat.\n\nLe vecteur \\(\\hat{\\varepsilon} = Y - X\\hat{\\boldsymbol{\\theta}}\\) est appelé vecteur des résidus. S’il est nul ou très petit, cela veut dire que les \\(Y\\) sont presque parfaitement des fonctions linéaires des \\(X\\)."
  },
  {
    "objectID": "ch5.html#modèle-gaussien",
    "href": "ch5.html#modèle-gaussien",
    "title": "8  Modèle linéaire",
    "section": "8.3 Modèle gaussien",
    "text": "8.3 Modèle gaussien\nÀ ce stade, nous n’avons fait aucune hypothèse statistique ni probabiliste sur le modèle : les \\(\\boldsymbol{x}_i, y_i\\) étaient donnés tels quels. Dans le modèle linéaire gaussien avec variables explicatives \\(\\boldsymbol{x}_1, \\dotsc, \\boldsymbol{x}_n\\) exogènes consiste à supposer que \\(Y = X\\boldsymbol{\\theta}+ \\varepsilon\\), où \\(\\varepsilon = N(0,\\sigma^2 I_n)\\). Formellement, le modèle est indexé par \\(\\boldsymbol{\\theta}\\) et \\(\\sigma^2\\), et donné par \\[P_{\\boldsymbol{\\theta}, \\sigma^2} = N(X\\boldsymbol{\\theta}, \\sigma^2 I_d).\\] Dans ce modèle, la loi de l’estimateur Équation 8.1 est connue.\n\nThéorème 8.2 Sous le modèle linéaire gaussien \\(P_{\\boldsymbol{\\theta}, \\sigma^2}\\), \\[\\hat{\\boldsymbol{\\theta}} \\sim N(\\boldsymbol{\\theta}, \\sigma^2 (X^\\top X)^{-1}), \\] \\[\\hat{\\varepsilon} \\sim N(0,\\sigma^2(I_n - H)), \\] et ces deux variables aléatoires sont indépendantes. En particulier, \\(|\\hat\\varepsilon|^2 / \\sigma^2 \\sim \\chi_2(d)\\).\n\n\nPreuve. Ce n’est rien de plus que le théorème de Cochran appliqué à notre problème."
  },
  {
    "objectID": "ch5.html#footnotes",
    "href": "ch5.html#footnotes",
    "title": "8  Modèle linéaire",
    "section": "",
    "text": "Celui qui est biaisé, contrairement à \\(\\hat{\\sigma}^2_n\\).↩︎"
  },
  {
    "objectID": "ch5_1.html#théorème-de-cochran",
    "href": "ch5_1.html#théorème-de-cochran",
    "title": "9  Outils gaussiens",
    "section": "9.1 Théorème de Cochran",
    "text": "9.1 Théorème de Cochran\n\nThéorème 9.1 (Théorème de Cochran) Soit \\(X \\sim N(0,I_d)\\) et soient \\(E_1, \\dotsc, E_k\\) des sous-espaces orthogonaux de \\(\\mathbb{R}^n\\) tels que \\(\\mathbb{R}^n = \\oplus_{j=1}^k E_j\\). On note \\(\\pi_j(X)\\) la projection orthogonale de \\(X\\) sur \\(E_j\\). Alors, la famille \\((\\pi_j(X))_{j = 1, \\dotsc, k}\\) est une famille de vecteurs gaussiens indépendants. De plus, \\[ |\\pi_j(X)|^2 \\sim \\chi_2(\\dim E_j).\\]\n\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch5_1.html#loi-de-fisher",
    "href": "ch5_1.html#loi-de-fisher",
    "title": "9  Outils gaussiens",
    "section": "9.2 Loi de Fisher",
    "text": "9.2 Loi de Fisher\nSoient \\(X,Y\\) deux variables aléatoires indépendantes, de lois respectives \\(\\chi_2(p)\\) et \\(\\chi_2(q)\\).\n\nThéorème 9.2 La loi du rapport \\((X/p)/(Y/q)\\) s’appelle loi de Fisher de paramètres \\(p,q\\). Sa densité est donnée par \\[ f_{p,q}(x) = \\frac{\\mathbf{1}_{x&gt;0}}{Z_{p,q}}\\frac{\\left(\\frac{px}{px + q}\\right)^{\\frac{p}{2}} \\left(1 - \\frac{px}{px + q}\\right)^{\\frac{q}{2}}}{x} \\tag{9.1}\\] où la constante \\(Z_{p,q}\\) est \\(B(p/2, q/2)\\), c’est-à-dire \\[ Z_{p,q} =  \\int_0^1 u^{\\frac{p}{2}-1}(1-u)^{\\frac{q}{2}-1}du.\\]\n\nLe calcul est facile, puisque les lois du \\(\\chi_2\\) ont une densité connue donnée par Équation 5.2. Soit \\(\\varphi\\) une fonction test et soit \\(F = (X/p)/(Y/q)\\). Alors, \\(\\mathbb{E}[\\varphi(F)]\\) vaut \\[\\frac{1}{C_p C_q}\\int_0^\\infty \\int_0^\\infty \\varphi\\left(\\frac{uq}{vp}\\right)e^{-\\frac{u}{2}-\\frac{v}{2}}u^{\\frac{p}{2}-1}v^{\\frac{q}{2}-1}dudv \\] avec \\(C_n = 2^{n/2}\\Gamma(n/2)\\). Dans l’intégrale en \\(v\\), on pose \\(x = uq/vp\\), de sorte que l’intégrale ci-dessus devient \\[\\frac{(p/q)^{\\frac{p}{2}}}{C_p C_q}\\int_0^\\infty \\varphi(x) x^{\\frac{p}{2}-1}\\int_0^\\infty e^{-\\frac{vpx}{2q}-\\frac{v}{2}}v^{\\frac{p}{2}-1}v^{\\frac{q}{2}}dv dx. \\] On reconnaît dans l’intégrale en \\(v\\) une fonction Gamma ; son intégrale est égale à \\[\\frac{\\Gamma(p/2 + q/2)}{\\left(\\frac{px+q}{2q}\\right)^{\\frac{p+q}{2}}}.\\] L’espérance \\(\\mathbb{E}[\\varphi(F)]\\) vaut donc \\[\\frac{(p/q)^{p/2}\\Gamma\\left(\\frac{p+q}{2}\\right)}{C_p C_q (2q)^{\\frac{p+q}{2}}}\\int_0^\\infty \\varphi(x)\\frac{x^{\\frac{p}{2}-1}}{(px + q)^{\\frac{p+q}{2}}}dx. \\] En simplifiant, on trouve exactement la densité donnée par Équation 9.1."
  },
  {
    "objectID": "ch6.html",
    "href": "ch6.html",
    "title": "10  Théorie de l’information",
    "section": "",
    "text": "Cours 8-9-10"
  },
  {
    "objectID": "ch7.html",
    "href": "ch7.html",
    "title": "11  Estimation de densité",
    "section": "",
    "text": "Cours 11-12"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Et après ?",
    "section": "",
    "text": "nasuitenasute"
  }
]