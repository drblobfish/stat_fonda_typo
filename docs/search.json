[
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "Statistiques Fondamentales",
    "section": "Organisation",
    "text": "Organisation\n\nLes CM ont lieu les jeudi à (8h30 - 10h30), et les vendredi (10h45 - 12h45) sauf le premier cours qui a lieu lundi 8 janvier à 10h45-12h45.\nLes TD ont lieu lundi (13h45 - 16h45) et vendredi (13h30 - 15h30), de lundi 8 janvier à vendredi 16 février.\nIl y aura deux contrôles de 2h, le vendredi 26 janvier et lundi 12 février.\nL’examen a lieu le 1er mars de 13h30 à 16h30.\nIl y aura une interro de 5 minutes chaque semaine le jeudi."
  },
  {
    "objectID": "index.html#utiliser-ce-site",
    "href": "index.html#utiliser-ce-site",
    "title": "Statistiques Fondamentales",
    "section": "Utiliser ce site",
    "text": "Utiliser ce site\nChaque chapitre de ce livre contient une page dédiée au cours théorique, et contiendra dans un futur proche une page d’exercices.\nLa saveur du cours est essentiellement mathématique et nous n’aurons pas de TP d’info ; cependant, je vous recommande vraiment d’essayer d’appliquer tout ça via votre langage de programmation favori, c’est-à-dire Python R SAS C++ Julia. J’essaierai autant que possible de fournir des mini-jeux de données avec des petits challenges pour appliquer ce que vous apprenez en cours.\nCes notes sont mises en lignes et totalement accessibles via Quarto. Si vous savez comment utiliser git, n’hésitez pas à corriger toutes les erreurs que vous pourriez voir (et Dieu sait qu’elles seront nombreuses) via des pull requests."
  },
  {
    "objectID": "ch1.html#un-exemple-pour-fixer-les-idées",
    "href": "ch1.html#un-exemple-pour-fixer-les-idées",
    "title": "1  Introduction",
    "section": "1.1 Un exemple pour fixer les idées",
    "text": "1.1 Un exemple pour fixer les idées\nUne grande enseigne de distribution possède \\(n=100\\) magasins identiques, qui génèrent chaque année un chiffre d’affaire annuel (CA, en millions d’euros). Ce chiffre oscille autour d’une valeur de référence \\(\\mu\\). Cette valeur n’est pas observée ; ce qui est observé, ce sont tous les chiffres d’affaires des \\(n\\) magasins, qui fluctuent tous autour de la vraie valeur \\(\\mu\\). Ces fluctuations proviennent de nombreuses sources : erreurs comptables, perturbations des ventes dues aux fournisseurs ou aux prix, etc. Ce qu’on observe, c’est donc des chiffres \\(x_1, \\dotsc, x_n\\) qui ne sont pas tous égaux ; comment avoir une idée de la véritable valeur de \\(\\mu\\) ?\nEstimation. Évidemment, la moyenne empirique \\[\\bar x_n = \\frac{x_1+\\dotsb + x_n}{n}\\] vient naturellement à l’esprit. En faisant le calcul, on trouve \\(\\bar{x}_n \\approx 21,6\\). Cette valeur est une estimation du CA moyen \\(\\mu\\). Ce chiffre peut être utilisé par l’enseigne, par exemple pour jauger la rentabilité d’un possible plan d’ouverture de nouveaux magasins.\nPrécision. On pourrait se demander à quel point cette estimation est précise ou, disons, essayer de quantifier l’erreur possible qu’on fait si l’on dit que \\(\\mu\\) est égal à 21,6 millions d’euros. Cela nécessite de faire quelques hypothèses sur le hasard qui génère les fluctuations des \\(x_i\\) autour de \\(\\mu\\). Ces fluctuations observées au cours de l’année proviennent de l’agrégation de toutes les fluctuations quotidiennes, lesquelles sont à peu près indépendantes, et pour cette raison on peut supposer (pour commencer) que ces fluctuations sont gaussiennes et ont à peu près la même variance, disons \\(\\sigma^2=1\\). Comme on a supposé que les \\(x_i\\) sont des réalisations d’une loi gaussienne \\(N(\\mu, 1)\\), alors on sait que \\(\\bar{x}_n\\) est la réalisation d’une loi \\(N(\\mu, 1/n)\\), ou encore que \\(\\bar{x}_n - \\mu\\) est la réalisation d’une gaussienne centrée de variance \\(1/n\\). Les lois gaussiennes sont bien connues ; par exemple, avec probabilité supérieure à 99%, une gaussienne \\(N(0, \\sigma^2)\\) est comprise entre les valeurs \\(-2,96\\sigma\\) et \\(2,96\\sigma\\). Autrement dit, il y a 99% de chances pour que le nombre \\(|\\bar{x}-\\mu|\\), qui représente l’erreur d’estimation, soit plus petite que \\(2,96/\\sqrt{n} = 2,96/10 \\approx 0,3\\).\nCe dernier raisonnement peut être vu d’une autre façon. Dire que \\(\\bar{x}_n\\) et \\(\\mu\\) ne diffèrent pas de plus de \\(0,3\\), c’est équivalent à dire que \\(\\mu\\) appartient à l’intervalle \\([\\bar{x} - 0,3, \\bar{x} +0,3]\\). En d’autres termes, avec une probabilité supérieure à 99%, le vrai CA \\(\\mu\\) de chaque magasin se situe entre \\(21,3\\) et \\(21,9\\). Cela laisse tout de même une chance de 1% que le paramètre \\(\\mu\\) ne soit pas dans cette région.\nTests. Il existe encore un autre point de vue sur ce problème. Par exemple, le conseil d’administration de la firme veut s’assurer que le dirigeant a bien tenu sa promesse selon laquelle le CA de chaque magasin était supérieur à 21 millions d’euros. La valeur exacte de \\(\\mu\\) n’est pas le plus important : ce qui nous intéresse maintenant, c’est plutôt d’être sûrs que \\(\\mu\\) n’est pas inférieur au seuil de 21. Le dirigeant, fin statisticien, effectue alors un raisonnement par l’absurde en probabilité. Supposons que le CA \\(\\mu\\) soit effectivement égal à 21 (ou même, inférieur). Alors, par les mêmes calculs que ci-dessus, cela voudrait dire qu’avec 99% de chances, \\(\\bar{x}_n\\) et \\(21\\) ne devraient pas différer de plus de \\(0,3\\) ; autrement dit, que \\(\\bar{x}_n\\) devrait se situer entre \\(20,7\\) et \\(21,3\\). Ce n’est pas le cas, puisque \\(\\bar{x}_n = 21,6\\). Si \\(\\mu\\) est réellement plus petit que 21, alors ce qu’on a observé est extrêmement peu probable. Par contraposée probabiliste, il est raisonnable de rejeter l’hypothèse selon laquelle \\(\\mu\\) est inférieur à 21.\n\nLes trois points de vue donnés ci-dessus sont en quelque sorte les piliers de l’analyse statistique. L’estimation consiste à deviner une valeur cachée dans du bruit ; les intervalles de confiance consistent à donner une région dans laquelle se trouve cette valeur ; les tests d’hypothèse permettent de raisonner de façon logique sur cette valeur.\n\nL’objectif du cours de statistiques de quantifier l’incertitude liée au hasard dans chacun de ces objectifs. Comme dans les exemples donnés ci-dessus, c’est un ensemble de méthodes scientifiques qui s’appuient sur la théorie des probabilités ; dans ce cours, on fera des hypothèses sur le hasard qui est en jeu, et on en tirera des conséquences probables sur le modèle sous-jacent. En théorie des probabilités, le jeu est plutôt inverse : partant d’un modèle probabiliste fixé, on essaie de déterminer quel sera le comportement des réalisations de ce modèle. Il semble difficile de faire l’un sans l’autre."
  },
  {
    "objectID": "ch1.html#quest-ce-quun-problème-statistique",
    "href": "ch1.html#quest-ce-quun-problème-statistique",
    "title": "1  Introduction",
    "section": "1.2 Qu’est-ce qu’un problème statistique ?",
    "text": "1.2 Qu’est-ce qu’un problème statistique ?\nIl n’y aurait pas de statistiques s’il n’y avait pas de monde réel, et comme chacun sait, le monde réel est principalement composé de quantités aléatoires.\nUn problème statistique tire donc toujours sa source d’un ensemble d’observations, disons \\(n\\) observations notées \\(x_1, \\dotsc, x_n\\) ; cet ensemble d’observations est appelé un échantillon. L’hypothèse de base de tout travail statistique consiste à supposer que cet échantillon suit une certaine loi de probabilité ; l’objectif est de trouver laquelle. Évidemment, on ne va pas partir de rien : il faut bien faire des hypothèse minimales sur cette loi. Ce qu’on appelle un modèle statistique est le choix d’une famille de lois de probabilités que l’on suppose pertinentes.\n\nDéfinition 1.1 Formellement, choisir un modèle statistique revient à choisir trois choses : \n\n\\(\\mathcal{X}\\), l’espace dans lequel vit notre échantillon ; \n\\(\\mathscr{F}\\), une tribu sur \\(\\mathcal{X}\\), pour donner du sens à ce qui est observable ou non ;\n\\((P_\\theta)_{\\theta \\in \\Theta}\\), une famille de mesures de probabilités sur \\(\\mathcal{X}\\) indexée par \\(\\theta \\in \\Theta\\), où \\(\\Theta\\) est appelé espace des paramètres. On écrira fréquemment \\(\\mathbb{E}_\\theta\\) ou \\(\\mathrm{Var}_\\theta\\) pour désigner des espérances, variances, etc., calculées avec la loi \\(P_\\theta\\).\n\n\nEn pratique, dans ce cours, on aura toujours un échantillon \\((x_1, \\dotsc, x_n)\\) où les \\(x_i\\) vivent dans un même espace, disons \\(\\mathbb{R}^d\\) pour simplifier. On devrait donc écrire \\(\\mathcal{X} = \\mathbb{R}^{d\\times n}\\) ; et l’on fera toujours l’hypothèse que ces observations sont indépendantes les unes des autres, et que ces observations ont la même loi de probabilité. Autrement dit, on se donnera toujours une mesure \\(p_\\theta\\) sur \\(\\mathbb{R}^d\\) et on supposera que la loi de notre échantillon est \\(P_\\theta = p_\\theta^{\\otimes n}\\). Dans ce cadre, les observations \\(x_i\\) sont des réalisations de variables aléatoires \\(X_i\\) iid de loi \\(p_\\theta\\).\nIl faut prendre garde à distinguer les variables aléatoires \\(X_i\\), qui sont des objets théoriques, de leurs réalisations \\(x_i\\), qui, elles, sont bel et bien observées.\n\nDéfinition 1.2 On dit qu’un modèle statistique est identifiable si \\(\\theta \\neq \\theta'\\) entraîne \\(P_\\theta \\neq P_{\\theta'}\\).\n\nSi l’on a bien choisi notre modèle statistique, alors il existe un « vrai » paramètre, disons \\(\\theta_\\star\\), tel que les observations \\(x_1, \\dotsc, x_n\\) sont des réalisations de loi \\(p_{\\theta_\\star}\\). L’objectif est alors de trouver \\(\\theta_\\star\\) ou quelque information que ce soit le concernant.\nDans un modèle identifiable, la statistique inférentielle (classique) permet de faire trois choses :\n\nTrouver une valeur approchée du vrai paramètre \\(\\theta_\\star\\) (estimation ponctuelle).\nDonner une zone de \\(\\Theta\\) dans laquelle le vrai paramètre \\(\\theta_\\star\\) a des chances de se trouver (intervalle de confiance).\nRépondre à des questions binaires sur \\(\\theta_\\star\\), par exemple « \\(\\theta_\\star\\) est-il positif ? »."
  },
  {
    "objectID": "ch1.html#quest-ce-quun-estimateur",
    "href": "ch1.html#quest-ce-quun-estimateur",
    "title": "1  Introduction",
    "section": "1.3 Qu’est-ce qu’un estimateur ?",
    "text": "1.3 Qu’est-ce qu’un estimateur ?\n\nDéfinition 1.3 Une statistique est une fonction mesurable des observations. Plus formellement, si le modèle statistique fixé est \\((\\mathcal{X}, \\mathscr{F}, P)\\), alors une statistique est n’importe quelle fonction mesurable de \\((\\mathcal{X}, \\mathscr{F})\\).\n\n\nLe premier point important est qu’une statistique ne peut pas prendre \\(\\theta\\) en argument. Ses valeurs ne doivent dépendre du paramètre \\(\\theta\\) qu’au travers de \\(P_\\theta\\).\nLe second point important est que, si \\(X\\) est une variable aléatoire et \\(T\\) une statistique, alors \\(T(X)\\) est une variable aléatoire. On peut donc définir des quantités théoriques liées à \\(T\\): typiquement, si \\(X\\) a pour loi \\(P_\\theta\\), on peut définir la valeur moyenne de \\(T\\) sous le modèle \\(P_\\theta\\) comme \\[\\mathbb{E}_\\theta[T(X)] = \\int_{\\mathcal{X}} T(x) P_\\theta(dx)\\] ou encore sa variance \\(\\mathbb{E}_\\theta[T(X)^2] - (\\mathbb{E}_\\theta[T(X)])^2\\), etc. On peut aussi calculer la valeur de cette statistique sur l’échantillon dont on dispose, c’est-à-dire \\(T(x_1, \\dotsc, x_n)\\). Par exemple, la moyenne empirique d’un \\(n\\)-échantillon réel est la fonction \\(T : (a_1, \\dots, a_n) \\to n^{-1}(a_1+\\dotsb + a_n)\\). Si les \\(x_i\\) sont des réalisations des variables aléatoires \\(X_i\\), alors \\(T(x_1, \\dotsc, x_n)\\) est une réalisation de la variable aléatoire \\(T(X_1, \\dotsc, X_n)\\).\nCe qui ne se voit pas dans la définition, c’est qu’une bonne statistique devrait être facilement calculable ; à la place de statistique, on peut penser à algorithme : une bonne statistique doit pouvoir être calculée facilement par un algorithme ne prenant en entrée que les échantillons \\(x_i\\).\n\nSi le but est de deviner la valeur de \\(\\theta\\) à partir des observations, il est naturel de considérer des statistiques à valeurs dans \\(\\Theta\\). C’est précisément la définition d’un estimateur.\n\nDéfinition 1.4 Dans le modèle \\((\\mathcal{X},\\mathcal{A}, (P_\\theta)_{\\theta \\in \\Theta})\\), un estimateur de \\(\\theta\\) est une statistique à valeurs dans \\(\\Theta\\).\n\nEn fait, on n’est pas obligés de vouloir estimer précisément \\(\\theta\\). Peut-être qu’on veut estimer quelque chose qui dépend de \\(\\theta\\), mais qui n’est pas \\(\\theta\\) ; disons, une fonction \\(\\varphi(\\theta)\\). Dans ce cas, un estimateur de \\(\\varphi(\\theta)\\) sera simplement une statistique à valeurs dans l’espace où vit \\(\\varphi(\\theta)\\)."
  },
  {
    "objectID": "ch1.html#points-de-vue",
    "href": "ch1.html#points-de-vue",
    "title": "1  Introduction",
    "section": "1.4 Points de vue",
    "text": "1.4 Points de vue\nInférence paramétrique. La plupart des expériences/modèles statistiques que nous rencontrerons dans ce cours, seront de nature dite paramétrique, autrement dit indexés par des parties de \\(\\mathbb{R}^d\\). Le mot “paramètre” est en lui-même trompeur : on parle souvent de paramètre d’une distribution pour désigner ce qui devrait plutôt s’appeler une fonctionnelle. Par exemple, la moyenne, la covariance d’une distribution sur \\(\\mathbb{R}^d\\) sont des paramètres de cette distribution. Les quantiles, l’asymétrie, la kurtosis sont d’autres paramètres.\nStatistique non paramétrique. Tous les modèles ne sont pas paramétriques au sens ci-dessus : dans de nombreux développements des statistiques, par exemple en estimation de densité, on travaille sur des modèles plus riches qui n’admettent pas de paramétrisation naturelle par une partie d’un espace euclidien de dimension finie. C’est ce qu’on appelle l’ estimation non-paramétrique. Nous y reviendrons au dernier chapitre.\nStatistique bayésienne. En statistique paramétrique, les paramètres \\(\\theta\\) déterminent le hasard qui génère les observations \\(x_i\\). La statistique bayésienne consiste à renverser le point de vue, et à rendre le paramètre \\(\\theta\\) lui-même aléatoire ; sa loi, appelée prior, mesure “le degré de connaissance a priori” qu’on en a. La règle de Bayes explique comment cette loi est modifiée par les observations. C’est un point de vue qui ne sera pas abordé dans ce cours."
  },
  {
    "objectID": "ch2.html#précision-dun-estimateur",
    "href": "ch2.html#précision-dun-estimateur",
    "title": "2  Estimation de paramètre",
    "section": "2.1 Précision d’un estimateur",
    "text": "2.1 Précision d’un estimateur\n\nDéfinition 2.1 (Biais , risque quadratique)  \n\nLe biais de \\(\\hat{\\theta}\\) est la quantité \\(\\mathbb{E}_\\theta[\\hat\\theta - \\theta]\\). L’estimateur est dit sans biais s’il est de biais nul.\nLe risque quadratique de \\(\\hat\\theta\\) est la quantité \\(\\mathbb{E}_{\\theta}[ |\\hat{\\theta}- \\theta|^2]\\).\n\n\nEn pratique, on peut vouloir estimer non pas \\(\\theta\\) lui-même, mais un paramètre \\(\\psi = \\psi_\\theta\\) qui dépend de \\(\\theta\\), comme \\(\\cos(\\theta)\\) ou \\(|\\theta|\\) par exemple. Dans ce cas, si \\(\\hat{\\psi}\\) est un estimateur de \\(\\psi\\) alors le biais est défini par \\(\\mathbb{E}_\\theta[\\hat{\\psi} - \\psi_\\theta]\\) et le risque quadratique par \\(\\mathbb{E}_\\theta [ |\\hat\\psi - \\psi_\\theta|^2]\\).\nLa dépendance du risque quadratique vis à vis de la taille de l’échantillon est une question importante : pour une suite d’expériences donnée, quelles sont les meilleures vitesses envisageables, et comment les obtenir ?\n\nThéorème 2.1 (Décomposition biais-variance) \\[\n\\mathbb{E}_{\\theta} [|\\hat{\\theta}-\\theta|^2]\n= \\underbrace{\\operatorname{Var}_{\\theta} (\\hat{\\theta})}_{\\text{variance}} +\n\\underbrace{\\mathbb{E}_{\\theta}[\\hat{\\theta}-\\theta]^2}_{\\text{carré du biais}} \\, .\n\\]"
  },
  {
    "objectID": "ch2.html#convergence",
    "href": "ch2.html#convergence",
    "title": "2  Estimation de paramètre",
    "section": "2.2 Convergence",
    "text": "2.2 Convergence\nRappelons brièvement deux notions de convergence des variables aléatoires. Une suite de variables aléatoires \\(X_n\\) à valeurs dans \\(\\mathbb{R}^d\\) converge en probabilité vers une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb{R}^d\\) si pour tout \\(\\varepsilon &gt;0\\), \\[\n\\lim_{n\\to\\infty} \\mathbb{P} (| X_n -X|&gt; \\varepsilon ) = 0 \\, .\n\\]\n\nDéfinition 2.2 (consistance d’un estimateur) Une suite d’estimateurs \\((\\widehat{\\theta}_n)\\) est convergente pour l’estimation de \\(\\theta\\) lorsque, pour tout \\(\\theta \\in \\Theta\\), sous \\(P_\\theta\\), la suite \\((\\hat{\\theta}_n)\\) converge en probabilité vers \\(\\theta\\) ; autrement dit, lorsque \\[ \\forall \\varepsilon&gt;0, \\qquad \\lim_n     P_\\theta ( | \\widehat{\\theta}_n-\\theta| &gt; \\varepsilon ) =0.\n\\] La suite est fortement convergente si, pour tout \\(\\theta\\), la convergence a lieu \\(P_\\theta\\)-presque sûrement. $$\n\nOn voit parfois le mot consistant utilisé au lieu de convergent. Je pense que c’est un anglicisme."
  },
  {
    "objectID": "ch2.html#normalité-asymptotique",
    "href": "ch2.html#normalité-asymptotique",
    "title": "2  Estimation de paramètre",
    "section": "2.3 Normalité asymptotique",
    "text": "2.3 Normalité asymptotique\nLorsqu’un estimateur est convergent, on peut se demander à quoi ressemblent ses fluctuations autour de sa valeur limite. Le théorème central limite indique que le comportement asymptotique gaussiens est relativement fréquent, et beaucoup d’estimateurs sont des sommes de réalisations de variables indépendantes.\n\nDéfinition 2.3 (normalité asymptotique) Soit \\(\\theta\\) un paramètre à estimer, et \\(\\hat{\\theta}_n\\) une suite d’estimateurs de \\(\\theta\\). On dit que ces estimateurs sont asymptotiquement gaussiens (ou normaux) si, après les avoir renormalisés convenablement, ils convergent en loi vers une loi gaussienne. Autrement dit, s’il existe une suite \\(a_n\\) de nombres réels tels que \\[ a_n(\\hat{\\theta}_n - \\theta) \\xrightarrow[n\\to \\infty]{\\text{loi}} N(0,\\Sigma)\\] où \\(\\Sigma\\) est une matrice de covariance qui dépend peut-être de \\(\\theta\\) — pour éviter les cas dégénérés, on demande à ce que \\(\\Sigma\\) soit non-nulle."
  },
  {
    "objectID": "ch2.html#trois-outils-sur-la-normalité-asymptotique",
    "href": "ch2.html#trois-outils-sur-la-normalité-asymptotique",
    "title": "2  Estimation de paramètre",
    "section": "2.4 Trois outils sur la normalité asymptotique",
    "text": "2.4 Trois outils sur la normalité asymptotique\nLa normalité asymptotique n’est pas intéressante en elle-même ; l’idée est plutôt de chercher le comportement asymptotique de la statistique recentrée pour pouvoir en déduire des garanties en terme de risque asymptotique ou d’intervalle de confiance. Nous utiliserons cela de nombreuses fois dans la suite ; la normalité asymptotique sera par exemple utilisée dans la construction des intervalles de confiance. Aussi, prouver que des estimateurs sont asymptotiquement normaux est une tache importante, qui est grandement simplifiée par les deux outils suivants. On commence par rappeler le théorème centra-limite.\n\nThéorème 2.2 (Théorème Central-Limite) Soit \\((X_i)\\) une suite de variables aléatoires réelles, indépendantes et identiquement distribuées. On suppose que ces variables ont une variance \\(\\sigma^2\\) finie. Alors, la variable aléatoire \\[ \\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i - \\mathbb{E}[X]\\right)\\] converge en loi vers une loi \\(N(0,\\sigma^2)\\).\n\nLe Lemme de Slutsky sera fréquemment utilisé pour combiner convergence en loi et convergence en probabilité.\n\nThéorème 2.3 (Lemme de Slutsky) Soit \\((X_n)\\) une suite de variables aléatoire qui converge en loi vers \\(X\\) et \\((Y_n)\\) une suite de variables aléatoires qui converge en probabilité (ou en loi) vers une constante \\(c\\). Alors, le couple \\((X_n, Y_n)\\) converge en loi vers \\((X,c)\\) ; autrement dit, pour toute fonction continue bornée \\(\\varphi\\), \\[\\mathbb{E}[\\varphi(X_n, Y_n)] \\to \\mathbb{E}[\\varphi(X,c)].\\]\n\n\nPreuve. Fixons une fonction test \\(\\varphi\\) continue à support compact, donc bornée par un certain \\(M\\). Il faut montrer que \\(\\mathbb{E}[\\varphi(X_n, Y_n) - \\varphi(X,c)]\\) tend vers zéro. L’intégrande est égal à la somme de \\(A = \\varphi(X_n, Y_n) - \\varphi(X_n, c)\\) et de \\(B=\\varphi(X_n, c) - \\varphi(X, c)\\).\nComme \\(X_n\\) tend en loi vers \\(X\\) et que \\(t\\to \\varphi(t,c)\\) est continue bornée, l’espérance de \\(B\\) tend vers zéro. Il faut donc montrer que l’espérance de \\(A\\) tend vers zéro. On fixe un \\(\\varepsilon&gt;0\\).\n\nPar le théorème de Heine, \\(\\varphi\\) est uniformément continue : il existe \\(\\delta&gt;0\\) tel que \\(|(x,y) - (x', y')|&lt;\\delta\\) entraîne que \\(|\\varphi(x,y) - \\varphi(x', y')|&lt; \\varepsilon/2\\).\nOn introduit l’événement \\(\\{|Y_n - c|\\leqslant \\delta\\}\\). Par le point précédent, sur cet événement on a \\(|A| &lt; \\varepsilon/2\\). Hors de cet événement, on peut toujours borner \\(|A|\\) par \\(2M\\). On a donc \\[|\\mathbb{E}A| \\leqslant \\mathbb{P}(|Y_n - c|\\leqslant \\delta)\\varepsilon/2 +  \\mathbb{P}(|Y_n - c| &gt; \\delta)2M.\\]\nComme \\(Y_n\\) converge en probabilité vers \\(c\\), lorsque \\(n\\) est assez grand on a \\(\\mathbb{P}(|Y_n - c| &gt; \\delta) &lt; \\varepsilon/4M\\).\nEn regroupant tout ce qui a été dit, on obtient bien \\(|\\mathbb{E}A| \\leqslant \\varepsilon\\) dès que \\(n\\) est assez grand, ce qui montre bien que \\(\\mathbb{E}A \\to 0\\).\n\n\n\n\nThéorème 2.4 (Delta-méthode) Soit \\((X_n)\\) une suite de variables aléatoires réelles telle que \\(\\sqrt{n}(X_n - \\alpha)\\) converge en loi vers \\(N(0,\\sigma^2)\\). Pour toute fonction \\(g : \\mathbb{R} \\to \\mathbb{R}\\) dérivable en \\(\\alpha\\) (de dérivée non nulle en \\(\\alpha\\)), on a \\[ \\sqrt{n}(g(X_n) - g(\\alpha)) \\xrightarrow[n \\to \\infty]{\\text{loi}} N(0, g'(\\alpha)^2 \\sigma^2).\\]\n\nPlus généralement, si les \\(X_n\\) sont à valeurs dans \\(\\mathbb{R}^d\\) et que \\(\\sqrt{n}(X_n - \\alpha) \\to N(0,\\Sigma)\\), alors pour toute application \\(g:\\mathbb{R}^d \\to \\mathbb{R}^k\\), la suite \\(\\sqrt{n}(g(X_n) - g(\\alpha))\\) converge en loi vers \\[N(0, Dg(\\alpha)\\Sigma Dg(\\alpha)^\\top)\\] où \\(Dg(x)\\) est la matrice jacobienne de \\(g\\) en \\(x\\).\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch2.html#deux-estimateurs-importants",
    "href": "ch2.html#deux-estimateurs-importants",
    "title": "2  Estimation de paramètre",
    "section": "2.5 Deux estimateurs importants",
    "text": "2.5 Deux estimateurs importants\nDeux estimateurs sont omniprésents en statistique : la moyenne empirique et la variance empirique. Ils sont pertinents dans n’importe quel modèle où les observations sont des réalisations de variables iid possédant une moyenne \\(\\mu\\) et une variance \\(\\sigma^2\\).\nLa moyenne empirique est définie par \\[ \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\\] Il est évident que \\(\\mathbb{E}[\\bar{X}_n] = \\mathbb{E}[X] = \\mu\\). Cet estimateur est donc toujours sans biais, et son risque quadratique est égal à sa variance, c’est-à-dire \\(\\frac{\\sigma^2}{n}\\).\nL’estimateur de la variance empirique est défini comme \\[\\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2.\\]\n\nThéorème 2.5 L’estimateur \\(\\hat{\\sigma}_n^2\\) est sans biais.\n\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch2_1.html#quest-ce-quun-moment",
    "href": "ch2_1.html#quest-ce-quun-moment",
    "title": "3  La méthode des moments",
    "section": "3.1 Qu’est-ce qu’un moment ?",
    "text": "3.1 Qu’est-ce qu’un moment ?\nDans un modèle statistique, supposons qu’on dispose d’une statistiques intégrable \\(T\\) (pas forcément réelle), dont la moyenne n’est pas le paramètre \\(\\theta\\) lui-même, mais plutôt une fonction de \\(\\theta\\) :\n\\[\\mathbb{E}_\\theta[T(X)] = \\varphi(\\theta).\\] C’est cette fonction \\(\\varphi\\) qu’on appelle moment. Typiquement,\n\nla moyenne d’une loi \\(\\mathscr{E}(\\theta)\\) n’est pas \\(\\theta\\) mais \\(1/\\theta\\).\nla moyenne d’une loi log-normale de paramètres \\((0, \\sigma^2)\\) est \\(e^{\\sigma^2/2}\\).\n\nPrenons la moyenne empirique associée à cet estimateur, \\(\\bar{T}_n\\). Par la loi des grands nombres, \\[\\bar{T}_n = \\frac{1}{n}\\sum_{i=1}^n T(X_i) \\to \\varphi(\\theta) \\qquad P_\\theta-ps, \\] ce qui permet d’estimer \\(\\varphi(\\theta)\\). Peut-on alors estimer \\(\\theta\\) ?"
  },
  {
    "objectID": "ch2_1.html#estimateur-des-moments",
    "href": "ch2_1.html#estimateur-des-moments",
    "title": "3  La méthode des moments",
    "section": "3.2 Estimateur des moments",
    "text": "3.2 Estimateur des moments\nSi la fonction \\(\\varphi\\) est inversible et si \\(\\bar{T}_n\\) appartient presque sûrement à l’ensemble de définition de \\(\\varphi^{-1}\\), alors \\(\\varphi^{-1}(\\bar{T_n})\\) est bien définie. Pour qu’en plus cette quantité converge presque sûrement vers \\(\\theta\\), il faut s’assurer que \\(\\varphi^{-1}\\) est continue. C’est par exemple le cas lorsque l’ensemble des paramètres \\(\\Theta\\) est un ouvert, et que \\(\\varphi\\) est un difféomorphisme sur son image — une situation si fréquente qu’elle mérite son propre théorème, et si agréable qu’elle garantit que l’estimateur associé est asymptotiquement normal.\n\nThéorème 3.1 (Estimation par moments) Sous l’hypothèse mentionnée ci-dessus (la fonction \\(\\varphi\\) est un difféomorphisme), l’estimateur \\[\\hat{\\theta}_n = \\varphi^{-1}(\\bar{T}_n)\\] est presque sûrement bien défini pour tout \\(n\\) suffisamment grand ; il est également consistant pour l’estimation de \\(\\theta\\). En outre, si \\(T\\) est de carré intégrable, cet estimateur est asymptotiquement normal, au sens où \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta)\\) converge en loi vers une gaussienne centrée de matrice de variance \\[ (D\\varphi(\\theta))^{-1}\\mathrm{Var}_\\theta(T)(D\\varphi(\\theta)^\\top)^{-1}.\\]\n\n\nPreuve. La première partie a essentiellement été démontrée un peu plus haut. Pour la seconde, il faut d’abord remarquer que si \\(T\\) est de carré intégrable, alors \\(\\sqrt{n}(\\bar{T}_n - \\varphi(\\theta))\\) converge vers une loi \\(N(0, \\mathrm{Var}_\\theta(T))\\) par le TCL. Une simple application de la delta-méthode (Théorème 2.4) donne alors le résultat, puisque la matrice jacobienne de \\(\\varphi^{-1}\\) en \\(\\varphi(\\theta)\\) n’est autre que l’inverse de la matrice jacobienne de \\(\\varphi\\) en \\(\\theta\\)."
  },
  {
    "objectID": "ch2_ex.html#questions",
    "href": "ch2_ex.html#questions",
    "title": "٭ Exercices",
    "section": "Questions",
    "text": "Questions\n\nMontrer que la convergence en loi vers une constante implique la convergence en probabilité.\nMontrer que, si un modèle statistique n’est pas identifiable, alors il ne peut exister aucun estimateur convergent.\nTrouver un couple de variables aléatoires \\((X_n, Y_n)\\) tel que \\(X_n\\) converge en loi et \\(Y_n\\) converge en loi, mais le couple ne converge pas en loi.\nOn observe un échantillon de lois de Poisson de paramètre \\(\\lambda\\), que l’on estime par la moyenne empirique. Calculer le risque quadratique de cet estimateur.\nQuelle est la loi d’une somme de lois de Bernoulli indépendantes ? L’écart-type ?"
  },
  {
    "objectID": "ch2_ex.html#exercices",
    "href": "ch2_ex.html#exercices",
    "title": "٭ Exercices",
    "section": "Exercices",
    "text": "Exercices\n\nExercice 1 (Variance empirique) On se donne \\(Y_1, \\dots, Y_n\\), i.i.d de moyenne \\(\\mu\\) et variance \\(\\sigma^2\\).\n\nOn suppose \\(\\mu\\) connu. Donner un estimateur non biaisé de \\(\\sigma^2\\).\nOn suppose \\(\\mu\\) inconnu. Calculer l’espérance de \\(\\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2\\). En déduire un estimateur non biaisé de \\(\\sigma^2\\).\n\n\n\nExercice 2 (Estimation de masse) Au cours de la seconde guerre mondiale, l’armée alliée notait les numéros de série \\(X_1, \\dots, X_n\\) de tous les tanks nazis capturés ou détruits, afin d’obtenir un estimateur du nombre total \\(N\\) de tanks produits.\n\nProposer un modèle pour le tirage de \\(X_1, \\dots, X_n\\).\nCalculer l’espérance de \\(\\bar X_n\\). En déduire un estimateur non biaisé de \\(N\\). Indication: la loi de \\(n\\) tirages sans remise est échangeable.\nÉtudier la loi de \\(X_{(n)}\\) et en déduire un estimateur non biaisé de \\(N\\).\nProposer deux intervalles de confiance de niveau \\(1-\\alpha\\) de la forme \\([aS, bS]\\) avec \\(a, b\\in\\mathbb{R}\\) et \\(S\\) une statistique. On pourra utiliser le fait que l’inégalité de Hoeffding s’applique également aux tirages sans remise.\n\nSelon Ruggles et Broodie (1947, JASA), la méthode statistique a fourni comme estimation une production moyenne de 246 tanks/mois entre juin 1940 et septembre 1942. Des méthodes d’espionnage traditionnelles donnaient une estimation de 1400 tanks/mois. Les chiffres officiels du ministère nazi des Armements ont montré après la guerre que la production moyenne était de 245 tanks/mois.\n\n\nExercice 3 (Lois uniformes (1)) On considère \\((X_1, \\dots, X_n)\\) un échantillon de loi uniforme sur \\(]\\theta, \\theta+1[\\).\n\nDonner la densité de la loi de la variable \\(R_n=X_{(n)} -X_{(1)}\\), où \\(X_{(1)}=\\min(X_1, \\dots, X_n)\\) et \\(X_{(n)}=\\max(X_1, \\dots, X_n)\\).\nÉtudier les différents modes de convergence de \\(R_n\\) quand \\(n\\to\\infty\\).\nÉtudier le comportement en loi de \\(n(1-R_n)\\) quand \\(n\\to\\infty\\).\n\n\n\nExercice 4 (Lois uniformes (2)) Soit \\(X_1,\\dots,X_n\\) un échantillon de loi \\(\\mathscr{U}([0,\\theta])\\), avec \\(\\theta &gt;0\\). On veut estimer \\(\\theta\\).\n\nDéterminer un estimateur de \\(\\theta\\) à partir de \\(\\bar{X}_n\\).\nOn considère l’estimateur \\(X_{(n)}= {\\max}_{1\\leq i \\leq n}X_i\\). Déterminer les propriétés asymotptiques de ces estimateurs.\nComparer les performances des deux estimateurs.\n\n\n\nExercice 5 (Lois Gamma) La loi Gamma \\(\\Gamma(\\alpha, \\beta)\\) de paramètres \\(\\alpha, \\beta&gt;0\\) a pour densité \\[ x\\mapsto \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x), \\quad x&gt;0.\\] On se donne un échantillon \\((X_1,\\dots,X_n)\\) de loi \\(\\Gamma(\\alpha, \\beta)\\) et on chercche à estimer les paramètres.\n\nOn suppose le paramètre \\(\\beta\\) connu. Proposer un estimateur de \\(\\alpha\\) par la méthode des moments.\nOn suppose à présent que les deux paramètres \\(\\alpha, \\beta\\) sont inconnus. Proposer un estimateur de \\((\\alpha,\\beta)\\) par la méthode des moments.\n\n\n\nExercice 6 (Lois de Gumbel) La loi de Gumbel (centrale) de paramètre \\(\\beta\\) a pour fonction de répartition \\(F(x)= e^{-e^{-x/\\beta}}\\). On observe un échantillon de lois de Gumbel et l’on cherche à estimer \\(\\beta\\).\n\nCalculer la densité des lois de Gumbel, ainsi que leur moyenne et variance [indice : \\(0.57721…\\)]\nEn déduire un estimateur convergent dont on calculera le risque quadratique et les propriétés asymptotiques.\n\n\n\nExercice 7 (Lois de Yule-Simon) Une variable aléatoire \\(X\\) suit la loi de Yule-Simon de paramètre \\(\\rho&gt;0\\) lorsque \\(\\mathbb{P}(X = n) = \\rho B(n, 1+\\rho)\\), où \\(n\\geqslant 1\\) et \\(B\\) est la fonction beta.\n\nMontrer que si \\(\\rho&gt;1\\), alors \\(\\mathbb{E}[X] = \\rho/(\\rho-1)\\).\nTrouver un estimateur de \\(\\rho\\) et donner ses propriétés."
  },
  {
    "objectID": "ch3.html#principe",
    "href": "ch3.html#principe",
    "title": "4  Intervalles de confiance",
    "section": "4.1 Principe",
    "text": "4.1 Principe\nDans un modèle statistique, l’estimation du paramètre d’intérêt \\(\\theta\\) par intervalles de confiance consiste à spécifier un intervalle calculable à partir des données, et qui contient \\(\\theta\\) avec grande probabilité : en d’autres termes, une région de confiance pour \\(\\theta\\).\nPour simplifier, on supposera d’abord que \\(\\theta\\) est un paramètre réel.\n\nDéfinition 4.1 (intervalle de confiance) Un intervalle de confiance de niveau \\(1-\\alpha\\) est un intervalle \\(I = [A,B]\\) dont les bornes \\(A,B\\) sont des statistiques, et tel que pour tout \\(\\theta\\), \\[P_\\theta(\\theta \\in I) \\geqslant 1 - \\alpha.\\] Un intervalle de confiance de niveau asymptotique \\(1-\\alpha\\) est une suite d’intervalles \\(I_n = [A_n,B_n]\\) dont les bornes \\(A_n,B_n\\) sont des statistiques, et tels que pour tout \\(n\\), \\[ P_\\theta(\\theta \\in I_n) \\geqslant 1 - \\alpha.\\]\n\nLe terme « niveau » désigne \\(1-\\alpha\\) ; la vocation de ce nombre est d’être proche de 1, typiquement 99%. Le nombre \\(\\alpha\\) est parfois appelé « erreur », « marge d’erreur » ou encore « niveau de risque » ; la vocation de ce nombre est d’être proche de zéro, typiquement 1%.\nIl n’y a rien d’autre à savoir sur les intervalles de confiance ; tout l’art de la chose consiste à savoir les construire. Commençons par des exemples essentiels à plusieurs titres : le cas d’un échantillon gaussien, et le cas de lois de Bernoulli."
  },
  {
    "objectID": "ch3.html#exemples-gaussiens",
    "href": "ch3.html#exemples-gaussiens",
    "title": "4  Intervalles de confiance",
    "section": "4.2 Exemples gaussiens",
    "text": "4.2 Exemples gaussiens\nOn dispose de variables aléatoires \\(X_1, \\dotsc, X_n\\) de loi \\(N(\\mu, \\sigma^2)\\). On va donner des intervalles de confiance pour l’estimation des paramètres \\(\\mu\\) et \\(\\sigma\\) dans plusieurs cas de figure.\n\n4.2.1 Estimation de \\(\\mu\\)\nLorsque \\(\\sigma\\) est connue. \nNous avons déjà vu que la moyenne empirique \\(\\bar{X}_n\\) est un estimateur sans biais de \\(\\mu\\). Or, nous savons aussi la loi exacte de \\(\\bar{X}_n\\), qui est \\(N(\\mu, \\sigma^2/n)\\). Autrement dit, \\[\\frac{\\sqrt{n}}{\\sigma}(\\bar{X}_n - \\mu) \\sim N(0,1). \\tag{4.1}\\]\nDans cette équation, on a trouvé une variable aléatoire dont la loi ne dépend plus de \\(\\mu\\). Il est donc possible de déterminer un intervalle dans lequel elle fluctue à l’aide des quantiles de la loi normale, qui sont étudiés dans Section 5.1. Si l’on se donne une marge d’erreur \\(\\alpha = 1\\%\\), alors \\[ \\mathbb{P}( (\\sqrt{n}/\\sigma)|\\bar{X}_n - \\mu| &gt; z_{0.99}) = 1\\%\\] où \\(z_{0.99} \\approx 2.32\\). Or, \\[ \\frac{\\sqrt{n}}{\\sigma}|\\bar{X}_n - \\mu| &gt; z_{0.99} \\tag{4.2}\\] est équivalent à \\[ \\bar{X}_n - \\frac{z_{0.99}\\sigma}{\\sqrt{n}} \\leqslant \\mu \\leqslant \\bar{X}_n + \\frac{z_{0.99}\\sigma}{\\sqrt{n}}. \\tag{4.3}\\] Le passage de Équation 4.2 à Équation 4.3 est souvent appelé pivot et sert à passer d’un intervalle de fluctuation à un intervalle de confiance.\nNous avons donc les deux bornes de notre intervalle de confiance : \\[ A = \\bar{X}_n - \\frac{z_{0.99}\\sigma}{\\sqrt{n}}\\] \\[ B = \\bar{X}_n + \\frac{z_{0.99}\\sigma}{\\sqrt{n}} .\\] Ces deux quantités sont bien des statistiques, car \\(\\sigma\\) est connu. De plus, nous venons de montrer que \\(P_\\mu(\\mu \\in [A,B]) = 99\\%\\). Ici, le choix de la marge d’erreur \\(\\alpha = 1\\%\\) ne jouait aucun rôle particulier ; ainsi, un intervalle de confiance de niveau \\(1-\\alpha\\) pour l’estimation de \\(\\mu\\) est donné par \\[\\left[\\bar{X}_n - \\frac{z_{1-\\alpha}\\sigma}{\\sqrt{n}}~~;~~\\bar{X}_n + \\frac{z_{1-\\alpha}\\sigma}{\\sqrt{n}} \\right]. \\tag{4.4}\\]\nLorsque \\(\\sigma\\) est inconnue. \nLorsque \\(\\sigma\\) n’est pas connu, les bornes \\(A,B\\) ci-dessus ne sont pas des statistiques, car elles dépendent de \\(\\sigma\\). Heureusement, on peut estimer \\(\\sigma\\) sans biais via l’estimateur \\[ \\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2.\\] Que se passe-t-il si, dans la statistique Équation 4.1, on remplace \\(\\sigma\\) par son estimation \\(\\hat{\\sigma}_n^2\\) ? On obtient la statistique dite de Student, \\[T_n = \\frac{\\sqrt{n}}{\\sqrt{\\hat{\\sigma}_n^2}}(\\bar{X}_n - \\mu). \\tag{4.5}\\] Sa loi n’est plus une loi gaussienne, mais une loi de Student à \\(n-1\\) paramètres de liberté \\(\\mathscr{T}(n-1)\\): le calcul de la densité est fait en détails dans Section 5.2.3 - Section 5.2.4. Les quantiles des lois de Student ont été calculés avec précision. On notera \\(t_{k,\\alpha}\\) le quantile symétrique de niveau \\(\\alpha\\) de \\(\\mathscr{T}(k)\\). Alors, \\[ P_{\\mu, \\sigma^2}(|T_n|&gt; t_{n-1,\\alpha})\\leqslant \\alpha .\\] Par le même raisonnement que tout à l’heure, l’inégalité \\[ \\left|\\frac{\\sqrt{n}}{\\hat{\\sigma}^2_n}(\\bar{X}_n - \\mu)\\right| &gt; t_{n-1,\\alpha}\\] est équivalente à \\[ \\bar{X}_n - \\frac{t_{n-1,\\alpha}\\hat{\\sigma}^2_n}{\\sqrt{n}} \\leqslant \\mu \\leqslant \\bar{X}_n + \\frac{t_{n-1,\\alpha}\\hat{\\sigma}^2_n}{\\sqrt{n}}.\\] et les deux côtés de ces inégalités sont des statistiques; en les notant \\(A,B\\), on a bien trouvé un intervalle de confiance de niveau \\(\\alpha\\), c’est-à-dire tel que \\(P_{\\mu,\\sigma^2}(\\mu \\in [A,B]) = \\alpha\\). Cet intervalle de confiance est d’une grande importance en pratique et mérite son propre théorème. Il est dû à William Gosset.\n\nThéorème 4.1 (Intervalle de Student) Un intervalle de confiance de niveau \\(1-\\alpha\\) pour l’estimation de \\(\\mu\\) lorsque \\(\\sigma\\) n’est pas connue est donné par\n\\[\\left[\\bar{X}_n - \\frac{t_{n-1, 1-\\alpha}\\hat{\\sigma}_n}{\\sqrt{n}}~~;~~\\bar{X}_n + \\frac{t_{n-1, 1-\\alpha}\\hat{\\sigma}_n}{\\sqrt{n}} \\right].\\]\n\n\n\n4.2.2 Estimation de \\(\\sigma\\)\nSupposons maintenant qu’on désire estimer la variance \\(\\sigma^2\\).\nLorsque \\(\\mu\\) est connue.\nEn supposant que \\(\\mu\\) est connue, l’estimateur des moments le plus naturel pour estimer \\(\\sigma^2\\) est évidemment \\[ \\tilde{\\sigma}^2_n = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2.\\] Comme les \\((X_i - \\mu)/\\sigma\\) sont des variables aléatoires gaussiennes centrées réduites, l’estimateur \\(\\tilde{\\sigma}^2_n \\times (n/ \\sigma^2)\\) est une somme de \\(n\\) gaussiennes standard indépendantes. La loi de cette statistique est connue : c’est une loi du chi-deux à \\(n\\) paramètres de liberté comme démontré dans Section 5.2.2. Cette loi n’est pas symétrique, puisqu’elle est supportée sur \\([0,\\infty[\\). On note souvent \\(k^-_{n,\\alpha}\\) et \\(k^+_{n,\\alpha}\\) les nombres les plus éloignées possible (ils exisent) tels que \\(\\mathbb{P}(k^-_{n,\\alpha}&lt; \\chi^2(n)&lt;k^+_{n,\\alpha}) = 1-\\alpha\\). Ainsi, \\[P_{\\sigma^2}(k^-_{n,\\alpha}&lt; \\frac{n \\tilde{\\sigma}^2_n}{\\sigma^2} &lt; k^+_{n,\\alpha}) = \\alpha.\\] En pivotant comme dans les exemples précédents, on obtient que l’intervalle \\[\\left[\\frac{n\\tilde{\\sigma}_n^2}{k^{+}_{n,\\alpha}} ~~;~~ \\frac{n\\tilde{\\sigma}_n^2}{k^-_{n,\\alpha}} \\right] \\] est un intervalle de confiance de niveau \\(\\alpha\\) pour \\(\\sigma^2\\).\nLorsque \\(\\mu\\) est inconnue.\nCette fois, on utilise l’estimateur déjà évoqué plus tôt, à savoir \\[ \\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2.\\] La loi de \\((n-1)\\hat{\\sigma}^2_n / \\sigma^2\\) est encore une loi du chi-deux, mais à \\(n-1\\) paramètres de liberté. Ainsi, le même raisonnement que ci-dessus donne l’intervalle de confiance de niveau \\(\\alpha\\) suivant :  \\[\\left[\\frac{(n-1)\\hat{\\sigma}_n^2}{k^+_{n-1,\\alpha}} ~~;~~ \\frac{(n-1)\\hat{\\sigma}_n^2}{k^-_{n-1,\\alpha}} \\right]. \\]"
  },
  {
    "objectID": "ch3.html#exemples-asymptotiques",
    "href": "ch3.html#exemples-asymptotiques",
    "title": "4  Intervalles de confiance",
    "section": "4.3 Exemples asymptotiques",
    "text": "4.3 Exemples asymptotiques\n\n4.3.1 Estimation du paramètre \\(p\\) dans un modèle de Bernoulli.\nSoient \\(X_1, \\dotsc, X_n\\) des variables indépendantes de loi \\(\\mathscr{B}(p)\\), dont on cherche à estimer le paramètre \\(p\\in ]0,1[\\). Un estimateur naturel est donné par la moyenne empirique, \\(\\hat{p}_n = (X_1 + \\dotsb + X_n)/n\\). Cet estimateur est non biaisé et son risque quadratique est égal à \\(p(1-p)/n\\). De plus, la loi de \\(\\hat{p}_n\\) est connue : \\(n\\hat{p}_n \\sim \\mathrm{Bin}(n,p)\\). Par conséquent, si l’on connaît les quantiles de \\(\\mathscr{Bin}(n,p)-p\\), on pourra construire des intervalles de confiance de niveau \\(1-\\alpha\\). Ces quantiles peuvent être calculés par des méthodes numériques, mais il existe des façons plus simples de faire.\nInégalité BT.  L’inégalité de Bienaymé-Tchebychev dit que \\[P_p(|\\hat{p}_n - p|&gt;t)\\leqslant \\frac{p(1-p)}{nt^2}.  \\tag{4.6}\\] Si l’on choisit \\[t = \\sqrt{\\frac{p(1-p)}{n\\alpha}},\\] cette probabilité est plus petite que \\(\\alpha\\). En pivotant, on en déduit que l’intervalle \\([\\hat{p_n} \\pm \\sqrt{p(1-p)/n\\alpha}]\\) contient \\(p\\) avec une probabilité supérieure à \\(1-\\alpha\\). Mais les bornes de cet intervalle ne sont pas des statistiques, car elles dépendent de \\(p\\) ! Fort heureusement, on sait que \\(p\\) est entre \\(0\\) et \\(1\\), ce qui entraîne que \\(p(1-p)\\) est plus petit que \\(1/4\\), donc l’intervalle ci-dessus est contenu dans l’intervalle plus grand \\[ \\left[\\hat{p}_n \\pm \\frac{1}{2\\sqrt{n\\alpha}}\\right]. \\] Ce dernier est bien un intervalle de confiance de niveau \\(1-\\alpha\\) pour l’estimation de \\(p\\).\nTCL.  On a mentionné que les quantiles des lois binomiales pourraient être calculés ; or, ils peuvent également être approchés grâce au théorème central-limite. Celui-ci dit que \\[ \\frac{\\sqrt{n}(\\hat{p}_n - p)}{\\sqrt{p(1-p)}} \\to N(0,1). \\tag{4.7}\\] Si \\(z_\\alpha\\) est le quantile symétrique d’ordre \\(\\alpha\\) de \\(N(0,1)\\), alors on en déduit que \\[\\mathbb{P}\\left(\\left|\\frac{\\sqrt{n}(\\hat{p}_n - p)}{\\sqrt{p(1-p)}} \\right|&gt;z_\\alpha \\right) \\to \\alpha. \\] En pivotant, on voit alors que l’intervalle \\[\\left[\\hat{p}_n \\pm z_\\alpha \\sqrt{p(1-p)/n}\\right] \\] contient \\(p\\) avec une probabilité qui tend lorsque \\(n\\to\\infty\\) vers \\(1-\\alpha\\). Là encore, cet intervalle n’est pas un intervalle de confiance. On pourrait utiliser deux techniques.\n\nComme tout à l’heure, l’intervalle ci-dessus est contenu dans l’intervalle plus grand \\([\\hat{p}_n \\pm z_\\alpha/2\\sqrt{n}]\\) qui est un intervalle de confiance asymptotique de niveau \\(1-\\alpha\\).\nIl y a plus fin. Nous savons par la loi des grands nombres que \\(\\hat{p}_n \\to p\\) en probabilité. Ainsi, \\(\\sqrt{\\hat{p}_n(1-\\hat{p}_n)} \\to \\sqrt{p(1-p)}\\) en probabilité. Le lemme de Slutsky nous assure alors que dans Équation 4.8, on peut remplacer le dénominateur par \\(\\sqrt{\\hat{p}_n (1-\\hat{p}_n)}\\) pour obtenir \\[ \\frac{\\sqrt{n}(\\hat{p}_n - p)}{\\sqrt{\\hat{p}_n(1-\\hat{p}_n)}} \\to N(0,1). \\tag{4.8}\\] Le reste du raisonnement est identique, et l’on obtient l’intervalle de confiance asymptotique de niveau \\(1-\\alpha\\) suivant : \\[\\left[\\hat{p}_n \\pm z_\\alpha \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}\\right] \\]\n\nHoeffding. L’inégalité de Bienaymé-Tchebychev n’est pas très fine. Il existe de nombreuses autres inégalités de concentration : l’inégalité de Hoeffding (Théorème 5.4) concerne les variables bornées, comme ici où les \\(X_i\\) sont dans \\([0,1]\\) . Cette inégalité dit que \\[\\mathbb{P}(|\\hat{p}_n - p|&gt;t)\\leqslant 2 e^{-2nt^2}. \\] Le choix \\[ t = \\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{1}{\\alpha}\\right)}\\] donne une probabilité inférieure à \\(\\alpha\\), et fournit donc l’intervalle de confiance non-asymptotique de niveau \\(1-\\alpha\\) suivant :  \\[ \\left[\\bar{X}_n \\pm \\frac{\\ln(1/\\alpha)}{\\sqrt{2n}}\\right].\\]\n\n\n4.3.2 Estimation de moyenne dans un modèle non-gaussien.\nLes deux techniques ci-dessus n’ont rien de spécifique au cas de variables de Bernoulli. En fait, elles s’appliquent à tout modèle statistique iid dont on cherche à estimer la moyenne \\(\\mu\\), pourvu que la variance existe.\nLa première méthode utilisant Bienaymé-Tchebychev nécessite de borner la variance. Cela peut se faire dans certains cas, mais pas dans tous.\nLa seconde méthode s’applique systématiquement en utilisant l’estimateur de la variance empirique \\(\\hat{\\sigma}_n^2\\). En effet, la convergence \\[\\frac{\\sqrt{n}}{\\hat{\\sigma}_n}(\\bar{X}_n - \\mu) \\to N(0,1)\\] est toujours vraie d’après le théorème de Slutsky.\n\nThéorème 4.2 Soient \\(X_1, \\dotsc, X_n\\) des variables iid possédant une variance. L’intervalle \\[ \\left[ \\bar{X}_n \\pm \\frac{z_\\alpha \\hat{\\sigma}_n}{\\sqrt{n}} \\right]\\] est un intervalle de confiance asymptotique de niveau \\(\\alpha\\) pour l’estimation de la moyenne des \\(X_i\\)."
  },
  {
    "objectID": "ch31_outils.html#sec-quantiles",
    "href": "ch31_outils.html#sec-quantiles",
    "title": "5  Outils pour les IC",
    "section": "5.1 Quantiles",
    "text": "5.1 Quantiles\nSi \\(X\\) est une variable aléatoire sur \\(\\mathbb{R}\\), un quantile d’ordre \\(\\beta \\in ]0,1[\\), noté \\(q_\\beta\\), est un nombre tel que \\(\\mathbb{P}(X \\leqslant q_\\beta) = \\beta\\). Lorsque \\(X\\) est continue, un tel nombre existe forcément, car la fonction de répartition \\(F(x) = \\mathbb{P}(X\\leqslant x)\\) est une surjection continue. Les quantiles symétriques \\(z_\\beta\\) sont, eux, définis par \\(\\mathbb{P}(|X|\\leqslant z_\\beta) = \\beta\\).\nSi la loi de \\(X\\) est de surcroît symétrique, les quantiles symétriques s’expriment facilement en fonction des quantiles classiques. En effet, \\(\\mathbb{P}(|X|\\leqslant z)\\) est égal à \\(\\mathbb{P}(X \\leqslant z) - \\mathbb{P}(X \\leqslant -z)\\). Or, si la loi de \\(X\\) est symétrique, alors \\(\\mathbb{P}(X \\leqslant -z) = 1 - \\mathbb{P}(X \\leqslant z)\\), et donc \\[ \\mathbb{P}(|X|\\leqslant z) = 2\\mathbb{P}(X \\leqslant z) - 1.\\] Il suffit alors de choisir pour \\(z\\) le quantile \\(q_{\\frac{1+\\beta}{2}}\\) pour obtenir \\(\\mathbb{P}(|X|\\leqslant z) = \\beta\\). Lorsque \\(\\beta\\) est de la forme \\(1-\\alpha\\) avec \\(\\alpha\\) petit (comme les niveaux des intervalles de confiance), on trouve alors \\(z_{1-\\alpha} = q_{1 - \\alpha/2}\\).\nLes quantiles s’obtiennent en inversant la fonction de répartition : lorsque celle-ci est une bijection sur \\(]0,1[\\), alors \\(q_\\beta = F^{-1}(\\beta)\\). En règle générale, il n’y a pas de forme fermée. Par exemple, pour une loi gaussienne standard, \\[F(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-u^2/2}du\\] qui elle-même n’a pas d’écriture plus simple. Fort heureusement, les outils de calcul numérique permettent d’effectuer ces calculs avec une grande précision. La table suivante donne les quantiles symétriques de la gaussienne.\n\n\n\n\\(\\beta\\)\n90%\n95%\n98%\n99%\n99.9%\n99.99999%\n\n\n\n\n\\(z_\\beta\\)\n1.64\n1.96\n2.32\n2.57\n3.2\n5.32\n\n\n\nVoir aussi la règle 1-2-3. Il existe de nombreuses tables de quantiles pour les lois usuelles.\n\nThéorème 5.1 (Queues de distribution de la gaussienne) Si \\(x\\) est plus grand que 1, \\[  \\left(\\frac{1}{x} - \\frac{1}{x^3}\\right) \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}}\\leqslant \\mathbb{P}(X &gt; x) \\leqslant \\frac{1}{x}\\frac{e^{-x^2/2}}{\\sqrt{2\\pi}} \\] En particulier, si \\(x\\) est grand, \\(\\mathbb{P}(X \\geqslant x) \\sim e^{-x^2/2}/x\\sqrt{2\\pi}\\) avec une erreur d’ordre \\(O(e^{-x^2/2}/x^3)\\).\n\nÀ titre d’exemple, pour \\(x=2.32\\) cette approximation donne 98.83%, ce qui est remarquablement proche de 98%. Pour \\(x = 2.57\\) on trouve 99.42%.\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch31_outils.html#calculs-de-lois",
    "href": "ch31_outils.html#calculs-de-lois",
    "title": "5  Outils pour les IC",
    "section": "5.2 Calculs de lois",
    "text": "5.2 Calculs de lois\n\n5.2.1 Lois Gamma\nUne variable aléatoire suit une loi Gamma de paramètres \\(\\lambda&gt;0, \\alpha&gt;0\\) lorsque sa densité est donnée par \\[\\gamma_{r,\\alpha}(x) =  \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}e^{-\\lambda x}x^{\\alpha -1}\\mathbf{1}_{x&gt;0}.\\] Les lois Gamma rassemblent les lois exponentielles (\\(\\Gamma(\\lambda, 1) = \\mathscr{E}(\\lambda)\\)) et les lois du chi-deux qu’on verra ci-dessous \\((\\Gamma(1/2, n/2) = \\chi_2(n)\\)). La transformée de Fourier \\(\\varphi_{\\lambda, \\alpha}\\) d’une loi \\(\\Gamma(\\lambda, \\alpha)\\) se calcule facilement par un changement de variables : \\[\\varphi_{\\lambda, \\alpha}(t) = \\left(1 - \\frac{it}{\\lambda}\\right)^{-\\alpha}. \\tag{5.1}\\] Cette identité montre également que si \\(X_1, \\dotsc, X_n\\) sont des variables indépendantes de loi \\(\\Gamma(\\lambda, \\alpha_i)\\), alors leur somme est une variable de loi \\(\\Gamma(\\lambda, \\alpha_1 + \\dotsc + \\alpha_n)\\).\n\n\n5.2.2 Loi du chi-deux\nSoit \\(X\\) une loi gaussienne standard. Calculons la densité de \\(X^2\\) ; pour toute fonction-test \\(\\varphi\\), \\(\\mathbb{E}[\\varphi(X^2)]\\) est donné par \\[\\frac{1}{\\sqrt{2\\pi}}\\int e^{-x^2/2}\\varphi(x^2)dx.\\] Cette intégrale est symétrique, donc on peut ajouter un facteur 2 et intégrer sur \\([0,\\infty[\\). En posant \\(u=x^2\\), on obtient alors la valeur \\[ \\frac{2}{\\sqrt{2\\pi}}\\int_0^\\infty e^{-u/2}\\varphi(u)\\frac{1}{2\\sqrt{u}}du.\\] On reconnaît la densité d’une loi Gamma de paramètres \\((1/2, 1/2)\\). Cette loi s’appelle loi du chi-deux et on la note \\(\\chi_2(1)\\). Sa tranformée de Fourier est donnée par \\[\\mathbb{E}[e^{itX^2}] = \\frac{1}{\\sqrt{1 - 2it}}. \\]\nSoient maintenant \\(X_1,\\dotsc, X_n\\) des variables de loi \\(N(0,1)\\) indépendantes. Chaque \\(X_i^2\\) est une \\(\\chi_2(1)\\) ; leur somme a pour loi la convolée \\(n\\) fois de \\(\\chi_2(1)\\). Calculons sa tranformée de Fourier :  \\[\\begin{align}\\mathbb{E}[e^{it(X_1^2 + \\dotsb + X_n^2)}] &= \\mathbb{E}[e^{itX_1^2}]^n \\\\ &= (1-2it)^{-\\frac{n}{2}} .\\end{align}\\] On reconnaît la transformée de Fourier d’une loi \\(\\Gamma(n/2, 1/2)\\) ; cette loi s’appelle loi du chi-deux à \\(n\\) paramètres de liberté et elle est notée \\(\\chi_2(n)\\). Sa densité est donnée par \\[ \\frac{1}{2^{n/2}\\Gamma(n/2)}e^{-x/2}x^{n/2 - 1}\\mathbf{1}_{x&gt;0}. \\tag{5.2}\\]\n\n\n5.2.3 Loi de Student\nSoit \\(X\\) une variable de loi \\(N(0,1)\\) et \\(Y_n\\) une variable de loi \\(\\chi_2(n)\\) indépendante de \\(X\\). On va calculer la loi de \\(T_n = X/\\sqrt{Y_n/n}\\). Soit \\(\\varphi\\) une fonction test ; l’espérance \\(\\mathbb{E}[\\varphi(T_n)]\\) est égale à \\[\\frac{1}{Z_n\\sqrt{2\\pi}}\\int_0^\\infty \\int_{-\\infty}^{\\infty}  \\varphi\\left(\\frac{x}{\\sqrt{y/n}}\\right) e^{-\\frac{x^2}{2}}e^{-\\frac{y}{2}}y^{\\frac{n}{2} - 1}dxdy \\] où \\(Z_n = 2^{n/2}\\Gamma(n/2)\\). Dans l’intégrale en \\(x\\), on effectue le changement de variable \\(u = x/\\sqrt{y/n}\\) afin d’obtenir \\[\\frac{1}{Z_n\\sqrt{2\\pi}}\\int_0^\\infty \\int_{-\\infty}^{\\infty}  \\varphi(u) e^{-\\frac{yu^2}{2n}}e^{-\\frac{y}{2}}y^{\\frac{n}{2} - 1}\\sqrt{\\frac{y}{n}} dxdy. \\] La densité de \\(T_n\\) est donc donnée par \\[t_n(u)= \\frac{1}{Z_n\\sqrt{2\\pi n}}\\int_0^\\infty  e^{-\\frac{yu^2}{2n}-\\frac{y}{2}}y^{\\frac{n+1}{2}-1} dy. \\] Le changement de variables \\(z = y(1+u^2/n)/2\\) nous ramène à \\[t_n(u) = \\frac{1}{Z_n\\sqrt{2\\pi n}}\\left(\\frac{2}{1+\\frac{u^2}{n}}\\right)^{\\frac{n+1}{2}}\\int_0^\\infty  e^{-z}z^{\\frac{n+1}{2}- 1} dz.\\] On reconnaît \\(\\Gamma((n+1)/2)\\) à droite. La densité \\(t_n(x)\\) est donc \\[t_n(x) = \\frac{1}{\\sqrt{n\\pi}}\\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)}\\left(\\frac{1}{1 + \\frac{x^2}{n}}\\right)^{\\frac{n+1}{2}}.\\]\nCette loi s’appelle loi de Student de paramètre \\(n\\) ; on dit parfois à \\(n\\) degrés de liberté. Elle est notée \\(\\mathscr{T}(n)\\). La loi de Student de paramètre \\(n=1\\) est tout simplement une loi de Cauchy.\n\n\n5.2.4 Loi de la statistique de Student\nSoient \\(X_1, \\dotsc, X_n\\) des variables gaussiennes \\(N(\\mu, \\sigma^2)\\) indépendantes, et soit \\(T_n = (\\bar{X}_n-\\mu)/\\sqrt{\\hat{\\sigma}^2_n}\\), où \\[\\hat{\\sigma}^2_n = \\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}. \\]\n\nThéorème 5.2 \\[T_n \\sim \\mathscr{T}(n-1).\\]\n\n\\(~~\\)\n\nPreuve. On va montrer 1° que \\(\\bar{X}_n\\) et \\(\\sqrt{\\hat{\\sigma}^2_n / \\sigma^2}\\) sont indépendantes, et 2° que \\(\\sqrt{\\hat{\\sigma}^2_n / \\sigma^2}\\) a bien la même loi que \\(\\sqrt{Y_{n-1}/(n-1)}\\) où \\(Y_{n-1}\\) est une \\(\\chi_2(n-1)\\). Dans la suite, on supposera que \\(\\mu=0\\) et \\(\\sigma=1\\), ce qui n’enlève rien en généralité.\nPremier point.  Le vecteur \\(X=(X_1, \\dotsc, X_n)\\) est gaussien. Posons \\(Z = (X_1 - \\bar{X}_n, \\dotsc, X_n - \\bar{X}_n)\\). Le couple \\((\\bar{X}_n, Z_n)\\) est linéaire en \\(X\\), donc ce couple est aussi un vecteur gaussien. Or, la covariance de ses deux éléments est nulle. Par exemple, \\(\\mathrm{Cov}(\\bar{X}_n, Z_1)\\) est égale à \\(\\mathrm{Cov}(\\bar{X}_n, X_1) - \\mathrm{Var}(\\bar{X}_n)\\), ce qui par linéarité donne \\(1/n - 1/n = 0\\). Ainsi, \\(\\bar{X}_n\\) et \\(Z\\) sont deux variables conjointement gaussiennes et décorrélées : elles sont donc indépendantes. Comme \\(\\hat{\\sigma}_n\\) est une fonction de \\(Z\\), elle est aussi indépendante de \\(\\bar{X}_n\\).\nSecond point.  \\(Z\\) est la projection orthogonale de \\(X\\) sur le sous-espace vectoriel \\(\\mathscr{V}=\\{x \\in \\mathbb{R}^n : x_1 + \\dotsc + x_n = 0\\}\\). Soit \\((f_i)_{i=2, \\dotsc, n}\\) une base orthonormale de \\(\\mathscr{V}\\), de sorte que \\(Z = \\sum_{i=2}^n \\langle f_i, X\\rangle f_i\\). Par l’identité de Parseval, \\[|Z|^2 = \\sum_{i=2}^n |\\langle f_i, X \\rangle|^2.\\] Or, les \\(n-1\\) variables aléatoires \\(G_i = \\langle f_i, X\\rangle\\) sont des gaussiennes standard iid. En effet, on vérifie facilement que \\(\\mathrm{Cov}(G_i, G_j) = \\langle f_i, f_j\\rangle = \\delta_{i,j}\\). On en déduit donc que \\(|Z|^2\\) suit une loi \\(\\chi_2(n-1)\\).\n\nLa seconde partie de la démonstration est un cas particulier du théorème de Cochran, que nous verrons dans le chapitre sur la régression linéaire."
  },
  {
    "objectID": "ch31_outils.html#inégalités-de-concentration",
    "href": "ch31_outils.html#inégalités-de-concentration",
    "title": "5  Outils pour les IC",
    "section": "5.3 Inégalités de concentration",
    "text": "5.3 Inégalités de concentration\nLes outils de base pour construire des intervalles de confiance dans des circonstances générales (non gaussiennes) sont les inégalités de concentration. Une inégalité de concentration pour une variable aléatoire intégrable \\(X\\) consiste à borner \\(\\mathbb{P}(|X - \\mathbb{E}[X]|&gt;x)\\) par quelque chose de petit quand x est grand : on cherche à contrôler la probabilité pour que les réalisations de la variable aléatoire \\(X\\) soient éloignées de leur valeur moyenne \\(\\mathbb{E}[X]\\) de plus de \\(x\\)."
  },
  {
    "objectID": "ch31_outils.html#inégalité-de-bienaymé-tchebychev",
    "href": "ch31_outils.html#inégalité-de-bienaymé-tchebychev",
    "title": "5  Outils pour les IC",
    "section": "5.4 Inégalité de Bienaymé-Tchebychev",
    "text": "5.4 Inégalité de Bienaymé-Tchebychev\n\nThéorème 5.3 Soit \\(X\\) une variable aléatoire de carré intégrable. Alors, \\[ \\mathbb{P}(|X - \\mathbb{E}[X]|\\geqslant x)\\leqslant \\frac{\\mathrm{Var}(X)}{x^2}.\\]\n\n\nPreuve. Élever au carré les deux membres de l’inégalité dans \\(\\mathbb{P}\\), puis appliquer l’inégalité de Markov à la variable aléatoire positive \\(|X - \\mathbb{E}X|^2\\) dont l’espérance est \\(\\mathrm{Var}(X)\\)."
  },
  {
    "objectID": "ch31_outils.html#inégalité-de-hoeffding",
    "href": "ch31_outils.html#inégalité-de-hoeffding",
    "title": "5  Outils pour les IC",
    "section": "5.5 Inégalité de Hoeffding",
    "text": "5.5 Inégalité de Hoeffding\n\nThéorème 5.4 (Inégalité de Hoeffding) Soient \\(X_1, \\dotsc, X_n\\) des variables aléatoires indépendantes, pas forcément de même loi. On suppose que chaque \\(X_i\\) est à valeurs dans un intervalle borné \\([a_i, b_i]\\) et on pose \\(S_n = X_1 + \\dotsc + X_n\\). Pour tout \\(t&gt;0\\),\n\\[\\mathbb{P}(S_n - \\mathbb{E}[S_n] \\geqslant t) \\leqslant e^{-\\frac{2t^2}{\\sum_{i=1}^n(b_i - a_i)^2}} \\tag{5.3}\\] et \\[\\mathbb{P}(|S_n - \\mathbb{E}[S_n]| \\geqslant t) \\leqslant 2e^{-\\frac{2t^2}{\\sum_{i=1}^n(b_i - a_i)^2}}.  \\tag{5.4}\\]\n\nLa démonstration se fonde sur le lemme suivant.\n\nLemme 5.1 (lemme de Hoeffding) Soit \\(X\\) une variable aléatoire à valeurs dans \\([a,b]\\). Pour tout \\(t\\),\n\\[\\mathbb{E}[e^{t(X-\\mathbb{E}[X]}] \\leqslant e^{\\frac{t^2(b-a)^2}{8}}. \\tag{5.5}\\]\n\n\nPreuve. Soit \\(X\\) une variable aléatoire, que par simplicité on supposera centrée et à valeurs dans l’intervalle \\([a,b]\\) (\\(a\\) est forcément négatif). En écrivant \\[x = a\\times \\frac{b-x}{b-a} + b\\times \\left(1 - \\frac{b-x}{b-a}\\right)\\] et en utilisant la convexité de la fonction \\(x \\mapsto e^{tx}\\), on obtient \\(e^{tX}\\leqslant (b-X)e^{ta}/(b-a) + (1 - (b-x)/(b-a)) e^{bt})\\), puis en prenant l’espérance et le fait que \\(X\\) est centrée et en simplifiant, \\[\\mathbb{E}[e^{tX}]\\leqslant \\frac{be^{ta} - ae^{tb}}{b-a}.\\] Notons \\(f(t)\\) le terme à droite ; pour montrer Équation 5.5, il suffit de montrer que \\(\\ln f(t) \\leqslant t^2(b-a)^2/8\\). La formule de Taylor dit que \\[ \\ln f(t) = \\ln f(0) + t (\\ln f)'(0) + \\frac{t^2}{2}(\\ln f)''(\\xi)\\] pour un certain \\(\\xi\\). Or, \\(\\ln f(0) = \\ln 1 = 0\\), \\((\\ln f)'(0) = f'(0)/f(0) = 0\\), et il suffit donc de montrer que \\((\\ln f)''(t)\\) est toujours plus petit que \\((b-a)^2/4\\) pour conclure. Un simple calcul montre que \\(\\ln f(t) = \\ln(b/(b-a)) + ta + \\ln(1 - ae^{t(b-a)} / b)\\), et donc \\[ (\\ln f)''(t) = \\frac{(a/b)(b-a)e^{t(b-a)}}{(1 - ae^{t(b-a)}/b)^2}.\\] L’inégalité \\(uv/(u-v)^2 \\leqslant 1/4\\) appliquée à \\(u = a/b\\) et \\(v = e^{t(b-a)}\\) permet alors de conclure.\n\nPreuve de l’inégalité de Hoeffding. En remplaçant \\(X_k\\) par \\(X_k - \\mathbb{E}[X_k]\\), on peut supposer que tous les \\(X_i\\) sont centrés et étudier seulement \\(\\mathbb{P}(S_n &gt;t)\\). Écrivons \\(\\mathbb{P}(S_n &gt; t) = \\mathbb{P}(e^{\\lambda S_n} &gt; e^{\\lambda t})\\), où \\(\\lambda\\) est un nombre positif que l’on choisira plus tard. L’inégalité de Markov borne cette probabilité par \\(\\mathbb{E}[e^{\\lambda S_n}]e^{-\\lambda t}\\). Comme les \\(X_i\\) sont indépendantes, \\(\\mathbb{E}[e^{tS_n}]\\) est le produit des \\(e^{ \\varphi_k(\\lambda)}\\) où \\(\\varphi_k(t) = \\ln \\mathbb{E}[e^{itX_k}]\\). En appliquant le lemme de Hoeffding à chaque \\(\\varphi_k\\), on borne \\(\\mathbb{P}(S_n &gt;t)\\) par \\[ \\exp\\left(\\sum_{i=1}^n \\frac{(b_i - a_i)^2 \\lambda^2}{8} - t\\lambda\\right).\\] Le minimum en \\(\\lambda\\) du terme dans l’exponentielle est atteint au point \\(4t / \\sum (a_i - b_i)^2\\) et la valeur du minimum est le terme dans l’exponentielle de Équation 5.3. On déduit Équation 5.4 par une simple borne de l’union.\nLa démonstration de l’inégalité de Hoeffding ne dépend pas directement du fait que \\(X\\) est bornée, mais plutôt de Équation 5.5. Toutes les variables aléatoires qui vérifient une inégalité de type \\(\\mathbb{E}[e^{tX}]\\leqslant e^{c t^2}\\) pour une constante \\(c\\) peuvent donc avoir leur propre inégalité de Hoeffding."
  },
  {
    "objectID": "ch3_ex.html#questions",
    "href": "ch3_ex.html#questions",
    "title": "٭ Exercices",
    "section": "Questions",
    "text": "Questions\n\nSoit \\(X_n\\) une variable aléatoire de loi de Student de paramètre \\(n\\). Montrer que \\(X_n\\) converge en loi vers \\(N(0,1)\\).\nSoit \\(X_n \\sim \\chi_2(n)\\). La suite \\((X_n)\\) est-elle asymptotiquement normale ?\nDonner un intervalle de confiance de la forme \\([A,+\\infty[\\) pour la moyenne d’un échantillon gaussien.\nMême question pour la variance dans un modèle gaussien centré.\nDans l’estimation de la moyenne \\(\\mu\\) d’un modèle gaussien où la variance \\(\\sigma^2\\) est connue, montrer que l’intervalle de confiance obtenu (Équation 4.4) est le plus grand possible de niveau \\(1-\\alpha\\).\nDémontrer le théorème Théorème 5.1 sur l’asymptotique des queues de distribution de la loi gaussienne.\nMontrer la borne suivante sur les quantiles de loi gaussienne standard: \\(q_\\beta &lt; \\sqrt{\\ln\\frac{1}{\\beta\\sqrt{2\\pi}}}\\) (pour tout \\(1/2&lt;\\beta&lt;1\\)).\n\nComparer les queues de distribution des lois \\(N(0,1), \\chi_2(n)\\) et \\(\\mathscr{T}(n)\\).\nExpliquer à votre grand-mère la différence entre un intervalle de fluctuation et un intervalle de confiance.\nL’intervalle de confiance de niveau \\(1-\\alpha\\) pour la moyenne d’un modèle \\(N(\\mu, 1)\\) avec \\(n\\) observations est \\(I_n = [\\bar{X}_n \\pm z_\\alpha /\\sqrt{n}]\\). Supposons qu’on obtienne une nouvelle observation indépendante des autres, disons \\(Z\\). La probabilité \\(\\mathbb{P}(Z \\in I_n)\\) est-elle plus grande ou plus petite que \\(1-\\alpha\\) ?\nComparer la longueur des intervalles de confiance obtenus par les différentes méthodes de la section Section 4.3.1."
  },
  {
    "objectID": "ch3_ex.html#exercices",
    "href": "ch3_ex.html#exercices",
    "title": "٭ Exercices",
    "section": "Exercices",
    "text": "Exercices\n\nExercice 1 (Lois de Poisson) On suppose que l’on observe \\(X_1, \\dots, X_n\\) i.i.d de loi \\(\\mathscr{P}(\\theta)\\).\n\nÉtudier \\(\\bar{X}_n\\).\nMontrer que \\(\\sqrt{\\bar{X}_n} \\underset{n \\rightarrow \\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}} \\sqrt{\\theta}\\).\nDonner deux intervalles de confiance au niveau \\(98 \\%\\) pour \\(\\sqrt{\\theta}\\), et les comparer.\n\n\n\nExercice 2 (Lois uniformes) Soit \\(X_1, \\dotsc, X_n\\) des variables aléatoires iid de loi \\(\\mathscr{U}[0,\\theta]\\). Donner un intervalle de confiance non asymptotique pour \\(\\theta\\) en utilisant l’estimateur \\(\\hat{\\theta}_n = \\max_{i=1,\\dotsc, n}X_i\\).\n\n\nExercice 3 (Lois exponentielles décalées) Soit \\(X_1, \\dotsc, X_n\\) des variables aléatoires iid de densité \\(e^{\\theta-x} \\mathbf{1}_{x&gt;\\theta}\\), où \\(\\theta &gt;0\\).\n\nCalculer \\(\\mathbb{E}_\\theta\\left[X_1\\right]\\) et en déduire un estimateur de \\(\\theta\\) que l’on notera \\(\\hat\\theta_n\\). Étudier ses propriétés (risque quadratique, convergence) et l’utiliser pour construire un premier intervalle de confiance \\(I_1(\\alpha)\\) non-asymptotique pour \\(\\theta\\) de niveau \\(1-\\alpha\\).\nConstruire un intervalle de confiance asymptotique \\(I_2(\\alpha)\\) pour \\(\\theta\\) à partir de \\(\\hat{\\theta}_n\\).\nMontrer que l’estimateur \\(\\theta_n^\\star := \\min_{1 \\leq i \\leq n} X_i\\) est meilleur que \\(\\hat \\theta_n\\) au sens du risque quadratique, puis l’utiliser pour construire un intervalle de confiance \\(I_3(\\alpha)\\) de niveau \\(1-\\alpha\\).\nComparer les longueurs de tous ces différents intervalles de confiance.\n\n\n\nExercice 4 (Lois exponentielles) Soit \\(X_1, \\dotsc, X_n\\) des variables aléatoires iid exponentielles de paramètre \\(\\lambda&gt;0\\).\n\nQuelle est la loi de \\(S_n = X_1 + \\dotsb + X_n\\) ?\nConstruire un intervalle de confiance de niveau \\(1-\\alpha\\) pour \\(\\lambda\\).\n\n\n\nExercice 5 (Inégalité d’Azuma) Montrer que l’inégalité de Hoeffding reste valable lorsque les \\(X_i\\) ne sont plus supposés indépendants, mais que la suite \\(S_k = X_1 + \\dotsb + X_k\\) est une martingale. Indice : \\(\\mathbb{E}[e^{\\lambda S_{n+1}}] = \\mathbb{E}[e^{\\lambda S_n}\\mathbb{E}[e^{\\lambda X_{n+1}}|S_n]]\\).\nCe raffinement s’appelle inégalite de Hoeffding-Azuma. C’est celui que nous avons utilisé dans l’exercice (ex-tanks?), lorsque les \\(X_1, \\dotsc, X_n\\) sont des tirages sans remise dans une urne à \\(N\\) éléments."
  },
  {
    "objectID": "ch4_0.html#exemples-de-tests-gaussiens",
    "href": "ch4_0.html#exemples-de-tests-gaussiens",
    "title": "6  Test d’hypothèses",
    "section": "6.1 Exemples de tests gaussiens",
    "text": "6.1 Exemples de tests gaussiens\nOn se place dans un modèle où \\(X_1, \\dotsc, X_n\\) sont des gaussiennes \\(N(\\mu, \\sigma^2)\\). Nous avons déjà vu plusieurs fois que \\(\\bar{X}_n \\sim N(\\mu, \\sigma^2/n)\\).\n\n6.1.1 Construction du test\nOn cherche à réfuter l’hypothèse selon laquelle ces variables aléatoires sont centrées ; autrement dit, on posera \\(H_0 = \\{\\mu=0\\}\\). Sous cette hypothèse, nos variables aléatoires sont donc des variables \\(N(0,\\sigma^2)\\).\nSupposons dans un premier temps que \\(\\sigma^2\\) est connue. Sous \\(H_0\\), on a donc \\[ \\frac{\\sqrt{n}\\bar{X}_n}{\\sigma} \\sim N(0,1)\\] et par conséquent, \\(P_0(|\\bar{X}_n| &lt; z_{1-\\alpha} \\sigma / \\sqrt{n}) = 1-\\alpha\\). Autrement dit, sous l’hypothèse \\(\\mu = 0\\), on devrait observer l’événement \\[ \\bar{X}_n \\in \\left[ \\pm \\frac{z_{1-\\alpha} \\sigma}{\\sqrt{n}}\\right]\\] avec probabilité élevée \\(1-\\alpha\\). Si cet événement n’est pas observé, il est alors très douteux que \\(\\mu\\) soit effectivement égal à zéro ! On pose donc \\[ \\mathsf{rejeter}_\\alpha = \\{|\\bar{X}_n | &gt; z_{1-\\alpha} \\sigma / \\sqrt{n}\\}.\\] Le niveau de ce test est bien \\(1-\\alpha\\) : nous l’avons construit pour cela.\nSupposons maintenant que \\(\\sigma\\) n’est pas connue. En l’estimant via \\(\\hat{\\sigma}_n\\), nous savons que (toujours sous l’hypothèse selon laquelle \\(\\mu=0\\)) \\[ \\frac{\\sqrt{n}\\bar{X}_n}{\\hat{\\sigma}_n} \\sim \\mathscr{T}(n-1).\\] On reproduit alors le raisonnement ci-dessus : comme \\(\\mathbb{P}(|\\bar{X}_n| &lt; t_{n-1, 1-\\alpha}\\hat{\\sigma}_n / \\sqrt{n}) = \\alpha\\) où \\(t_{n-1,1-\\alpha}\\) est le quantile symétrique de \\(\\mathscr{T}(n-1)\\), on voit que l’événement \\[ \\mathsf{rejeter}_\\alpha = \\{|\\bar{X}_n| &gt; t_{n-1,1-\\alpha}\\hat{\\sigma}_n / \\sqrt{n}\\}\\] est bien un test de niveau \\(1-\\alpha\\).\n\n\n6.1.2 Calcul de la puissance et hypothèse alternative\nNous n’avons pas encore eu besoin de spécifier une hypothèse alternative, mais nous allons en avoir besoin pour calculer la puissance du test. Pour commencer, on va supposer que, si \\(\\mu\\) n’est pas nulle, alors elle ne peut être égale qu’à 1. Autrement dit, \\(H_1 = \\{1\\}\\). Ce genre d’hypothèse alternative ne peut évidemment avoir de pertinence qu’en fonction du problème réel sous-jacent !\nSous l’hypothèse alternative, donc, nous savons que \\(\\bar{X}_n \\sim N(1, \\sigma^2)\\). La puissance du test est définie par \\(1-\\beta\\) où \\(\\beta=P_1(\\mathsf{accepter}_\\alpha)\\) c’est-à-dire \\[\\begin{align}\\beta &= P_1(|\\bar{X}_n|\\leqslant z_{1-\\alpha} \\sigma / \\sqrt{n}) \\\\\n&= P_1 \\left(-\\frac{z_{1-\\alpha} \\sigma}{\\sqrt{n}}\\leqslant \\bar{X}_n \\leqslant \\frac{z_{1-\\alpha} \\sigma}{\\sqrt{n}} \\right) \\\\\n&= P_1 \\left(-\\frac{z_{1-\\alpha} \\sigma}{\\sqrt{n}}-1\\leqslant \\bar{X}_n - 1 \\leqslant \\frac{z_{1-\\alpha} \\sigma}{\\sqrt{n}} -1 \\right)\\\\\n&= \\Phi(-\\sqrt{n}/\\sigma + z_{1-\\alpha}) - \\Phi(-\\sqrt{n}/\\sigma + z_{1-\\alpha}).\n\\end{align}\\] où \\(\\Phi(x) = \\mathbb{P}(N(0,1)\\leqslant x)\\). Cette expression ne peut pas plus se simplifier, mais on peut quand même la borner par \\(F(-\\sqrt{n}/\\sigma + z_{1-\\alpha})\\). Lorsque \\(x\\) est grand, nous avons vu (Théorème 5.1) que \\(F(x) &lt; e^{-x^2}/|x|\\sqrt{2\\pi}\\). Ainsi, l’erreur de première espèce est bornée par \\(O(e^{-n/\\sigma^2/2} / \\sqrt{n})\\). Cela tend extrêmement vite vers 0 ; en fait, dès que \\(n\\) est plus grand que 10 et \\(\\sigma=1\\), cette erreur est inférieure à 0.1%, donc dans ce cas le test aura une puissance supérieure à \\(99.9\\%\\).\nQue se serait-il passé si notre hypothèse alternative n’avait pas été \\(\\mu=1\\) mais \\(\\mu = m\\) pour n’importe quel \\(m\\neq 0\\) ? Dans ce cas, on aurait eu \\(H_1 = \\mathbb{R}\\setminus \\{0\\}\\). L’erreur de première espèce aurait alors été \\(\\beta = \\sup_{m\\neq 0}\\beta_m\\) où \\[ \\beta_m = P_m(\\mathsf{accepter}_\\alpha).\\] On revoyant les calculs ci-dessus, on voit que \\[\\beta_m = \\Phi(-m\\sqrt{n}/\\sigma + z_{1-\\alpha}) - \\Phi(-m\\sqrt{n}/\\sigma + z_{1-\\alpha}).\\] En particulier, \\[\\lim_{m\\to 0}\\beta_m =\\Phi(-z_{1-\\alpha}) - \\Phi(-z_{1-\\alpha}) =  1-\\alpha\\] par continuité de \\(\\Phi\\) et par définition de \\(z_{1-\\alpha}\\). Ainsi, \\(1-\\beta = \\alpha\\) : pour cette seconde hypothèse alternative, la puissance de notre test… est extrêmement faible.\nCela vient du fait que notre hypothèse alternative contient des situations quasiment indiscernables de notre hypothèse nulle. Par exemple, il est quasiment impossible de distinguer \\(\\mu = 0\\) de \\(\\mu = 10^{-100}\\) par exemple. Cet exemple illustre la dissymétrie entre \\(H_0\\) et \\(H_1\\)."
  },
  {
    "objectID": "ch4_0.html#la-notion-de-p-valeur",
    "href": "ch4_0.html#la-notion-de-p-valeur",
    "title": "6  Test d’hypothèses",
    "section": "6.2 La notion de \\(p\\)-valeur",
    "text": "6.2 La notion de \\(p\\)-valeur\nLa construction d’un test dépend du niveau de risque \\(\\alpha\\). Si le niveau de risque acceptable est de plus en petit, alors l’événement \\(\\mathsf{rejeter}_\\alpha\\) devrait être de moins en moins probable. D’ailleurs, \\(\\mathsf{rejeter}_0 = \\varnothing\\) et \\(\\mathsf{accepter}_0 = \\Omega\\) : si l’on ne tolère aucun niveau de risque de première espèce, c’est qu’on ne veut pas rejeter l’hypothèse nulle.\nTrès souvent, si \\(\\alpha&lt;\\beta\\), on a même \\[\\mathsf{rejeter}_\\alpha \\subset \\mathsf{rejeter}_\\beta.  \\]\n\nDéfinition 6.2 La \\(p\\)-valeur d’une famille croissante de tests est le plus petit niveau de risque qui nous amène à rejeter l’hypothèse nulle compte tenu des observations. Formellement, \\[ p_\\star = \\inf\\{\\alpha&gt;0 : \\mathsf{rejeter}_\\alpha\\} = \\sup\\{\\alpha&gt;0 : \\mathsf{accepter}_\\alpha\\}.\\]\n\nLa \\(p\\)-valeur dépend des observations. C’est une observation cruciale : la \\(p\\)-valeur n’est pas une propriété intrinsèque d’un test. Sur deux ensembles différents d’observations, la \\(p\\)-valeur ne sera pas la même en général.\nCalcul de \\(p\\)-valeur. Dans de nombreux tests, la construction d’un test se fonde sur une statistique, disons \\(S\\), qui sous l’hypothèse nulle suit une loi particulière (par exemple, \\(\\sqrt{n}\\bar{X}_n / \\hat{\\sigma}_n \\sim \\mathscr{T}(n-1)\\) sous l’hypothèse \\(X_i \\sim N(\\mu,\\sigma^2)\\) avec \\(\\mu=0\\) dans le cas d’un test de Student). Si le test est de la forme \\(S &lt; q_{1-\\alpha}\\), ce qui équivaut à \\(F(S)&lt;1-\\alpha\\). La \\(p\\)-valeur est donnée par \\[p_\\star = \\sup\\{\\alpha &gt; 0 : S &lt; q_{1-\\alpha}\\} = \\sup\\{\\alpha : F(S)&lt;1-\\alpha\\} = 1 - F(S).\\]"
  },
  {
    "objectID": "ch4_1.html#la-distance-en-variation-totale",
    "href": "ch4_1.html#la-distance-en-variation-totale",
    "title": "7  Théorie des tests simples",
    "section": "7.1 La distance en variation totale",
    "text": "7.1 La distance en variation totale\nLorsqu’on cherche à tester une hypothèse de type \\(\\text{loi} = P\\) contre une hypothèse de type \\(\\mathrm{loi} = Q\\) (c’est-à-dire, deux hypothèses simples), on en revient à chercher un événément très improbable sous la loi \\(P\\), et très probable sous la loi \\(Q\\). On peut se demander en toute généralité quels sont les événements pour lesquels ces probabilités diffèrent le plus, c’est-à-dire les événements \\(A\\) qui maximisent \\(P(A) - Q(A)\\). Cela mène directement à la définition de la variation totale.\n\nDéfinition 7.1 (distance en variation totale) Soient \\(P,Q\\) deux mesures de probabilité sur un même espace \\((\\mathcal{X}, \\mathscr{F})\\). Leur distance en variation totale est \\[ \\dtv(P,Q) = \\sup_{A \\in \\mathscr{F}}P(A) - Q(A). \\]\n\nLa distance en variation totale est un objet important en probabilités, qui possède de nombreuses propriétés. Parmi elles, voici les plus importantes.\n\nC’est une distance sur l’espace des mesures de probabilité.\nElle génère une topologie plus fine que celle de la convergence en loi ; autrement dit, si \\(\\dtv(P_n, Q) \\to 0\\) alors \\(P_n\\) converge en loi vers \\(Q\\) mais l’inverse n’est pas vrai.\n\n\nProposition 7.1 Soit \\(\\nu\\) une mesure telle que \\(P\\) et \\(Q\\) sont absolument continues1 par rapport à \\(\\nu\\), de densités respectives \\(p\\) et \\(q\\) par rapport à \\(\\nu\\). Alors, \\(\\dtv(P,Q)\\) est égale à chacune des quantités suivantes :\n\\[\\int_{\\mathcal{X}} (p(x) - q(x))_+\\mathrm{d}\\nu\\] \\[ \\frac{1}{2}\\int_{\\mathcal{X}} |p(x) - q(x)|\\mathrm{d}\\nu. \\tag{7.1}\\]\nDe plus, notons \\(E\\) l’ensemble mesurable \\(\\{x \\in \\mathcal{X} : p(x)&gt;q(x)\\}\\). Alors, \\[\\dtv(P,Q) = P(E) - Q(E). \\tag{7.2}\\]\n\nL’hypothèse selon laquelle \\(P,Q\\) sont a.c. par rapport à \\(\\nu\\) est toujours vérifiée pour \\(\\nu = (P+Q)/2\\), et n’est donc pas restrictive.\n\nPreuve. Pour tout événement \\(A \\in \\mathscr{F}\\), la différence \\(P(A) - Q(A)\\) est égale à \\(\\int_A p(x) - q(x) \\mathrm{d}\\nu\\), qui peut elle-même s’écrire sous la forme \\[\\int_{A \\cap E} (p - q) \\mathrm{d}\\nu + \\int_{A \\cap \\bar{E}} (p - q) \\mathrm{d}\\nu.\\] Le second terme est négatif, puisque si \\(x \\notin E\\) alors \\(p(x)\\leqslant q(x)\\). Ainsi, \\(P(A) - Q(A)\\) est plus petit que le premier terme, lequel est à son tour plus petit que \\(\\int_E (p-q)d\\nu = P(E) - Q(E)\\). Cela montre directement Équation 7.2. Au passage, il est évident que \\[\\int_E (p(x)-q(x))\\mathrm{d}\\nu = \\int_{\\mathcal{X}}(p(x) - q(x))_+ \\mathrm{d}\\nu,  \\] ce qui montre la première égalité de Équation 7.1. La seconde égalité résulte de la première, puisque comme \\(p\\) et \\(q\\) sont des densités de probabilité, on a forcément \\(\\int (p-q)_+ = \\int(p-q)_-\\).\n\nDans la suite, on supposera toujours que les diverses lois possèdent toutes une densité par rapport à une mesure de référence \\(\\nu\\). C’est le cas dans de très nombreux modèles — pas tous, hélas. Les lettres majuscules désigneront les mesures, tandis que les lettres minuscules désigneront leurs densités."
  },
  {
    "objectID": "ch4_1.html#test-optimal-au-sens-de-laffinité",
    "href": "ch4_1.html#test-optimal-au-sens-de-laffinité",
    "title": "7  Théorie des tests simples",
    "section": "7.2 Test optimal au sens de l’affinité",
    "text": "7.2 Test optimal au sens de l’affinité\nL’affinité d’un test est la somme de ses erreurs de première et seconde espèce : c’est la probabilité de « se tromper » en général, quelle que soit l’hypothèse.\n\nThéorème 7.1 Soit \\(\\mathfrak{T}\\) l’ensemble des tests possibles de l’hypothèse \\(H_0 : P = P_0\\) contre l’hypothèse alternative \\(H_1 : P = P_1\\). Alors, le test possédant la meilleure affinité possible parmi tous les tests possibles vérifie \\[\\inf_{T \\in \\mathfrak{T}}~\\{\\alpha_T + \\beta_T \\} = 1 - \\dtv(P_0, P_1). \\] En particulier, le test optimal pour l’affinité est donné par la région de rejet \\[ \\mathsf{rejeter}_\\star = \\{p_0(x) &lt; p_1(x)\\}.\\]\n\n\nPreuve. Soit \\(T\\) n’importe quel test. Son affinité est \\(P_1(\\{T=0\\}) + P_0(\\{T=1\\})\\). En passant au complémentaire dans le second terme, on obtient \\[1 - (P_0(\\{T=0\\}) - P_1(\\{T=0\\})). \\] Cette quantité est forcément plus petite que \\(1 - \\dtv(P_0, P_1)\\) par la définition même de la variation totale. De plus, cette borne est atteinte en choisissant le test \\(T\\) donné dans l’énoncé, d’où l’égalité.\n\nCommentaire. Le théorème précédent semble donner au problème de la construction de tests une réponse définitive : il donne le test optimal au sens de l’affinité, test qui est élémentaire et intuitif. En effet, si \\(P_0, P_1\\) sont les deux lois et si \\((x_1, \\dotsc, x_n)\\) est l’échantillon observé, alors on rejette l’hypothèse nulle si la probabilité de cette observation est plus grande sous \\(P_1\\) que sous \\(P_0\\) : autrement dit, si \\[ \\frac{p_1(x_1, \\dotsc, x_n)}{p_0(x_1, \\dotsc, x_n)}&gt;1. \\] Le terme de droite s’appelle rapport de vraisemblance. Pourtant, ce test ne permet pas de contrôler l’erreur de première espèce. Il peut tout à fait exister d’autres tests qui ont un niveau plus élevé. Il est donc naturel de se demander si, parmi les tests ayant un niveau fixé \\(1-\\alpha\\), il existe un autre critère d’optimalité."
  },
  {
    "objectID": "ch4_1.html#théorème-de-neyman-pearson",
    "href": "ch4_1.html#théorème-de-neyman-pearson",
    "title": "7  Théorie des tests simples",
    "section": "7.3 Théorème de Neyman-Pearson",
    "text": "7.3 Théorème de Neyman-Pearson\nOn se place toujours dans un cadre où les deux lois \\(P_0\\) et \\(P_1\\) possèdent deux densités \\(p_0, p_1\\) par rapport à une mesure commune \\(\\nu\\).\n\nDéfinition 7.2 Un test du rapport de vraisemblance est un test dont la région de rejet est de la forme \\[\\mathsf{rejeter}= \\left\\lbrace \\frac{p_1(x)}{p_0(x)} &gt; z \\right\\rbrace  \\tag{7.3}\\] pour un certain \\(z&gt;0\\).\n\nLe test optimal au sens de l’affinité est un test de rapport de vraisemblance (\\(z=1\\)).\n\nThéorème 7.2 (Théorème de Neyman-Pearson) Tout test de même niveau qu’un test du rapport de vraisemblance est moins puissant que celui-ci.\n\n\nPreuve. On suppose que la région de rejet de \\(T_\\star\\) est de la forme Équation 7.3. Soit \\(T\\) un autre test de même niveau que \\(T_\\star\\). La quantité \\[ \\int_{\\mathcal{X}} (T(x) - T_\\star(x))(p_1(x) - z p_0(x))\\mathrm{d}\\nu \\] est forcément négative ou nulle : en effet, si \\(T_\\star(x)=1\\), alors \\(T(x)-T_\\star(x) = T(x)-1 \\leqslant 0\\), mais \\(p_1(x)\\) est plus grand que \\(zp_0(x)\\), donc \\((p_1(x) - zp_0(x))\\geqslant 0\\). De même, si \\(T(x) = 0\\), alors cette fois ce terme est négatif. Dans les deux cas, la fonction dans l’intégrale est toujours le produit de deux nombres de signes opposés : elle est donc négative. Or, en développant cette intégrale, on constate qu’elle vaut aussi \\[P_1(T=1) - P_1(T_\\star=1) - zP_0(T=1)+zP_0(T_\\star=1). \\] Tout ceci n’est rien d’autre que \\(\\beta_\\star-\\beta - z(\\alpha-\\alpha_\\star)\\), où \\(\\alpha, \\beta\\) désignent les deux types d’erreurs du test \\(T\\) et \\(\\alpha_\\star, \\beta_\\star\\) celles de \\(T_\\star\\). Mais nous avons supposé que \\(\\alpha = \\alpha_\\star\\) : des deux termes ci-dessus, ne reste que le premier, à savoir \\(\\beta_\\star - \\beta\\), qui est bien négatif comme demandé."
  },
  {
    "objectID": "ch4_1.html#un-exemple-de-test-de-rapport-de-vraisemblance",
    "href": "ch4_1.html#un-exemple-de-test-de-rapport-de-vraisemblance",
    "title": "7  Théorie des tests simples",
    "section": "7.4 Un exemple de test de rapport de vraisemblance",
    "text": "7.4 Un exemple de test de rapport de vraisemblance\nPlaçons-nous dans un modèle de Bernoulli : on a des variables aléatoires \\(X_1, \\dotsc, X_n\\) iid de loi \\(\\mathrm{Ber}(p)\\), et l’on souhaite tester une valeur \\(p_0\\) de \\(p\\) contre une valeur \\(p_1 \\neq p_0\\) à partir d’une réalisation \\(x_1, \\dotsc, x_n\\) du modèle.\nIci, les lois sont discrètes : elles possèdent une densité par rapport à la mesure de comptage. La probabilité d’observer \\(x_1, \\dotsc, x_n\\) dans le modèle avec paramètre \\(p\\) est égale à \\[\\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = p^s(1-p)^{n-s}\\] où \\(s = x_1 + \\dotsc + x_n\\). Ainsi, le rapport des vraisemblances \\(r\\) est égal à \\[\\frac{p_1^s (1-p_1)^{n-s}}{p_0^s(1-p_0)^{n-s}} = \\left(\\frac{p_1(1-p_0)}{p_0(1-p_1)}\\right)^s\\left(\\frac{1-p_1}{1-p_0}\\right)^n. \\] Le théorème de Neyman-Pearson dit qu’un test de la forme \\(r&gt;z\\) est plus puissant que tous les tests ayant le même niveau. Or, cette région de rejet peut encore s’écrire \\[ s \\ln \\left(\\frac{p_1(1-p_0)}{p_0(1-p_1)}\\right) &gt; \\ln(z) - n\\ln\\left(\\frac{1-p_1}{1-p_0}\\right).\\]\nDans le cas où \\(p_0&lt;p_1\\), alors par croissance \\(p_1 / (1-p_1)\\) est plus grand que \\(p_0/(1-p_0)\\), et donc cette région de rejet peut encore s’écrire \\[ \\frac{s}{n} &gt; \\frac{\\ln(z)/n - \\ln((1-p_1)/(1-p_0))}{\\ln \\left(\\frac{p_1(1-p_0)}{p_0(1-p_1)}\\right) }.\\] Cette écriture n’a rien d’intéressant en soi. Tout ce qui compte, c’est que la région de rejet optimale au sens de Neyman-Pearson est de la forme \\(\\{\\bar{X}_n &gt; z'\\}\\) où \\(z'\\) correspond au terme de droite ci-dessus.\nDans le cas où \\(p_0&gt;p_1\\), alors le même raisonnement donne une région de rejet de la forme \\(\\{\\bar{X}_n &lt; z'\\}\\).\nLa détermination de \\(z'\\) dépendra du niveau de confiance que l’on veut se donner. L’erreur de première espèce est \\(P_{p_0}(\\bar{X}_n &gt; z')\\), qui est la probabilité qu’une binomiale \\(\\mathrm{Bin}(n,p_0)\\) soit plus grande que \\(nz'\\). En choisissant pour \\(nz'\\) le quantile de niveau \\(1-\\alpha\\) de cette loi, la probabilité ci-dessus est plus petite que \\(\\alpha\\) et le test est de niveau de confiance supérieur à \\(1-\\alpha\\)."
  },
  {
    "objectID": "ch4_1.html#une-borne-sur-la-variation-totale",
    "href": "ch4_1.html#une-borne-sur-la-variation-totale",
    "title": "7  Théorie des tests simples",
    "section": "7.5 Une borne sur la variation totale",
    "text": "7.5 Une borne sur la variation totale\nCe chapitre n’a pas été vu en cours et n’est pas au programme.\nLa construction du test optimal au sens de l’affinité nécessite le calcul de la distance en variation totale, laquelle peut être notoirement difficile : - d’abord, parce que la formule Équation 7.1 peut être impossible à calculer même si \\(P\\) et \\(Q\\) sont connues ; - ensuite, parce que \\(Q\\) elle-même peut parfois être très difficile à calculer (le calcul peut être de complexité exponentielle).\nEn pratique, on peut chercher à borner cette distance par d’autres quantités plus faciles à calculer. Parmi ces quantités, la divergence de Kullback-Leibler joue un rôle extrêmement important, notamment pour son lien avec le maximum de vraisemblance que nous verrons plus tard.\n\nDéfinition 7.3 Soient \\(P\\) et \\(Q\\) deux mesures, \\(P\\) étant absolument continue par rapport à \\(Q\\). Alors,\n\\[ \\dkl(P \\mid Q) = \\int \\ln \\left(\\frac{\\mathrm{d}P}{\\mathrm{d}Q}\\right)dP. \\]\nSi \\(P\\) n’est pas absolument continue par rapport à \\(Q\\), on pose simplement \\(\\dkl(P \\mid Q) = +\\infty\\).\n\nLa notation \\(\\mathrm{d}P/\\mathrm{d}Q\\) désigne la densité de \\(P\\) par rapport à \\(Q\\). Formellement, c’est la dérivée de Radon-Nikodym. Dans le cas de variables aléatoires continues sur \\(\\mathbb{R}^d\\), c’est le rapport des densités de \\(P\\) et de \\(Q\\).\nLa divergence \\(\\dkl\\) n’est pas une distance, et c’est pour cela qu’on l’appelle divergence et qu’on la note avec une barre plutôt qu’une virgule : elle n’est pas symétrique en général. Cependant, elle est toujours positive (éventuellement égale à \\(+\\infty\\) même si \\(P\\ll Q\\)), et n’est nulle que si \\(P=Q\\).\n\nThéorème 7.3 (Borne de Bretagnole-Huber-Pinsker) \\[ \\dtv(P,Q) \\leqslant \\sqrt{1 - e^{-\\dkl(P \\mid Q)}}. \\tag{7.4}\\]\n\n\\[~~\\]\nRemarque. Il est facile de vérifier que \\(\\sqrt{1-e^{-x}}\\leqslant \\sqrt{x}\\) lorsque \\(x&gt;0\\). Ainsi, Équation 7.4 entraîne la borne plus simple \\(\\dtv \\leqslant \\sqrt{\\dkl}\\). La borne classique de Pinsker améliore légèrement ce résultat, puisqu’elle dit que \\(\\dtv \\leqslant \\sqrt{\\dkl/2}\\).\n\nPreuve. Si \\(P\\) n’est pas absolument continue par rapport à \\(Q\\), alors \\(\\dkl(P \\mid Q)=+\\infty\\) et la borne demandée est vraie. Sinon, on note \\(\\rho\\) la densité de \\(P\\) par rapport à \\(Q\\), de sorte que \\(\\dkl(P\\mid Q) = -\\int \\ln \\rho(x) \\mathrm{d}P\\). On définit ensuite \\(v = (\\rho-1)_+\\) et \\(w = (\\rho-1)_-\\), de sorte que \\(vw\\) vaut toujours 0, et donc \\((1 + v)(1-w) = 1 - w + v = \\rho\\). En particulier, \\(\\dkl(P\\mid Q)\\) vaut \\[\\int(-\\ln(1+v))dP + \\int (-\\ln(1-w))dP.\\] Or, les deux fonctions \\(x\\mapsto -\\ln(1+x)\\) et \\(x\\mapsto -\\ln(1-x)\\) sont concaves sur leurs ensembles de définition. Ainsi, l’inégalité de Jensen entraîne d’une part \\[ \\int(-\\ln(1+v))\\mathrm{d}P \\leqslant - \\ln \\left(1 + \\int v \\mathrm{d}P\\right)\\] et d’autre part \\[ \\int(-\\ln(1-w))\\mathrm{d}P \\leqslant - \\ln \\left(1 - \\int w \\mathrm{d}P\\right).\\] Or, la formule Équation 7.1 montre que \\(\\int v\\mathrm{d}P = \\dtv(P,Q)\\), et de même pour \\(\\int w \\mathrm{d}P\\). En additionnant les deux inégalités ci-dessus, on obtient Alors \\[-\\dkl \\leqslant - \\ln \\left((1 + \\dtv)(1 - \\dtv\\right)\\] soit \\(-\\dkl \\leqslant -\\ln(1 - \\dtv^2)\\), c’est-à-dire Équation 7.4."
  },
  {
    "objectID": "ch4_1.html#footnotes",
    "href": "ch4_1.html#footnotes",
    "title": "7  Théorie des tests simples",
    "section": "",
    "text": "Si \\(P\\) est absolument continue par rapport à \\(Q\\) (ce qu’on note \\(P \\ll Q\\)), alors la dérivée de Radon-Nikodym existe, et c’est une fonction mesurable positive \\(f\\) (unique à un ensemble \\(Q\\)-négligeable près) qui vérifie \\(P(A) = \\int f(x)\\mathbf{1}_{x\\in A}\\mathrm{d}Q\\). On appelle cette fonction densité de \\(P\\) par rapport à \\(Q\\).↩︎"
  },
  {
    "objectID": "ch4_2.html#loi-multinomiale",
    "href": "ch4_2.html#loi-multinomiale",
    "title": "8  Tests du \\(\\chi_2\\)",
    "section": "8.1 Loi multinomiale",
    "text": "8.1 Loi multinomiale\nSoit \\(\\Omega\\) un ensemble fini à \\(k\\) éléments, disons pour simplifier \\(\\{1, \\dotsc, k\\}\\). On notera \\(S_k\\) l’ensemble des lois de probabilités sur cet ensemble, c’est-à-dire les \\(k\\)-uplets \\(\\mathbf{p} = (p_1, \\dotsc, p_k)\\) de nombres positifs dont la somme vaut 1. On observe \\(n\\) tirages indépendants et identiquement distribués selon une même loi sur \\(\\Omega\\). Formellement, le modèle statistique est donné par \\((\\mathbf{p}^{\\otimes n} : \\mathbf{p} \\in S_k)\\).\nOn note \\(N_j\\) le nombre d’observations égales à \\(j\\). Le vecteur \\(N=(N_1, \\dotsc, N_k)\\) suit alors une loi multinomiale de paramètres \\(n\\) et \\(\\mathbf{p}\\), donnée par \\[\\begin{align*}\n\\mathbb{P}(N = (n_1, \\dotsc, n_k)) = \\frac{n!}{n_1! \\dotsc n_k!} \\prod_{j=1}^k {p}_j^{n_j},\n\\end{align*}\\] où \\(\\sum_{j=1}^k n_j = n\\). Cette loi sera notée \\(\\mathrm{Mult}(n, \\mathbf{p})\\).\n\nThéorème 8.1 Soit \\(N \\sim \\mathrm{Mult}(n,\\mathbf{p})\\). Alors, \\(\\sqrt{n}(\\frac{N}{n}- \\mathbf{p})\\) converge en loi lorsque \\(n\\to\\infty\\) vers \\(\\mathcal{N}(0, \\Sigma)\\), où \\[ \\Sigma = \\mathrm{diag}(\\mathbf{p}) - \\mathbf{p}\\mathbf{p}^\\top. \\tag{8.1}\\]\n\n\nPreuve. On commence par remarquer que \\(N = \\sum_{i=1}^n Z_i\\), où \\(Z_i=(\\mathbf{1}_{X_i=1}, \\dotsc, \\mathbf{1}_{X_i=k})\\). Les \\(Z_i\\) sont iid de moyenne \\(\\mathbf{p}\\). Les covariances des entrées \\(i\\) et \\(j\\) de \\(Z_k\\) sont données par \\[\\mathbb{E}[\\mathbf{1}_{X_k=i}\\mathbf{1}_{X_k=j}] - p_i p_j = \\delta_{i,j}p_i - p_i p_j,\\] ce qui montre que la matrice de covariance des \\(Z_k\\) est Équation 8.1. Il suffit alors d’appliquer le TCL.\n\nRemarque. On considère que cette approximation normale est correcte dès que \\(\\mathbb{E}[N_j]\\) est plus grand que \\(5\\) pour tout \\(j\\)."
  },
  {
    "objectID": "ch4_2.html#test-dadéquation",
    "href": "ch4_2.html#test-dadéquation",
    "title": "8  Tests du \\(\\chi_2\\)",
    "section": "8.2 Test d’adéquation",
    "text": "8.2 Test d’adéquation\nLe test du \\(\\chi^2\\) d’adéquation consiste à tester l’hypothèse nulle \\[H_0: \\mathbf{p}= \\mathbf{p}_0 \\tag{8.2}\\] contre l’hypothèse alternative \\[H_1:\\mathbf{p} \\neq \\mathbf{p}_0, \\tag{8.3}\\] pour une valeur de \\(\\mathbf{p}_0\\) fixée au préalable. À partir de maintenant, on supposera implicitement que toutes les entrées de \\(\\mathbf{p}_0\\) sont non nulles — cela garantira que les limites en loi trouvées ci-dessous ne sont pas dégénérées.\n\nExemple 8.1 On peut se demander si, dans la langue courante, les 21 lettres de l’alphabet ont à peu près la même probabilité d’apparaître comme première lettre d’un mot. Cela revient à tester si \\(\\mathbf{p}_0=(1/26, \\dotsc, 1/26)\\), hypothèse qui est évidemment fausse.\nQu’en est-il des 9 chiffres ? On peut vouloir tester si, dans n’importe quel document (journal, site internet, article scientifique), ces 9 chiffres apparaissent à peu près uniformément en tant que premier chiffre d’un nombre. Cela reviendrait à tester \\(\\mathbf{p}_0 = (1/9, \\dotsc, 1/9)\\).\nCe n’est pas le cas et cette hypothèse est très fréquemment réfutée : le premier chiffre significatif d’un nombre est bien plus souvent 1 (\\(\\approx 30\\%\\) des cas) que \\(9\\) (\\(\\approx 5\\%\\) cas). Ce phénomène s’appelle loi de Benford.\n\nLe théorème Théorème 8.1 dit que \\(\\sqrt{n}(\\frac{N}{n}- \\mathbf{p}) \\approx N(0, \\Sigma)\\). Notons \\(\\sqrt{\\mathbf{p}_0} = (\\sqrt{p_1}, \\dotsc, \\sqrt{p_k})\\) et \\(D = \\mathrm{diag}(\\sqrt{\\mathbf{p}}_0)\\). Sous \\(H_0\\), \\(D^{-1} \\sqrt{n}(\\frac{N}{n}- \\mathbf{p}_0)\\) converge en loi vers \\(D^{-1}N(0,\\Sigma) = N(0,D^{-1}\\Sigma (D^{-1})^\\top)\\). Que vaut cette matrice de covariance ? \nD’abord, comme \\(D\\) est diagonale, \\(D^{-1}\\) l’est aussi et \\((D^{-1})^\\top\\) vaut \\(D^{-1}\\). De plus, \\(D^2\\) est égal à \\(\\mathrm{diag}(\\mathbf{p}_0)\\). Enfin, en faisant la multiplication on voit vite que \\(D^{-1}\\mathbf{p}_0 = \\sqrt{\\mathbf{p}}_0\\). Ainsi, on voit que \\(D^{-1}\\Sigma D^{-1}\\) vaut également \\[D^{-1}D^2 D^{-1} - D^{-1}\\mathbf{p}_0 \\mathbf{p}_0 D^{-1} = I_k - \\sqrt{\\mathbf{p}}_0 \\mathbf{p}_0^{\\top}.\\] En regroupant tout cela, on obtient donc que \\(D^{-1}\\sqrt{n}(N/n - \\mathbf{p}_0)\\) converge en loi vers \\[N(0, I_k - \\sqrt{\\mathbf{p}_0} \\sqrt{\\mathbf{p}_0}^T).\\] La statistique qui va nous servir à faire des tests est tout simplement la norme au carré de \\(D^{-1}\\sqrt{n}(N/n - \\mathbf{p}_0)\\). En manipulant légèrement cette expression, on obtient sa forme usuelle, le contraste du \\(\\chi_2\\).\n\nDéfinition 8.1 (Contraste du \\(\\chi_2\\)) Dans le contexte ci-dessus, le contraste du \\(\\chi_2\\) associé à la loi \\(\\mathbf{p}\\) est la statistique\n\\[ D_n(\\mathbf{p}) = \\sum_{j=1}^k \\frac{(N_j - n{p}_j)^2}{n{p}_j}.\\]\n\nPour faire des tests, il suffit donc de trouver la loi asymptotique de cette statistique.\n\nThéorème 8.2 Sous l’hypothèse nulle Équation 8.2, la statistique \\(D_n\\) converge en loi vers \\(\\chi_2(k-1)\\). De plus, sous l’hypothèse alternative Équation 8.3, \\(D_n\\) tend vers \\(+\\infty\\) presque sûrement.\n\n\nPreuve. Comme \\(|\\sqrt{\\mathbf{p}_0}|\\) vaut 1, la matrice \\(\\pi_0=I_k -\\sqrt{\\mathbf{p}_0} \\sqrt{\\mathbf{p}_0}^T\\) est la matrice de projection sur l’orthogonal du vecteur \\(\\sqrt{\\mathbf{p}_0}\\) (je vous renvoie à l’appendice Chapitre 18). Le théorème de Cochran (Théorème 11.3) implique alors que la statistique \\(D_n\\), qui est égale à \\[\n\\left| \\mathrm{diag}(1/\\sqrt{\\mathbf{p}_0}) \\sqrt{n}\\left(\\frac{N}{n}- \\mathbf{p}_0\\right) \\right |^2,  \\tag{8.4}\\] converge en loi vers la norme de la projection d’une gaussienne \\(N(0,I_k)\\) sur un sous-espace de dimension \\(k-1\\), c’est-à-dire une loi \\(\\chi_2(k-1)\\). Sous l’hypothèse alternative, il y a au moins un \\(p_i\\) non nul tel que \\(p_i \\neq (p_0)_i\\). Ainsi, Équation 8.4 est plus grand que \\(n(N_i/n - (p_0)_i)^2 / p_i\\), mais \\(N_i\\) suit une loi \\(\\mathrm{Bin}(n,p_i)\\) et donc \\(N_i / n\\) converge en probabilité vers \\(p_i\\). Il est alors clair que \\(n(N_i/n - (p_0)_i)\\) converge vers \\(+\\infty\\).\n\nUn test de niveau \\(1-\\alpha\\) pour l’hypothèse Équation 8.2 est alors donné par la région de rejet \\[ \\{ D_n(\\mathbf{p}_0) &gt; \\kappa_{k-1, 1-\\alpha} \\}\\]\noù \\(\\kappa_{k-1, 1-\\alpha}\\) est le quantile d’ordre \\(1-\\alpha\\) d’une \\(\\chi^2(k-1)\\). Si \\(\\mathbf{p}\\) n’est pas égal à \\(\\mathbf{p}_0\\), le contraste \\(D_n\\) tend vers l’infini, donc le test sera forcément dans la zone de rejet : si l’hypothèse alternative est simple, la puissance du test tend donc vers 1."
  },
  {
    "objectID": "ch4_2.html#test-dindépendance",
    "href": "ch4_2.html#test-dindépendance",
    "title": "8  Tests du \\(\\chi_2\\)",
    "section": "8.3 Test d’indépendance",
    "text": "8.3 Test d’indépendance\nLes tests du \\(\\chi_2\\) d’indépendance sont omniprésents en sciences humaines. Dans ces tests, on observe des variables aléatoires qui sont des couples à valeur dans deux espaces discrets ; disons, pour simplifier, que cet espace est \\(\\Omega = \\{1, \\dotsc, k\\}\\times \\{1, \\dotsc, h\\}\\). Les observations \\((x_i, y_i)\\) sont des réalisations d’une variable aléatoire \\((X,Y)\\). Ici, le modèle statistique sera donc \\((\\mathbf{p}^{\\otimes n} : \\mathbf{p} \\in S_{k,h})\\), où \\(S_{k,h}\\) est l’ensemble des \\(\\mathbf{p} = (p_{i,j}, i \\in \\{1,\\dots, k\\}, j\\in \\{1, \\dots, h\\})\\) qui sont des lois de probabilité.\nSi \\(\\mathbf{p}\\) est la loi de \\((X,Y)\\), alors \\(X\\) et \\(Y\\) sont indépendantes si et seulement si \\(\\mathbf{p}\\) peut s’écrire sous la forme \\(p_{i,j} = p^x_i p^y_j\\), où \\(\\mathbf{p}^x \\in S_k\\) et \\(\\mathbf{p}^y \\in S_h\\). L’ensemble de ces lois sera noté \\(I_{k,h}\\) (« I » pour « Indépendant » ). Les tests d’indépendance visent à tester l’hypothèse nulle \\[ H_0 : \\mathbf{p}\\in I_{k,h} \\tag{8.5}\\] contre l’hypothèse alternative \\[ H_1 : \\mathbf{p} \\notin I_{k,h}.\\]\n\nExemple 8.2 On récolte des données sur le groupe socio-professionnel (GSP) et le genre. Chaque observation correspond à une personne, possédant deux attributs : \\(\\mathtt{genre}\\), valant 0 ou 1, et \\(\\mathtt{GSP}\\), valant l’une des 6 groupes définis par l’INSEE (Agriculteur, artisan, cadre, etc.). Le test ci-dessus vise à déterminer si les deux modalités sont indépendantes, c’est-à-dire si la proportion d’hommes et de femmes dans chaque groupe ne diffère pas significativement en fonction du groupe.\n\nLa procédure pour effectuer un tel test nécessite plusieurs étapes.\nSi \\(\\mathbf{p}\\) était effectivement la loi de deux variables indépendantes \\(\\mathbf{p}^x\\) et \\(\\mathbf{p}^y\\), alors ses marginales seraient précisément \\(\\mathbf{p}^x\\) et \\(\\mathbf{p}^y\\), que l’on pourrait facilement estimer. Pour chaque \\(i\\) et chaque \\(j\\), les estimateurs \\(\\hat{\\mathbf{p}}^x\\) et \\(\\hat{\\mathbf{p}}^y\\) définis par \\[\\hat{p}^x_i = \\frac{\\sum_{j=1}^h N_{i,j}}{n}\\] et \\[\\hat{p}^y_j = \\frac{\\sum_{i=1}^k N_{i,j}}{n}\\] sont effectivement des estimateurs sans biais et convergents des quantités \\(p^x_i, p^y_j\\). De plus, sous l’hypothèse nulle, \\(\\hat{p}^x_i \\hat{p}^y_i\\) serait effectivement un estimateur convergent de \\(p_{i,j}\\).\nDe plus, si \\(\\mathbf{p}\\) était effectivement de la forme \\(\\hat{\\mathbf{p}}^x\\hat{\\mathbf{p}}^y\\), alors la moyenne théorique des éléments de classe \\((i,j)\\) serait \\(n\\hat{p}^x_i \\hat{p}^y_j\\). Cette quantité, notée \\(\\check{N}_{i,j}\\), s’appelle effectif théorique. Nous pouvons maintenant construire la statistique qui nous servira à tester tout cela.\n\nDéfinition 8.2 (Statistique de Pearson) La statistique de Pearson est définie par\n\\[C_n = \\sum_{i=1}^k \\sum_{j=1}^k \\frac{(N_{i,j} - \\check{N}_{i,j})^2}{\\hat{N}_{i,j}}. \\]\n\nCette statistique possède une loi limite connue, encore en vertu du théorème de Cochran. Noter que la statistique de Pearson possède une expression alternative, \\[C_n = \\sum\\sum \\frac{n(\\hat{p}_{i,j} - \\hat{p}^x_i \\hat{p}^y_j)^2}{\\hat{p}^x_i \\hat{p}^y_j}.  \\]\n\nThéorème 8.3 (Loi de la statistique de Pearson) Sous l’hypothèse nulle Équation 8.5, \\(C_n\\) converge en loi vers \\[ \\chi_2((k-1)(h-1)).\\] De plus, pour n’importe quelle loi \\(\\mathbf{p}_1\\) qui n’est pas dans \\(I_{k,h}\\), \\(C_n \\to +\\infty\\) presque sûrement.\n\n\nPreuve. C’est une conséquence un peu plus technique du théorème de Cochran.\n\nTout cela permet encore une fois d’obtenir des tests très efficacement : en abrégeant \\(\\kappa_{1 - \\alpha} = \\kappa_{(k-1)(h-1), 1-\\alpha}\\), on obtient que \\(\\mathbb{P}(C_n &gt; \\kappa_{1-\\alpha}) \\to \\alpha\\). Ainsi, la région de rejet \\[\\{C_n &gt; \\kappa_{1-\\alpha}\\} \\] fournit un test de niveau asymptotique \\(1-\\alpha\\). La seconde partie du théorème dit que si la véritable loi sous-jacente n’est effectivement pas la loi de deux variables indépendantes, alors ce test sera systématiquement rejeté — autrement dit, si l’hypothèse alternative est simple, la puissance de ce test tend vers 1."
  },
  {
    "objectID": "ch4_ex.html",
    "href": "ch4_ex.html",
    "title": "٭ Exercices",
    "section": "",
    "text": "Annales de partiel\nJe ne garantis pas que les notations et les concepts utilisés dans ces annales soient en phase avec le cours de cette année !"
  },
  {
    "objectID": "ch4_ex.html#questions",
    "href": "ch4_ex.html#questions",
    "title": "٭ Exercices",
    "section": "Questions",
    "text": "Questions\n\nQuelles sont les erreurs du test consistant à toujours accepter l’hypothèse nulle ?\nQuelles sont les erreurs du test consistant à toujours refuser l’hypothèse nulle ?\nMontrer que la distance en variation totale entre deux mesures de densités \\(p,q\\) peut aussi s’écrire \\(\\int(p/q-1)_+ \\mathrm{d}p\\).\nMontrer que si \\(\\mathrm{d}_{\\mathrm{KL}}(P_n \\mid Q) \\to 0\\), alors \\(P_n\\) converge en loi vers \\(Q\\).\nCalculer la distance en variation totale entre deux lois de Bernoulli de paramètres respectifs \\(p\\) et \\(q\\).\nCalculer la distance en variation totale entre une loi \\(\\mathrm{Bin}(n,p)\\) et une loi \\(N(\\mu,\\sigma^2)\\).\nSoient \\(P,Q\\) deux mesures. Montrer que \\(\\dkl(P^{\\otimes n} \\mid Q^{\\otimes n}) = n \\dkl(P \\mid Q)\\)."
  },
  {
    "objectID": "ch4_ex.html#tests-élémentaires",
    "href": "ch4_ex.html#tests-élémentaires",
    "title": "٭ Exercices",
    "section": "Tests élémentaires",
    "text": "Tests élémentaires\nPour tous les cas suivants, il faut savoir réaliser rapidement un test puissant, voire même optimal au sens qu’il vous plaira.\n\nTester \\(\\mu = \\mu_0\\) contre \\(\\mu = \\mu_1\\) dans un échantillon \\(N(\\mu, \\sigma^2)\\) lorsque \\(\\sigma\\) est connu.\nMême question lorsque \\(\\sigma\\) est inconnu.\nSoient \\(X_1, \\dotsc, X_n\\) un échantillon iid \\(N(\\mu_1,\\sigma_1^2)\\) et \\(Y_1, \\dotsc, Y_m\\) (\\(m\\) et \\(n\\) ne sont pas forcément égaux) un échantillon iid de loi \\(N(\\mu_2,\\sigma_2^2)\\). Tester \\(\\sigma_1 = \\sigma_2\\) lorsque \\(\\mu_1\\) et \\(\\mu_2\\) sont connues.\nMême question lorsque \\(\\mu_1\\) et \\(\\mu_2\\) ne sont pas connues.\nDonner la forme d’un test sur la valeur de \\(p\\) pour une réalisation d’une loi \\(\\mathrm{Bin}(n,p)\\) et calculer son niveau asymptotique quand \\(n\\to\\infty\\).\nDonner la forme d’un test sur la valeur de \\(\\lambda\\) dans un échantillon de \\(n\\) variables aléatoires de Poisson de paramètre \\(\\lambda\\)."
  },
  {
    "objectID": "ch4_ex.html#exercices",
    "href": "ch4_ex.html#exercices",
    "title": "٭ Exercices",
    "section": "Exercices",
    "text": "Exercices\n\nExercice 1 Soient \\(X_1, \\dotsc, X_n\\) des variables indépendantes de loi \\(\\chi_2(p)\\). On cherche à tester l’hypothèse nulle \\(p=1\\) contre l’hypothèse alternative \\(p=2\\).\n\nÉcrire la forme de la région de rejet des tests de rapport de vraisemblance.\nEssayer de calculer le niveau de ce test ; si ce n’est pas possible, essayer de le borner.\n\n\n\nExercice 2 Soient \\(X_1, \\dotsc, X_n\\) des variables indépendantes de loi \\(N(0,\\sigma^2)\\). Proposer un test de niveau \\(\\alpha\\) de l’hypothèse \\(\\sigma^2=1\\) contre l’hypothèse \\(\\sigma^2 = 1+\\varepsilon\\), et estimer sa puissance. Comment varie-t-elle en fonction de \\(n\\) et de \\(\\varepsilon\\) ?\n\n\nExercice 3 Soient \\(X_1, \\dotsc, X_n\\) des variables indépendantes de même loi \\(P\\). On cherche à tester l’hypothèse nulle \\(P = N(0,1)\\) contre l’hypothèse alternative \\(P = \\mathscr{T}(n)\\).\n\nDonner le test optimal au sens de l’affinité.\nDonner un autre test, de niveau \\(1-\\alpha\\), et calculer sa puissance.\nComparer ces deux tests, en particulier dans le régime où \\(n\\) est grand.\n\n\n\nExercice 4 Montrer que le nombre de lancers nécessaire pour distinguer une pièce équilibrée \\((p=1/2)\\) d’une pièce légèrement déséquilibrée (\\(p_1 = 1/2 + \\varepsilon\\)) est d’ordre \\(1/\\varepsilon^2\\).\n\n\nExercice 5 On note \\(p\\) la probabilité qu’un enfant né vivant soit un garçon. On suppose que les enfants sont de sexe indépendants, et que cette probabilité est la même pour toutes les grossesses.\n\nIl y a eu en France métropolitaine en 2015 \\(n=760\\,421\\) naissances. , dont \\(389\\,181\\) garçons. Tester l’hypothèse \\(p=\\frac12\\) contre l’alternative pertinente.\nEn 1920, il y a eu \\(838\\,137\\) naissances dont \\(432\\,044\\) garçons. Tester l’hypothèse \\(p_{2015}=p_{1920}\\).\n\n\n\nExercice 6 Soient \\(X_1, \\dotsc, X_n\\) i.i.d de loi \\(N(\\theta,1)\\), où \\(\\theta\\) est un paramètre réel.\n\nDonner un intervalle de confiance pour \\(\\theta\\) au niveau de risque \\(5\\%\\) de la forme \\([\\hat{\\theta}_n, +\\infty[\\).\nEn déduire un test de niveau \\(5\\%\\) pour les hypothèses \\(H_0: \\theta = 0\\) et \\(H_1: \\theta &gt;0\\).\nDonner le modèle de l’expérience statistique. Donner l’expression du test de rapport de vraisemblance \\(T\\) pour les hypothèses \\(H_0: \\theta =0\\) et \\(H_1: \\theta = \\mu\\), où \\(\\mu &gt;0\\). Quel test retrouve-t-on?\nConstruire le test de rapport de vraisemblance au niveau \\(5\\%\\) pour les hypothèses \\(H_0: \\theta = 0\\) et \\(H_1: \\theta &gt;0\\).\n\n\n\nExercice 7 (Test sur des lois uniformes) On se donne \\(X_1, \\dots, X_n\\) iid de loi \\(\\mathscr{U}(0,\\theta)\\), et on note \\(M_n = \\max_{j=1, \\dots, n} X_i\\).\n\nÉcrire la fonction de répartition de \\(M_n\\), puis en déduire un test \\(T\\) de niveau \\(1-\\alpha\\) pour les hypothèses \\(H_0: \\theta = 1\\) contre \\(H_1: \\theta &lt; 1\\).\nDonner le test du rapport de vraisemblance pour pour les hypothèses \\(H_0: \\theta = 1\\) contre \\(H_1: \\theta = \\theta_0\\), où \\(\\theta_0&lt;1\\). Calculer sa puissance.\nOn cherche à tester \\(H_0: \\theta = 1\\) contre \\(H_1: \\theta &lt; 1\\). Comme la seconde hypothèse est composite, on ne peut pas directement appliquer le test du rapport de vraisemblance ; à la place, on utilise un test du maximum de vraisemblance, qui est de la forme \\[ \\frac{\\sup_{\\theta &lt; 1}\\rho_\\theta(x_1, \\dotsc, x_n)}{\\rho_1(x_1, \\dotsc, x_n)} &gt; z\\] où \\(\\rho_\\theta\\) est la densité d’un échantillon iid de lois \\(\\mathscr{U}[0,\\theta]\\). Calculer le supremum dans cette expression, et en déduire la région de rejet.\nMontrer que la puissance de \\(T\\) vaut \\(1-\\alpha\\).\nEn utilisant la même technique, construire le test du rapport de maximum de vraisemblance pour pour les hypothèses \\(H_0: \\theta = 1\\) contre \\(H_1: \\theta&gt;1\\), noté \\(T'\\), au niveau \\(1-\\alpha\\). Calculer sa puissance.\nDonner un test de niveau \\(1-\\alpha\\) pour \\(H_0: \\theta = 1\\) contre \\(H_1: \\theta&gt;1\\), plus puissant que \\(T'\\) pour n’importe quel \\(\\theta &gt;1\\).\n\n\n\nExercice 8 Une réalisation d’une variable aléatoire \\(X \\sim \\mathrm{Bin}(20,p)\\) donne \\(X = 8\\).\n\nProposer un test du rapport de vraisemblance de l’hypothèse nulle \\(p=p_0=1/2\\) contre l’hypothèse alternative \\(p=p_1=1/3\\). Donner l’expression de la \\(p\\)-valeur du test.\nOn tire des variables aléatoires iid de Bernoulli jusqu’à obtenir 8 succès. Écrire la loi de probabilité du nombre de lancers \\(N\\).\nIl se trouve que le nombre de lancers nécessaires pour cela était \\(N=20\\). Proposer un test du rapport de vraisemblance de l’hypothèse nulle \\(p=p_0=1/2\\) contre l’hypothèse alternative \\(p=p_1=1/3\\). Donner l’expression de la \\(p\\)-valeur du test.\nPourquoi les deux \\(p\\)-valeurs sont-elles différentes, alors que les deux tests sont identiques ?\n\n\n\nExercice 9 (Test d’adéquation du \\(\\chi_2\\)) On lance \\(60\\) fois un dé et on obtient les résultats suivants :\n\n\n\nFace \\(k\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\nEffectif \\(N_k\\)\n10\n13\n8\n12\n9\n8\n\n\n\nLe dé est-il bien équilibré ? À titre indicatif, le quantile d’une loi \\(\\chi^2(5)\\) d’ordre \\(95\\%\\) est \\(11.07\\).\n\n\nExercice 10 (Test d’indépendance du \\(\\chi_2\\)) On cherche à savoir si les variables « être riche » et « être heureux » sont indépendantes. On interroge un grand échantillon de personnes à ce sujet, et l’on récolte les données suivantes :\n\n\n\n\nriche\npauvre\n\n\n\n\n\nheureux\n344\n700\n\n\n\ntriste\n257\n705\n\n\n\n\nL’argent fait-il le bonheur ?"
  },
  {
    "objectID": "ch5_0.html#ajustement-affine-en-une-dimension.",
    "href": "ch5_0.html#ajustement-affine-en-une-dimension.",
    "title": "9  Moindres carrés",
    "section": "9.1 Ajustement affine en une dimension.",
    "text": "9.1 Ajustement affine en une dimension.\nOn suppose qu’il existe entre les données \\(x_i\\) et \\(y_i\\) une relation de la forme \\(y_i \\approx \\alpha + \\beta x_i\\) où \\(\\alpha, \\beta\\) sont deux nombres réels. Ici, \\(\\approx\\) signifie que la relation n’est pas parfaite : peut-être par exemple que les sorties sont bien égales à \\(\\alpha+\\beta x_i\\), mais que les observations \\(y_i\\) ont été polluées par du bruit ou des erreurs. Nous verrons cela plus tard.\nPour l’heure, nous voulons chercher les meilleurs \\(\\alpha, \\beta\\) possibles. On calcule la distance entre le nuage de points \\((x_i, y_i)\\) et la droite d’équation \\(y = \\alpha + \\beta x\\). Cette distance au carré est donnée par \\[ L(\\alpha, \\beta) = \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2.\\]\nOn cherchera donc les \\((\\hat\\alpha, \\hat\\beta)\\) qui minimisent cette distance. La fonction \\(L\\) est manifestement une fonction quadratique qui tend vers \\(+\\infty\\) lorsque \\((\\alpha, \\beta) \\to \\infty\\), par conséquent cette fonction possède un unique minimiseur \\((\\hat{\\alpha}, \\hat{\\beta})\\), et ce minimiseur est le seul point en lequel les dérivées partielles s’annulent (conditions de premier ordre) : \\(\\partial_\\alpha L(\\hat{\\alpha}, \\hat{\\beta}) = 0\\) et \\(\\partial_\\beta L(\\hat{\\alpha}, \\hat{\\beta})=0\\). Or, \\[\\partial_\\alpha L(\\alpha, \\beta)  = \\sum_{i=1}^n (\\alpha + \\beta x_i - y_i)\\] \\[ \\partial_\\beta L(\\alpha, \\beta) = \\sum_{i=1}^n x_i(\\alpha + \\beta x_i - y_i).\\] Les conditions de premier ordre deviennent donc \\(n\\alpha + \\beta (x_1 + \\dotsc + x_n) - (y_1 + \\dotsb + y_n) =0\\) soit encore \\(\\alpha + \\beta \\bar{x} - \\bar{y}=0\\), et d’autre part \\(\\alpha (x_1 + \\dotsb + x_n) + \\beta(x_1^2 + \\dotsb + x_n^2) - (x_1y_1 + \\dotsb + x_ny_n) = 0\\), soit \\(\\alpha \\bar{x}+ \\beta \\overline{xx} - \\overline{xy} = 0\\), où \\(\\overline{xx}\\) est la moyenne des carrés des \\(x_i\\) et \\(\\overline{xy}\\) la moyenne des \\(x_iy_i\\). En résolvant ces équations, on trouve d’abord \\(\\alpha\\) puis \\(\\beta\\) :  \\[\\beta = \\frac{\\overline{xy}-\\bar{x}\\bar{y}}{\\overline{xx} - \\bar{x}\\bar{x}} ,~~\\quad\\alpha = \\bar{y} - \\hat{\\beta}\\bar{x}.\\] Le coefficient \\(\\beta\\) n’est rien d’autre que la covariance empirique des \\(x_i\\) et des \\(y_i\\), normalisé par la variance empirique des \\(x_i\\).\nL’inégalité de Cauchy-Schwartz dit que \\(\\left|\\overline{xy} - \\bar{x}{\\bar{y}}\\right| \\leqslant \\tilde{\\sigma}_x \\tilde{\\sigma}_y\\), où l’on a noté \\[\\tilde{\\sigma}_x^2 = \\frac{1}{n}\\sum_{i=1}^n x_i^2 - \\left(\\frac{1}{n}\\sum_{i=1}^n x_i\\right)^2\\] l’estimateur naïf de la variance[^1]. L’inégalité n’est une égalité que si \\(x\\) et \\(y\\) sont effectivement colinéaires, c’est-à-dire si \\(y_i = \\hat{\\alpha} + x_i \\hat{\\beta}\\) pour tous les \\(i\\). La qualité de l’ajustement affine est donc bien mesurée par la quantité \\[ R^2 = \\frac{\\overline{xy}-\\bar{x}\\bar{x}}{\\tilde{\\sigma}_x \\tilde{\\sigma}_y}.\\]"
  },
  {
    "objectID": "ch5_0.html#moindres-carrés-ordinaires",
    "href": "ch5_0.html#moindres-carrés-ordinaires",
    "title": "9  Moindres carrés",
    "section": "9.2 Moindres carrés ordinaires",
    "text": "9.2 Moindres carrés ordinaires\nDans le cadre général le nombre \\(d\\) de variables explicatives est plus grand que 1. On notera \\(\\bx = (x_1, \\dotsc, x_d)\\) un élément de \\(\\mathbb{R}^d\\) ; les variables explicatives seront alors \\(\\bx_1,\\dotsc, \\bx_n\\). Avec mes notations, ces vecteurs sont des vecteurs lignes1.\nOn cherchera donc des nombres \\(\\theta_i\\) tels que \\(y_i\\) est aussi proche que possible de \\[\\theta_1 \\bx_{i,1} + \\dotsb + \\theta_d \\bx_{i,d} = \\bx_i \\bt,  \\] oùe paramètre \\(\\bt\\), sera toujours vu comme le vecteur colonne2 des \\(\\theta_i\\).\nRemarque : où est passée la constante ? Dans l’équation ci-dessus, on a l’impression que le terme constant, qui correspondait à \\(\\alpha\\) dans l’exemple en dimension 1, a disparu. Ce n’est pas le cas : intégrer la constante au modèle revient à considérer que la variable constante égale à 1 fait partie des variables explicatives. En pratique, cela revient à poser, par exemple, \\(\\bx_{i,1} = 1\\) pour tout \\(i\\). Ainsi, la constante correspondra toujours à \\(\\theta_1\\).\nOn pose \\(X\\) la matrice \\(n\\times d\\) dont la \\(i\\)-ème ligne est \\(\\bx_i\\) et \\(Y\\) le vecteur colonne des \\(y_i\\). La matrice dont les lignes sont composées des nombres réels \\(\\bx_i \\bt\\) n’est autre que la matrice \\(X\\bt\\). De façon générale, pour n’importe quel \\(\\theta \\in \\mathbb{R}^d\\), la distance entre le nuage de points \\((X,Y)\\) et la droite d’équation \\(Y = X\\theta\\) est alors \\(|Y - X\\theta|\\). On pourrait reproduire la méthode analytique ci-dessus pour trouver les paramètres optimaux, à savoir \\[\\hat{\\bt} = \\arg \\min_{\\theta} |Y - X\\theta|^2.  \\tag{9.1}\\] Cependant, une interprétation géométrique simplifie la tâche : le \\(\\hat{\\bt}\\) qui minimise Équation 9.1 est précisément celui qui garantit que \\(X\\hat{\\bt}\\) est la projection orthogonale de \\(Y\\) sur le sous-espace vectoriel \\(\\mathscr{V}_X = \\{X\\theta : \\theta \\in \\mathbb{R}^d\\}\\).\n\nThéorème 9.1 Si \\(d\\leqslant n\\) et si \\(X\\) est de rang \\(d\\), alors\n\\[\\hat{\\bt} = (X^\\top X)^{-1}X^\\top Y. \\tag{9.2}\\]\n\n\nPreuve. La projection orthogonale sur le sous-espace vectoriel engendré par les colonnes d’une matrice \\(X\\) est la matrice \\(X(X^\\top X)^{-1}X^\\top\\), comme démontré dans l’appendice Chapitre 18. Ainsi, la projection de \\(Y\\) sur ce sous-espace est \\(X(X^\\top X)^{-1}X^\\top Y\\), et c’est aussi (par définition de l’argmin) \\(X \\hat{\\bt}\\). Comme \\(X\\) est injective en vertu du théorème du rang, on en déduit le résultat.\n\nL’expression Équation 9.2 possède de nombreuses expressions alternatives. Parmi elles, on pourra noter que \\[\\hat{\\bt} = \\bt + \\left(\\frac{1}{n}\\sum_{i=1}^n \\bx_i^\\top\\bx_i \\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n \\bx_i^\\top \\varepsilon_i.  \\tag{9.3}\\]\nRemarque générale. Si, en dimension 1, on cherche à trouver le \\(\\theta\\) qui résout l’équation \\(y = x\\theta\\), on trouve évidemment \\(\\theta = y/x\\), c’est-à-dire qu’on divise \\(y\\) par \\(x\\), ou encore qu’on multiplie \\(y\\) par l’inverse de \\(x\\). En dimension supérieure, quand on veut résoudre en \\(\\theta\\) l’équation \\(Y = X\\theta\\), c’est pareil. Le problème, c’est qu’on ne sait pas forcément inverser \\(X\\). La formule Équation 9.2 dit que même si \\(X\\) n’est pas inversible, on peut quand même “diviser par \\(X\\)” : c’est pour cela que la matrice \\((X^\\top X)^{-1} X^\\top\\) est appelée pseudo-inverse à gauche de \\(X\\) – parfois associée au nom de Moore-Penrose. Multiplier \\(Y\\) par \\((X^\\top X)^{-1}X^\\top\\) donne \\((X^\\top X)^{-1}X^\\top Y = \\hat{\\bt}\\) : ce vecteur ne vérifie par forcément \\(X{\\hat{\\bt}} = Y\\), mais parmi tous les vecteurs possibles, c’est celui qui rend \\(X\\bt\\) le plus proche possible de \\(Y\\)."
  },
  {
    "objectID": "ch5_0.html#résidus-et-r2",
    "href": "ch5_0.html#résidus-et-r2",
    "title": "9  Moindres carrés",
    "section": "9.3 Résidus et \\(R^2\\)",
    "text": "9.3 Résidus et \\(R^2\\)\nLe vecteur \\(\\hat{Y} = X\\hat{\\bt}\\) est appelé vecteur des prédictions. Le vecteur \\(\\hat{\\varepsilon} = Y-\\hat{Y} = Y - X\\hat{\\bt}\\) est appelé vecteur des résidus. Si ce dernier est nul ou très petit, cela veut dire que les \\(Y\\) sont presque parfaitement des fonctions linéaires des \\(X\\).\n\nDéfinition 9.1 Dans le cas d’une régression Équation 9.2 avec constante, le coefficient de détermination est défini par \\[R^2 = \\frac{\\sum_{i=1}^n |\\hat{y}_i - \\bar{y}|^2}{\\sum_{i=1}^n |y_i - \\bar{y}|^2}. \\] C’est un nombre entre 0 et 1.\n\nLe numérateur est la variance empirique des prédictions \\(\\hat{y}_i\\). Le dénominateur est la variance empirique des observations. Dans les des cas, il s’agit de la norme carrée d’un vecteur (\\(\\hat{Y}\\) et \\(Y\\)) projeté sur l’espace des vecteurs de moyenne nulle. Comme \\(\\hat{Y}\\) est déjà une projection de \\(Y\\) sur un certain sous-espace, on a forcément \\(|\\hat{Y} - \\bar{y}| \\leqslant |Y - \\bar{y}|\\), donc le coefficient \\(R^2\\) est toujours entre 0 et 1.\nPlus le coefficient de détermination est proche de 1, meilleure est la régression – attention, cet indicateur possède de nombreuses limites."
  },
  {
    "objectID": "ch5_0.html#footnotes",
    "href": "ch5_0.html#footnotes",
    "title": "9  Moindres carrés",
    "section": "",
    "text": "Ils sont de dimension \\((1,d)\\) si on les voit comme des matrices↩︎\nDonc, de dimension \\((d,1)\\) cette fois. ↩︎"
  },
  {
    "objectID": "ch5_01.html#sec-mlg",
    "href": "ch5_01.html#sec-mlg",
    "title": "10  Modèles linéaires",
    "section": "10.1 Modèle gaussien",
    "text": "10.1 Modèle gaussien\nÀ ce stade, nous n’avons fait aucune hypothèse statistique ni probabiliste sur le modèle : les \\(\\bx_i, y_i\\) étaient donnés tels quels. Le modèle linéaire gaussien avec variables explicatives \\(\\bx_1, \\dotsc, \\bx_n\\) exogènes consiste à supposer que \\(Y = X\\bt + \\varepsilon\\), où \\(\\varepsilon = N(0,\\sigma^2 I_n)\\). Formellement, le modèle est indexé par \\(\\bt\\) et \\(\\sigma^2\\), et donné par \\[P_{\\bt, \\sigma^2} = N(X\\bt, \\sigma^2 I_d).\\] Dans ce modèle, la loi de l’estimateur Équation 9.2 est connue. Par simplicité, je note \\(H = X(X^\\top X)^{-1}X^\\top\\) la matrice de projection orthogonale sur l’espace vectoriel engendré par les colonnes de \\(X\\), qui est de dimension \\(d\\).\n\nThéorème 10.1 (Loi de \\(\\hat{\\theta}\\)) Sous le modèle linéaire gaussien \\(P_{\\bt, \\sigma^2}\\), \\[\\hat{\\bt} \\sim N(\\bt, \\sigma^2 (X^\\top X)^{-1}), \\] \\[\\frac{|\\hat{\\varepsilon}|^2}{\\sigma^2} \\sim \\chi_2(n-d), \\] et ces deux variables aléatoires sont indépendantes.\n\n\nPreuve. Ce n’est rien de plus que le théorème de Cochran appliqué à notre problème : en effet, le vecteur des résidus est la projection orthogonale de \\(Y\\) sur le sous-espace orthogonal à l’espace des colonnes de \\(X\\).\n\nLa variable aléatoire \\(|\\hat\\varepsilon|^2\\) est souvent appelée Somme des Carrés des Résidus (SCR). Le théorème précédent implique que \\[\\hat{\\sigma}^2_n = \\frac{|\\hat\\varepsilon|^2}{n-d}\\] est un estimateur sans biais de \\(\\sigma^2\\). et ces deux variables aléatoires sont indépendantes. En particulier, \\((n-d)\\hat{\\sigma}^2_n/\\sigma^2 \\sim \\chi_2(n-d)\\)."
  },
  {
    "objectID": "ch5_01.html#sec-mlgen",
    "href": "ch5_01.html#sec-mlgen",
    "title": "10  Modèles linéaires",
    "section": "10.2 Modèle linéaire général",
    "text": "10.2 Modèle linéaire général\nIl est possible de ne pas faire d’hypothèses gaussiennes sur le modèle. Dans ce cadre plus général, on supposera que \\(Y = X\\bt + \\varepsilon\\), où les \\(\\varepsilon_i\\) sont iid, centrés, et de même variance \\(\\sigma^2\\) — sous cette dernière hypothèse, on parle de modèle homoscédastique.\nSous ces hypothèses, \\(\\hat{\\bt}\\) est toujours un estimateur sans biais de \\(\\bt\\) : cela se voit directement en prenant l’espérance de Équation 9.3. De plus, la loi de \\(\\bt\\) n’est plus gaussienne, mais \\(\\bt\\) est asymptotiquement normal sous des hypothèses supplémentaires sur \\(X\\). Ces hypothèses sont les suivantes.\n\nOn suppose que les variables explicatives \\(\\bx_i\\) vérifient la propriété suivante1 :  \\[ \\lim_{n \\to \\infty}\\frac{1}{n}\\sum_{i=1}^n \\bx_i^\\top \\bx_i = \\Sigma_x, \\tag{10.1}\\] où \\(\\Sigma_x\\) est inversible. Cette propriété s’écrit aussi \\(X^\\top X / n \\to \\Sigma_x\\).\n\n\nThéorème 10.2 Sous les hypothèses précédentes, \\(\\sqrt{n}(\\hat{\\bt} - \\bt)\\) converge en loi lorsque \\(n\\to\\infty\\) vers \\(N(0,\\sigma^2\\Sigma_x^{-1}).\\)\n\n\nPreuve. Rappelons que \\(\\hat{\\bt}\\) peut s’écrire \\(\\bt + (X^\\top X/n)^{-1}\\frac{1}{n}\\sum_{i=1}^n \\bx_i^\\top \\varepsilon_i\\). Pour montrer que \\(\\sqrt{n}(\\hat{\\bt}-\\bt)\\) converge, il suffit donc de démontrer que \\[\\sqrt{n}\\frac{1}{n}\\sum_{i=1}^n \\bx_i^\\top \\varepsilon_i  \\tag{10.2}\\] converge en loi vers \\(N(0,\\Sigma_x^2)\\) : comme le terme \\((X^\\top X/n)^{-1}\\) converge vers \\(\\Sigma_x^{-1}\\) par hypothèse, la limite de \\(\\sqrt{n}(\\hat{\\bt} - \\bt)\\) sera bien \\(N(0,\\Sigma_x^{-1}\\Sigma_x\\Sigma_x^{-1}) = N(0,\\Sigma_x^{-1})\\). Malheureusement, on ne peut pas directement appliquer le TCL classique à Équation 10.2 : en effet, les variables aléatoires \\(X_i = \\bx_i^\\top \\varepsilon_i\\) ne sont pas identiquement distribuées. On doit pour cela appliquer une version plus générale du TCL, que j’ai écrite en appendice (Théorème 19.3). Pour appliquer ce théorème en toute rigueur, on a besoin d’une hypothèse supplémentaire sur les \\(\\bx_i\\) que je n’ai pas mentionnée — c’est une hypothèse technique2."
  },
  {
    "objectID": "ch5_01.html#ellipsoïde-de-confiance",
    "href": "ch5_01.html#ellipsoïde-de-confiance",
    "title": "10  Modèles linéaires",
    "section": "10.3 Ellipsoïde de confiance",
    "text": "10.3 Ellipsoïde de confiance\nLes deux théorèmes énoncés ci-dessus permettent de définir des régions de confiance associées à \\(\\bt\\) ; ici, \\(\\bt\\) n’est plus un nombre réel mais un vecteur, d’où le terme de région et plus simplement d’intervalle.\n\nPréliminaire : la variance est connue\nCommençons par construire une région probable pour un vecteur gaussien \\(\\xi \\sim N(0,I_d)\\). Nous savons que \\(|\\xi|^2 \\sim \\chi_2(d)\\). Si \\(\\kappa_{d,1-\\alpha}\\) désigne le quantile d’ordre \\(1-\\alpha\\) de cette loi, on en déduit que \\(\\xi\\) est de norme inférieure à \\(\\sqrt{\\kappa_{d,1-\\alpha}}\\) avec probabilité \\(1-\\alpha\\) ; autrement dit, \\[ \\mathbb{P}(0 \\in B(\\xi, \\sqrt{\\kappa})) = 1-\\alpha.\\] Il est immédiat d’en déduire que si \\(\\xi \\sim N(\\mu, I_d)\\), alors comme \\(\\xi - \\mu \\sim N(0,I_d)\\), on a \\[ \\mathbb{P}(\\mu \\in B(\\xi, \\sqrt{\\kappa})) = 1-\\alpha. \\] Maintenant, en toute généralité, si \\(\\xi \\sim N(\\mu,\\Sigma)\\), alors \\(\\Sigma^{-1/2}(\\xi - \\mu) \\sim N(0,I_d)\\). On en déduit donc que \\[ \\mathbb{P}(\\mu \\in \\Sigma^{1/2} B(\\xi, \\sqrt{\\kappa})) = 1-\\alpha.\\] La région de confiance est donc l’image de la boule \\(B(\\xi, \\sqrt{\\kappa})\\) par la matrice symétrique \\(\\Sigma^{1/2}\\) : c’est une ellipse. Par ailleurs, l’ensemble \\(\\Sigma B(x, \\delta)\\) peut aussi s’écrire \\(\\{y \\in \\mathbb{R}^d : |\\Sigma^{1/2}(x - y)|^2 \\leqslant \\delta\\}\\).\nEn combinant ce résultat avec la loi de \\(\\hat{\\bt}\\) donnée dans Théorème 10.1, on obtient la région de confiance \\[ \\left\\lbrace \\theta \\in \\mathbb{R}^d : \\left|\\frac{1}{\\sigma}(X^{\\top} X)^{1/2}(\\hat{\\bt} - \\theta) \\right|^2 &lt; \\kappa_{d,1-\\alpha} \\right\\rbrace.\\] Malheureusement, cette région nécessite de connaître \\(\\sigma\\). Lorsqu’on ne le connaît pas, il faut l’estimer.\n\n\nCas général\nToujours sous le modèle linéaire gaussien, nous avons vu que la loi de \\(|\\sigma^{-1}(X^{\\top} X)^{1/2}(\\hat{\\bt} - \\bt)|^2\\) est une \\(\\chi_2(d)\\), et que la loi de \\((n-d)\\hat{\\sigma}_n^2\\sigma^{-2}\\) est une \\(\\chi_2(n-d)\\). Par conséquent, la variable aléatoire \\[\\frac{|(X^\\top X)^{1/2}(\\hat{\\bt} - \\bt)|^2}{\\hat{\\sigma}_n^2}\\] a pour loi le rapport de lois du \\(\\chi_2\\) indépendantes de paramètres \\(d\\) et \\(n-d\\). Cette loi est connue : elle est égale à \\(d\\) fois une loi de Fisher dont les propriétés sont données dans Section 11.4. Cela donne directement le théorème suivant.\n\nThéorème 10.3 (Ellipsoïde de confiance) Soit \\(\\hat{\\bt}\\) l’estimateur des MCO dans un modèle linéaire gaussien.\nSi \\(f_{d,n-d,1-\\alpha}\\) est le quantile d’ordre \\(1-\\alpha\\) d’une loi \\(\\mathscr{F}_{d,n-d}\\), alors la région \\[ \\left\\lbrace \\theta \\in \\mathbb{R}^d : \\frac{|(X^\\top X)^{1/2}(\\hat{\\bt} - \\bt) |^2 / d}{\\hat{\\sigma}_n^2 } &lt;  f_{d,n-d,1-\\alpha} \\right\\rbrace \\] est une région de confiance de niveau \\(1-\\alpha\\) pour \\(\\bt\\).\nLorsque le modèle n’est pas gaussien, mais qu’il vérifie les hypothèses de la section Section 10.2, le même résultat est valable mais le niveau de confiance de la région ci-dessus est asymptotiquement égal à \\(1-\\alpha\\)."
  },
  {
    "objectID": "ch5_01.html#footnotes",
    "href": "ch5_01.html#footnotes",
    "title": "10  Modèles linéaires",
    "section": "",
    "text": "On rappelle que \\(\\bx_i\\) est un vecteur ligne de taille \\(d\\), et donc que les matrices \\(\\bx_i^\\top \\bx_i\\) sont bien des matrices carrées de taille \\(d \\times d\\).↩︎\nIl faut que la quantité \\(\\max_{j=1, \\dotsc, n}|\\bx_j|^2 / \\sum |\\bx_i|^2\\) tende vers zéro lorsque \\(n\\to\\infty\\). Cela revient à dire que toute l’information apportée par les \\(x_i\\) n’est pas concentrée sur une seule observation ou sur un très petit nombre d’observations. ↩︎"
  },
  {
    "objectID": "ch5_1.html#vecteurs-gaussiens",
    "href": "ch5_1.html#vecteurs-gaussiens",
    "title": "11  Outils gaussiens",
    "section": "11.1 Vecteurs gaussiens",
    "text": "11.1 Vecteurs gaussiens\nUn vecteur aléatoire \\(X\\) à valeurs dans \\(\\mathbb{R}^n\\) est un vecteur gaussien de loi \\(N(\\mu,\\Sigma)\\) si sa densité est donnée par \\[ \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}}\\exp\\left\\lbrace - \\frac{1}{2}\\left\\langle x-\\mu, \\Sigma^{-1}(x-\\mu)\\right\\rangle \\right\\rbrace.\\] Ici, le vecteur \\(\\mu \\in \\mathbb{R}^n\\) est appelé moyenne de \\(X\\) parce que \\[\\mathbb{E}[X] = \\mu. \\] La matrice \\(\\Sigma\\), qui est toujours supposée symétrique et à valeurs propres strictement positives (on dit définie positive), est appelée matrice de covariance, parce que \\[\\mathbb{E}[(X-\\mu)(X-\\mu)^\\top] = \\Sigma. \\]\nDe même que la transformée de Fourier d’une variable gaussienne réelle \\(N(m, \\sigma^2)\\) est égale à \\(e^{imt - \\frac{t^2\\sigma^2}{2}}\\), la transformée de Fourier d’un vecteur gaussien \\(N(\\mu,\\Sigma)\\) est \\[\\mathbb{E}[e^{i\\langle t, X\\rangle}] = \\exp\\left\\lbrace i\\langle t, \\mu\\rangle - \\frac{\\langle(t-\\mu), \\Sigma (t-\\mu) \\rangle}{2} \\right\\rbrace. \\]\n\nThéorème 11.1  \n\nToute fonction linéaire d’un vecteur gaussien est encore un vecteur gaussien. Si \\(M\\) est une matrice et \\(X \\sim N(\\mu,\\Sigma)\\), \\[MX \\sim N(M\\mu, M\\Sigma M^\\top). \\]\nSi le couple \\((X,Y)\\) forme un vecteur gaussien, alors \\(X\\) et \\(Y\\) sont indépendants si et seulement si leur covariance \\(\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])^\\top]\\) est la matrice nulle."
  },
  {
    "objectID": "ch5_1.html#conditionnement-gaussien",
    "href": "ch5_1.html#conditionnement-gaussien",
    "title": "11  Outils gaussiens",
    "section": "11.2 Conditionnement gaussien",
    "text": "11.2 Conditionnement gaussien\nSoit \\((X,Y)\\) un vecteur gaussien de dimension \\(n+m\\), avec \\(X \\in \\mathbb{R}^n\\) et \\(Y \\in \\mathbb{R}^m\\). On peut écrire sa moyenne \\(\\mu\\) en deux blocs \\[ \\mu = \\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\end{bmatrix}\\] et sa covariance \\(\\Sigma\\) en quatre blocs \\[ \\Sigma = \\begin{bmatrix}\\Sigma_{1,1} & \\Sigma_{1,2} \\\\ \\Sigma_{2,1} & \\Sigma_{2,2} \\end{bmatrix}\\] où , par symétrie, \\(\\Sigma_{2,1} = \\Sigma_{1,2}^\\top\\).\n\nThéorème 11.2 La loi de \\(X\\) conditionnellement à \\(Y\\) est une loi gaussienne de moyenne \\[\\mu_1 + \\Sigma_{1,2}\\Sigma_{2,2}^{-1}(X_2-\\mu_2) \\] et de covariance \\[ \\Sigma_{1,1} - \\Sigma_{2,1}\\Sigma_{2,2}^{-1}\\Sigma_{1,2}.\\]\n\nL’expression loi conditionnelle signifie ici que, pour toute fonction test \\(\\varphi : \\mathbb{R}^n \\to \\mathbb{R}\\), l’espérance conditionnelle \\(\\mathbb{E}[\\varphi(X)\\mid Y]\\), qui est une variable aléatoire \\(Y\\)-mesurable, vaut \\[\\frac{1}{(2\\pi \\det(S^{-1}))^{n/2}}\\int_{\\mathbb{R}^n} \\varphi(x) e^{-\\frac{\\langle X-1 - (\\mu_1 + \\Sigma_{1,2}\\Sigma_{2,2}^{-1}(X_2-\\mu_2)) S^{-1}(X_1 - (\\mu_1 + \\Sigma_{1,2}\\Sigma_{2,2}^{-1}(X_2-\\mu_2)))\\rangle}{2}}dx \\] où \\(S = \\Sigma_{1,1} - \\Sigma_{2,1}\\Sigma_{2,2}^{-1}\\Sigma_{1,2}\\)."
  },
  {
    "objectID": "ch5_1.html#théorème-de-cochran",
    "href": "ch5_1.html#théorème-de-cochran",
    "title": "11  Outils gaussiens",
    "section": "11.3 Théorème de Cochran",
    "text": "11.3 Théorème de Cochran\n\nThéorème 11.3 (Théorème de Cochran) Soit \\(X \\sim N(0,I_n)\\) et soient \\(E_1, \\dotsc, E_k\\) des sous-espaces orthogonaux de \\(\\mathbb{R}^n\\) tels que \\(\\mathbb{R}^n = \\oplus_{j=1}^k E_j\\). On note \\(\\pi_j(X)\\) la projection orthogonale de \\(X\\) sur \\(E_j\\). Alors, la famille \\((\\pi_j(X))_{j = 1, \\dotsc, k}\\) est une famille de vecteurs gaussiens indépendants. De plus, \\[ |\\pi_j(X)|^2 \\sim \\chi_2(\\dim E_j).\\]\n\n\nPreuve. Pour chaque \\(E_i\\), notons \\(d_i\\) sa dimension et choisissons-lui une base orthonormale \\(e^i_1, \\dotsc, e^i_{d_i}\\). La projection orthogonale de \\(X\\) sur \\(E_i\\) est \\(\\pi_i(X)= \\sum_{t=1}^{d_i} \\langle X, e^i_t\\rangle e^i_t\\). Notons \\(X^i_t=\\langle X, e^i_t\\rangle\\). Le vecteur \\((X^i_t)\\) (avec \\(i=1, \\dotsc, k\\) et \\(t = 1, \\dotsc, d_i\\)), qui contient bien \\(d_1+\\dotsb+d_k=n\\) éléments, est une fonction linéaire du vecteur gaussien centré \\(X\\), donc est lui-même un vecteur gaussien centré. Calculons sa covariance : de façon générale, si \\(e,f\\) sont deux vecteurs fixés, \\[\\mathbb{E}[\\langle X, e\\rangle \\langle X, f\\rangle] = \\sum_{i,j}e_if_j \\mathrm{Cov}(X_i, X_j) = \\langle e, f\\rangle.\\] Il est alors immédiat que la matrice de covariance du vecteur gaussien \\((X^i_t)\\) n’est autre que la matrice \\((\\langle e^i_t, e^j_s \\rangle)\\), c’est-à-dire l’identité puisque les \\((e^i_t)\\) forment une base orthonormale de \\(\\mathbb{R}^n\\). Il en résulte les deux points de l’énoncé.\n\nLes \\(\\pi_i(X)\\) sont des variables indépendantes, puisque fonctions linéaires de variables indépendantes entre elles.\nLa formule de Parseval dit que \\[|\\pi_i(X)|^2 = \\sum_{t=1}^{d_i}|X^i_t|^2 \\] ce qui est bien une somme de \\(d_i\\) gaussiennes \\(N(0,1)\\) indépendantes, donc une \\(\\chi_2(d_i)\\)."
  },
  {
    "objectID": "ch5_1.html#sec-fisher",
    "href": "ch5_1.html#sec-fisher",
    "title": "11  Outils gaussiens",
    "section": "11.4 Loi de Fisher",
    "text": "11.4 Loi de Fisher\nSi \\(N\\) est un vecteur et \\(X,Y\\) sont les projections de \\(N\\) sur deux sous-espaces vectoriels orthogonaux, le théorème de Cochran dit que \\(X\\) et \\(Y\\) sont des lois du \\(\\chi_2\\) indépendantes de paramètres \\(p=\\dim E, q = \\dim F\\). La loi de leur rapport \\(X/Y\\) est connue et fréquemment utilisée en statistiques.\n\nThéorème 11.4 Soient \\(X,Y\\) deux variables aléatoires indépendantes, de lois respectives \\(\\chi_2(p)\\) et \\(\\chi_2(q)\\). La loi du rapport \\((X/p)/(Y/q)\\) s’appelle loi de Fisher de paramètres \\(p,q\\), et on la note \\(\\mathscr{F}_{p,q}\\). Sa densité est donnée par \\[ f_{p,q}(x) = \\frac{\\mathbf{1}_{x&gt;0}}{Z_{p,q}}\\frac{\\left(\\frac{px}{px + q}\\right)^{\\frac{p}{2}} \\left(1 - \\frac{px}{px + q}\\right)^{\\frac{q}{2}}}{x} \\tag{11.1}\\] où la constante \\(Z_{p,q}\\) est \\(B(p/2, q/2)\\), c’est-à-dire \\[ Z_{p,q} =  \\int_0^1 u^{\\frac{p}{2}-1}(1-u)^{\\frac{q}{2}-1}du.\\]\n\nLe calcul est facile, puisque les lois du \\(\\chi_2\\) ont une densité connue donnée par Équation 5.2. Soit \\(\\varphi\\) une fonction test et soit \\(F = (X/p)/(Y/q)\\). Alors, \\(\\mathbb{E}[\\varphi(F)]\\) vaut \\[\\frac{1}{C_p C_q}\\int_0^\\infty \\int_0^\\infty \\varphi\\left(\\frac{uq}{vp}\\right)e^{-\\frac{u}{2}-\\frac{v}{2}}u^{\\frac{p}{2}-1}v^{\\frac{q}{2}-1}dudv \\] avec \\(C_n = 2^{n/2}\\Gamma(n/2)\\). Dans l’intégrale en \\(v\\), on pose \\(x = uq/vp\\), de sorte que l’intégrale ci-dessus devient \\[\\frac{(p/q)^{\\frac{p}{2}}}{C_p C_q}\\int_0^\\infty \\varphi(x) x^{\\frac{p}{2}-1}\\int_0^\\infty e^{-\\frac{vpx}{2q}-\\frac{v}{2}}v^{\\frac{p}{2}-1}v^{\\frac{q}{2}}dv dx. \\] On reconnaît dans l’intégrale en \\(v\\) une fonction Gamma, égale à \\[\\frac{\\Gamma(p/2 + q/2)}{\\left(\\frac{px+q}{2q}\\right)^{\\frac{p+q}{2}}}.\\] L’espérance \\(\\mathbb{E}[\\varphi(F)]\\) vaut donc \\[\\frac{(p/q)^{p/2}\\Gamma\\left(\\frac{p+q}{2}\\right)}{C_p C_q (2q)^{\\frac{p+q}{2}}}\\int_0^\\infty \\varphi(x)\\frac{x^{\\frac{p}{2}-1}}{(px + q)^{\\frac{p+q}{2}}}dx. \\] En simplifiant, on trouve exactement la densité donnée par Équation 11.1."
  },
  {
    "objectID": "ch5_2.html#significativité-dun-coefficient",
    "href": "ch5_2.html#significativité-dun-coefficient",
    "title": "12  Tests linéaires",
    "section": "12.1 Significativité d’un coefficient",
    "text": "12.1 Significativité d’un coefficient\nDans une régression de la forme \\(y_i = \\theta_1 \\bx_{i,1}+ \\dotsb + \\theta_d \\bx_{i,d}\\), si le \\(j\\)-ème coefficient \\(\\theta_j\\) est nul, alors cela veut dire que la \\(j\\)-ème variable explicative n’a aucun effet sur la variable expliquée : en effet, \\(\\bx_{i,j}\\) pourrait avoir une toute autre valeur sans modifier la sortie \\(y_i\\). Pour cette raison, le test d’une hypothèse de type \\(\\theta_j = 0\\) s’appelle test de significativité.\nDans un modèle gaussien comme Section 10.1, nous savons que \\(\\hat{\\bt}\\sim N(\\bt, \\sigma^2(X^\\top X)^{-1})\\). Notons \\(\\ell_j^2\\) le \\(j\\)-ème coefficient diagonal de la matrice \\((X^\\top X)^{-1}\\) ; ce nombre est fréquemment appelé levier. Il est explicitement calculable, car il ne dépend que des données d’entrée \\(\\bx_t\\) ; de plus, \\(\\hat{\\theta}_j \\sim N(\\theta_j, \\sigma^2 \\ell_{j}^2)\\), et l’on en déduit (comme dans un test de Student) que sous l’hypothèse nulle \\(\\theta_j=0\\), la statistique \\[\\frac{ \\hat{\\theta}_j}{\\ell_j \\hat{\\sigma}_n} \\] suit une loi de Student \\(\\mathscr{T}(n-d)\\). Il est fréquent d’utiliser la notation \\[ \\hat{\\sigma}(\\hat{\\theta}_j) = \\ell_j \\hat{\\sigma}_n\\] car c’est un estimateur de la variance de \\(\\hat{\\theta}_j\\).\n\nThéorème 12.1 Soit \\(t_{n-d, 1-\\alpha}\\) le quantile symétrique d’ordre \\(1-\\alpha\\) de la loi \\(\\mathscr{T}(n-d)\\). Dans un modèle gaussien, le test ayant pour région de rejet \\[\\left\\lbrace  \\frac{|\\hat{\\theta_j}|}{\\hat{\\sigma}(\\hat{\\theta}_j)}&gt; t_{n-d, 1-\\alpha}\\right\\rbrace\\] est un test de significativité de \\(\\theta_j\\) au niveau \\(1-\\alpha\\).\nLorsque le modèle n’est pas gaussien mais vérifie les conditions de Section 10.2, ce test est asymptotiquement de niveau \\(1-\\alpha\\).\n\nLa statistique \\(|\\hat{\\theta}_j|/\\hat{\\sigma}(\\hat{\\theta}_j)\\) qui apparaît ci-dessus est appelée \\(\\mathsf{t}\\) de Student. Les outils usuels de statistique donnent fréquemment la valeur de cette statistique pour chaque coefficient d’une régression, ainsi que la \\(p\\)-valeur du test qui est égale à \\[1-F_{n-d}(\\mathsf{t}), \\] où \\(F_{n-d}\\) est la fonction de répartition d’une loi \\(\\mathscr{T}(n-d)\\). Cette quantité est fréquemment notée Prob&gt;t."
  },
  {
    "objectID": "ch5_2.html#test-de-contraintes-linéaires",
    "href": "ch5_2.html#test-de-contraintes-linéaires",
    "title": "12  Tests linéaires",
    "section": "12.2 Test de contraintes linéaires",
    "text": "12.2 Test de contraintes linéaires\nLes tests de contraintes linéaires consistent à tester si \\(\\bt\\) vérifie une équation linéaire. Le test de significativité est un test de contrainte linéaire : en notant \\(\\delta_j\\) le vecteur avec des zéro partout sauf en \\(j\\), il s’agit du test de \\(\\langle \\delta_j, \\bt\\rangle = 0\\). On pourrait cependant vouloir tester beaucoup d’autres contraintes de ce type : par exemple, savoir si l’influence de la variable \\(i\\) et de la variable \\(j\\) sont identiques se traduit par \\(\\bt_i = \\bt_j\\), ou encore \\(\\langle \\delta_i - \\delta_j, \\bt\\rangle = 0\\).\nFormellement, un test de contrainte linéaire consiste à tester si \\(\\bt\\) \\[C\\bt = c.\\] où \\(C\\) une matrice \\(r \\times d\\) et \\(c\\) un vecteur de taille \\(r\\). Comme \\(r\\) possède \\(r\\) lignes, cela signifie que l’on test les \\(r\\) contraintes \\(\\langle C_i, \\bt\\rangle = c_i\\), où \\(C_i\\) est la \\(i\\)-ème ligne de \\(C\\).\nSous cette hypothèse nulle, \\[C\\hat{\\bt}-c \\sim N(0, \\sigma^2 C(X^\\top X)^{-1}C^\\top).\\] En multipliant par la matrice \\([\\sigma^{-2}C(X^\\top X)^{-1}C^\\top]^{-1/2}\\) puis en prenant la norme au carré et en simplifiant l’expression, on voit que \\[ \\frac{1}{\\sigma^2}\\langle C\\hat{\\bt}-c, [C(X^\\top X)^{-1}C^\\top]^{-1} (C\\hat{\\bt}-c)\\rangle \\sim \\chi^2(r).\\] Maintenant, si l’on estime le terme \\(\\sigma^2\\) comme d’habitude et que l’on utilise le théorème Théorème 10.1, on obtient la loi de la statistique de test (une loi de Fisher), résumée dans le théorème suivant.\n\nThéorème 12.2 (Test de contraintes linéaires) Sous l’hypothèse nulle \\(C\\bt = c\\), on a \\[ \\frac{\\langle C\\hat{\\bt}-c, [C(X^\\top X)^{-1}C^\\top]^{-1} (C\\hat{\\bt}-c)\\rangle /r}{\\hat{\\sigma}_n^2} \\sim \\mathscr{F}_{r, n-d}. \\tag{12.1}\\]\n\nLa statistique Équation 12.1 est couramment appelée statistique de Wald associée au système linéaire \\(C\\bt = c\\). Formellement, la région de rejet du test de niveau \\(1-\\alpha\\) de ce l’hypothèse nulle \\(C\\bt = c\\) est donc donnée par \\[\\left\\lbrace \\frac{\\langle C\\hat{\\bt}-c, [C(X^\\top X)^{-1}C^\\top]^{-1} (C\\hat{\\bt}-c)\\rangle /r}{\\hat{\\sigma}_n^2}  &gt; f^{r,n-d}_{1-\\alpha} \\right\\rbrace\\] où \\(f\\) est le quantile d’ordre \\(1-\\alpha\\) de la loi \\(\\mathscr{F}_{r,n-d}\\)."
  },
  {
    "objectID": "ch5_2.html#test-de-significativité-globale-de-fisher",
    "href": "ch5_2.html#test-de-significativité-globale-de-fisher",
    "title": "12  Tests linéaires",
    "section": "12.3 Test de significativité globale de Fisher",
    "text": "12.3 Test de significativité globale de Fisher\nLa significativité globale de la régression consiste à tester si tous les coefficients sont significatifs, sauf la constante. Il s’agit donc du test de l’hypothèse nulle \\[ \\theta_2 = \\dotsb = \\theta_d = 0.\\] Il s’agit bien d’un test de contraintes linéaires au sens du paragraphe précédent : il y a exactement \\(d-1\\) contraintes linéaires. En notant \\(C\\) la matrice de taille \\((d-1, d)\\) \\[C = \\begin{pmatrix}0 & 1 && 0 \\\\ \\vdots \\\\ 0 & \\dots && 1 \\end{pmatrix}, \\] on teste bien \\(C\\bt = 0\\). La matrice \\(C(X^\\top X)^{-1}C^\\top\\) n’est autre que le bloc \\(B_X\\) obtenu à partir de \\((X^\\top X)^{-1}\\) en lui enlevant la première ligne et la première colonne (qui correspondent à la constante). La statistique de test devient alors \\[ \\frac{\\langle \\hat{\\bt'}, B_X^{-1} \\hat{\\bt'} \\rangle}{(d-1)\\hat{\\sigma}^2_n}. \\tag{12.2}\\] Cette quantité peut sembler difficile à calculer : elle ne l’est pas, et s’exprime à l’aide du coefficient de détermination Définition 9.1.\n\nThéorème 12.3 \\[\\frac{R^2}{1-R^2}\\frac{n-d}{d-1} \\sim \\mathscr{F}_{d-1, n-d}. \\]\n\n\nPreuve. Il suffit de montrer que l’expression dans Équation 12.2 est égale à \\((n-d)R^2/(d-1)(1-R^2)\\)."
  },
  {
    "objectID": "ch5_ex.html#questions",
    "href": "ch5_ex.html#questions",
    "title": "٭ Exercices",
    "section": "Questions",
    "text": "Questions\n\nRetrouver la formule des MCO par une méthode analytique.\nConstruire un intervalle de confiance de niveau \\(1-\\alpha\\) pour le coefficient \\(\\theta_j\\) d’une régression.\nSoit \\(X\\) une matrice. Pourquoi les nombres \\(\\ell^2_j = ((X^\\top X)^{-1})_{j,j}\\) sont-ils toujours des nombres positifs ?\nÉcrire explicitement les deux leviers dans un modèle linéaire simple en une dimension.\nConcrètement, comment s’interprète la condition “la matrice des variables explicatives \\(X\\) est de rang \\(d\\)” ? Qu’est-ce qui ne va pas lorsque ce n’est pas le cas ?\nAu lieu de faire un test de significativité sur un coefficient d’une régression linéaire (Théorème 12.1), tester \\(\\theta_j=x\\) pour n’importe quel \\(x\\) (pas forcément \\(0\\)).\nDans un ajustement affine sans constante \\(y_i=\\beta x_i\\), monter que \\(\\hat{\\beta} = \\sum_{i=1}^n p_i y_i\\) où \\(p_i = x_i/|x|^2\\).\nCalculer la limite en loi de \\(\\mathscr{F}_{r,n}\\) lorsque \\(r\\) est fixé et \\(n \\to \\infty\\).\nCalculer la limite en loi de \\(\\mathscr{F}_{n,n}\\) lorsque \\(n \\to \\infty\\)."
  },
  {
    "objectID": "ch5_ex.html#exercices",
    "href": "ch5_ex.html#exercices",
    "title": "٭ Exercices",
    "section": "Exercices",
    "text": "Exercices\n\nExercice 1 (Les limites du coefficient de détermination)  \n\nConstruire un jeu de variables explicatives \\(x_i\\) et expliquées \\(y_i\\) tel que l’ajustement affine des \\(x\\) vers les \\(y\\) possède un \\(R^2\\) égal à 0 (aucune significativité linéaire), mais tel que les \\(y_i\\) sont parfaitement déterminés par les \\(x_i\\) (c’est-à-dire tels qu’il y a une fonction \\(f\\) avec \\(f(x_i)=y_i\\) pour tout \\(i\\)).\nMontrer qu’ajouter des variables explicatives dans un modèle augmente le coefficient de détermination.\n\n\n\nExercice 2 (Pénalité \\(\\ell^2\\) (régression ridge)) Dans une régression de type \\(Y = X\\bt + \\varepsilon\\), on s’intéresse au problème \\[ \\arg \\min |Y - X\\theta|^2 + \\lambda |\\theta|^2.\\] Il s’agit du problème des moindres carrées, mais où la présence de la pénalité \\(\\lambda|\\theta|^2\\) impose que les coefficients de \\(\\theta\\) ne soient pas trop grands au sens \\(\\ell^2\\).\n\nMontrer que la solution au problème est donnée par \\(\\hat{\\bt}_{\\lambda} = (X^\\top X + \\lambda I_d)^{-1}X^\\top Y\\) sans aucune contrainte de rang sur \\(X\\).\nCalculer la loi de \\(\\hat{\\bt}_{\\lambda}\\) lorsque les résidus sont gaussiens. Quel est son biais ?\n\n\n\nExercice 3 Dans un modèle linéaire \\(Y = X\\bt + \\varepsilon\\), on cherche à tester une unique contrainte linéaire, à savoir \\(\\langle z, \\bt \\rangle = c\\) où \\(z \\in \\mathbb{R}^d\\) et \\(c \\in \\mathbb{R}\\).\n\nMontrer que dans ce cas, la statistique de Wald s’écrit \\[ |\\langle z, \\hat{\\bt}\\rangle - c|^2/\\hat{\\sigma}_n^2\\langle z, (X^\\top X)^{-1}z\\rangle.\\] Écrire le test associé à cette statistique.\nTrouver un estimateur \\(\\hat{\\tau}^2\\) de la variance de \\(\\langle z, \\hat{\\bt}\\rangle -c\\). Sous l’hypothèse nulle, quelle est la loi de \\((\\langle z, \\hat{\\bt}\\rangle -c)/\\hat{\\tau}_n\\) ? En déduire un test associé à cette statistique.\nMontrer que les deux tests sont équivalents.\n\n\n\nExercice 4 Soient \\((x_1, y_1), \\dotsc, (x_n, y_n)\\) des points dans \\(\\mathbb{R}^2\\). Comparer l’ajustement affine des \\(x\\) vers les \\(y\\), et l’ajustement affine des \\(y\\) vers les \\(x\\).\n\n\nExercice 5 (Test de Chow) On dispose de deux jeux de données, disons \\((X^1_1, Y^1_1), \\dotsc, (X^1_n, Y^2_n)\\) et \\((X^2_{1}, Y^2_{1}), \\dotsc, (X^2_m, Y^2_m)\\). Dans les deux régressions \\(Y^1 = X^1 \\bt^1+ \\varepsilon^1\\) et \\(Y^2 = X^2\\bt^2+\\varepsilon^2\\), on souhaite tester si \\(\\bt^1 = \\bt^2\\).\n\nOn suppose dans un premier temps que les erreurs \\(\\varepsilon^1, \\varepsilon^2\\) ont la même loi, avec une variance \\(\\sigma^2\\) connue. Proposer un test simple de l’hypothèse nulle.\nMême question lorsque \\(\\sigma\\) n’est pas connue.\nMême question lorsque \\(\\varepsilon^1,\\varepsilon^2\\) n’ont pas la même variance.\n\n\n\nExercice 6 On se place dans un modèle linéaire gaussien de la forme \\(Y = X\\bt + \\varepsilon\\), mais on suppose que les entrées de \\(\\varepsilon_i\\) ne sont plus iid, mais possèdent une covariance \\(\\Sigma\\) non scalaire.\n\nSi l’on connaît \\(\\Sigma\\), on pose \\(Y' = \\Sigma^{-1/2}Y\\) et \\(X' = \\Omega^{-1/2}X\\). Montrer que \\[\\hat{\\bt}_{\\mathrm{MCG}}= (X^\\top \\Sigma^{-1} X)^{-1}X^\\top \\Sigma^{-1}Y\\] est un estimateur sans biais de \\(\\bt\\), appelé estimateur des moindres carrés généralisés.\nOn suppose qu’on dispose d’un estimateur \\(\\hat{\\Sigma}\\) de \\(\\Sigma\\). Montrer que \\[(X^\\top \\hat{\\Sigma}^{-1} X)^{-1}X^\\top \\hat{\\Sigma}^{-1}Y\\] est un estimateur asymptotiquement normal et sans biais de \\(\\bt\\).\n\n\n\nExercice 7 On considère le modèle de régression linéaire \\(y_i=b_0+b_1x_i+\\varepsilon_i\\) où \\(i = 1, \\dotsc, n\\) et les \\(\\varepsilon_i\\) sont des variables aléatoires indépendantes \\({\\mathcal N}(0,\\sigma^2)\\) et \\(b_0,b_1\\) et \\(\\sigma^2\\) sont inconnus.\n\nDonner les estimateurs des moindres carrés ordinaires \\(\\hat{b}_0,\\hat{b}_1\\) et \\(\\hat{\\sigma}^2\\) et leur loi jointe.\nOn dispose d’une nouvelle observation, disons \\(y_0\\), pour laquelle la valeur de \\(x_0\\) de la variable explicative est inconnue. L’objectif est d’estimer \\(x_0\\). On suppose que \\(y_0\\) est une réalisation d’une variable aléatoire \\(Y_0\\) s’écrivant \\(Y_0=b_0+b_1x_0+\\eta\\), avec \\(\\eta\\) une erreur d’observation de loi \\(N(0,\\sigma^2)\\) indépendante des \\(\\varepsilon_i\\). On suppose en outre que la variable que l’on cherche, \\(x_0\\), n’est pas trop éloignée des autres \\(x_i\\) : \\(|x_0- \\bar x| \\leq 1\\).\n\nQuelle est la loi de \\(Y_0-\\hat{b}_0-\\hat{b}_1x_0\\) ?\nEn utilisant l’estimateur \\(\\hat{\\sigma}\\) de \\({\\sigma}\\), déterminer un intervalle de confiance de niveau \\(1-\\alpha\\) pour \\(x_0\\).\n\nOn dispose maintenant de \\(m\\) observations \\(y_{0,1},\\dots,y_{0,m}\\) correspondant à la valeur \\(x_0\\) inconnue ; ce sont des réalisations de copies indépendantes \\(Y_{0,j}\\) de \\(Y_0\\).\n\nMontrer que \\[\\tilde{\\sigma}^2=\\frac{(n-2)\\hat{\\sigma}^2+\\sum_{j=1}^{m}(Y_{0j}-\\bar{Y}_0)^2}{n+m-3}\\] est un estimateur sans biais de \\(\\sigma^2\\). Quelle est sa loi ?\nQuelle est la loi de \\(\\bar{Y}_0-\\hat{b}_0-\\hat{b}_1x_0\\) ?\nA l’aide de \\(\\tilde{\\sigma}^2\\) et de \\(\\bar{Y}_0\\), donner un intervalle de confiance pour \\(x_0\\) de niveau \\(1-\\alpha\\).\nAurait-on pu construire un intervalle de confiance pour \\(x_0\\) à l’aide de \\(\\hat{\\sigma}^2\\) et de \\(\\bar{Y}_0\\) ?\n\n\n\n\nExercice 8 (Théorème de Gauss-Markov) On se place dans un modèle linéaire gaussien \\(Y = X\\bt + \\varepsilon\\). L’objectif est de montrer que \\(\\hat{\\bt}\\), l’estimateur des moindres carrés, est le meilleur estimateur linéaire de \\(\\bt\\) qui soit sans biais1. Soit donc \\(\\tilde{\\bt}\\) un autre estimateur linéaire sans biais, disons \\(\\tilde{\\bt} = MY\\).\n\nMontrer que \\((M - (X^\\top X)^{-1}X^\\top)X= 0\\).\nCalculer la matrice de variance de \\(\\tilde{\\bt}\\) en fonction de \\(M - (X^\\top X)^{-1}X^\\top\\) et conclure."
  },
  {
    "objectID": "ch5_ex.html#footnotes",
    "href": "ch5_ex.html#footnotes",
    "title": "٭ Exercices",
    "section": "",
    "text": "BLUE, best linear unbiased estimator. ↩︎"
  },
  {
    "objectID": "ch6_0.html#exemples",
    "href": "ch6_0.html#exemples",
    "title": "13  Modèles exponentiels",
    "section": "Exemples",
    "text": "Exemples\nJusqu’ici, nous avons vu de nombreux exemples de modèles statistiques. Dans la plupart des cas, il s’agissait de modèles de la loi de \\(n\\) variables aléatoires indépendantes et identiquement distribuées selon une même loi \\(P\\) (le modèle était donc \\(P^{\\otimes n}\\)). Cette loi \\(P\\) possède souvent une densité par rapport à une mesure de référence. Par exemple, la loi gaussienne a une densité par rapport à la mesure de Lebesgue sur \\(\\mathbb{R}\\) : \\[ \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2}}=\\frac{1}{\\sqrt{2\\pi}e^{\\frac{\\mu^2}{2}}}e^{-\\frac{x^2}{2}}e^{x\\mu}.\\] La loi exponentielle a une densité par rapport à la mesure de Lebesgue sur \\(\\mathbb{R}_+\\) : \\[ \\frac{1}{1/\\lambda}e^{-\\lambda x}\\] Les lois discrètes ont une densité par rapport à la mesure de comptage : la loi de Bernoulli, par exemple, s’écrit \\[ p^n(1-p)^{1-n} = \\frac{e^{n\\ln(p/(1-p))}}{(1-p)^{-1}} \\] avec \\(n\\) valant zéro ou 1, ou encore la loi de Poisson \\[ e^{-\\lambda}\\frac{\\lambda^n}{n!} = \\frac{1}{e^{\\lambda}}\\frac{1}{n!}e^{-\\lambda n}\\] ou enfin la loi géométrique \\[p^n(1-p) = \\frac{e^{n\\ln(p)}}{(1-p)^{-1}}.\\] Dans tous ces exemples, j’ai volontairement écrit la densité de façon inhabituelle : tous ces modèles peuvent s’écrire sous la forme \\[ \\frac{1}{Z(\\theta)}h(x)e^{\\theta f(x)}\\] où \\(f\\) et \\(g\\) sont des fonctions qui ne dépendent pas de \\(\\theta\\), et où \\(Z\\) est une constante qui ne dépend que de \\(\\theta\\). Ces modèles appartiennent à la famille des modèles exponentiels."
  },
  {
    "objectID": "ch6_0.html#définitions",
    "href": "ch6_0.html#définitions",
    "title": "13  Modèles exponentiels",
    "section": "13.1 Définitions",
    "text": "13.1 Définitions\nSoit \\(\\nu\\) une mesure de référence (\\(\\sigma\\)-finie) sur \\(\\mathbb{R}^d\\).\nSoit \\(\\Theta \\subset \\mathbb{R}^p\\) (l’espace des paramètres) et soit \\(T : \\mathbb{R}^d \\to \\mathbb{R}^p\\) une fonction mesurable.\nOn suppose que pour tout \\(\\theta \\in \\Theta\\), la fonction \\(x \\mapsto e^{\\langle \\theta, T(x)\\rangle}\\) est intégrable par rapport à \\(\\nu\\) : son intégrale \\[Z_\\theta = \\int e^{\\langle \\theta, T(x)\\rangle}\\nu(dx)\\] est appelée fonction de partition.\n\nDéfinition 13.1 Le modèle exponentiel associé à \\(T\\) est la famille de densités (par rapport à \\(\\nu\\)) définie par \\[ p_\\theta(x) = \\frac{e^{\\langle \\theta, T(x)\\rangle}}{Z_\\theta}.\\]\n\nLorsqu’on fixe un \\(x\\) dans l’espace des observations, la fonction\n\\[ \\theta \\mapsto p_\\theta(x)\\] est appelée vraisemblance et son logarithme \\[\\ell_\\theta(x) = \\ln p_\\theta(x)\\] est appelé log-vraisemblance. Lorsqu’il est bien défini, le gradient (en \\(\\theta\\)) de la log-vraisemblance \\[ \\nabla \\ln p_\\theta(x)\\] est appelé fonction de score du modèle. Ces termes ne sont pas propres aux modèles exponentiels. On omet fréquemment de noter la dépendance en les observations, qu’on suppose fixées une bonne fois pour toutes."
  },
  {
    "objectID": "ch6_0.html#retour-sur-des-exemples",
    "href": "ch6_0.html#retour-sur-des-exemples",
    "title": "13  Modèles exponentiels",
    "section": "13.2 Retour sur des exemples",
    "text": "13.2 Retour sur des exemples\n\nExemple 13.1 (Gaussiennes) La densité de \\(N(\\mu,\\sigma^2)\\) s’écrit\n\\[\\frac{e^{-\\frac{\\mu^2}{2\\sigma^2}}}{\\sqrt{2\\pi\\sigma^2}}e^{ \\frac{\\mu}{\\sigma^2} x - \\frac{1}{2 \\sigma^2} x^2 - \\frac{\\mu^2}{2 \\sigma^2}}.\\] La mesure \\(\\nu\\) est la mesure de Lebesgue sur \\(\\mathbb{R}\\). Le moment \\(T\\) est \\[T(x) = \\begin{bmatrix}x \\\\ -x^2/2 \\end{bmatrix} .\\] Le bon paramètre \\(\\theta\\) est \\[ \\theta = \\begin{bmatrix}\\mu/\\sigma^2 \\\\ 1 / \\sigma^2\\end{bmatrix}.\\] La fonction de partition \\(Z(\\theta)\\) s’écrit aussi \\[\\exp\\left(\\frac{\\mu^2}{2 \\sigma^2} \\right)\\sqrt{2 \\pi \\sigma^2} = \\exp(\\theta_1 / 2)\\sqrt{2\\pi / \\theta_2}. \\]\n\nL’exemple de la loi Gaussienne montre qu’en règle générale, il peut être nécessaire de « reparamétriser » le modèle pour l’écrire sous sa forme exponentielle.\n\nExemple 13.2 (Poisson) La mesure \\(\\nu\\) est la mesure de comptage sur \\(\\mathbb{N}\\). Le paramètre est \\[ \\theta = \\begin{bmatrix}\\lambda \\\\ 1 \\end{bmatrix}\\] et le moment \\(T\\) est \\[ T = \\begin{bmatrix}-n \\\\ - \\ln(n!)\\end{bmatrix}\\] de sorte que la fonction de partition est \\(Z(\\theta) = e^{-\\lambda} = e^{-\\theta_1}\\)."
  },
  {
    "objectID": "ch6_0.html#régularité",
    "href": "ch6_0.html#régularité",
    "title": "13  Modèles exponentiels",
    "section": "13.3 Régularité",
    "text": "13.3 Régularité\nOn supposera dorénavant que l’espace des paramètres \\(\\Theta\\) (qui est une partie de \\(\\mathbb{R}^p\\)) est un ouvert, et que \\(Z(\\theta)\\) est fini pour tout \\(\\theta \\in \\Theta\\).\n\nProposition 13.1  \n\nPour tout \\(n\\), \\(E_\\theta[|T(X)|^n]\\) est fini.\nLa fonction de partition d’un modèle exponentiel est infiniment différentiable.\nLe gradient de la log-partition est donné par \\[\\nabla \\ln Z(\\theta) = E_\\theta[T(X)] \\tag{13.1}\\] et sa matrice hessienne1 par \\[ \\nabla^2 \\ln Z(\\theta) = \\mathrm{Var}_\\theta(T(X)). \\tag{13.2}\\]\n\n\n\nPreuve. \n\nPrenons un petit \\(\\delta\\) tel que \\(\\theta + \\delta\\) et \\(\\theta - \\delta\\) sont dans \\(\\Theta\\). Comme \\(Z(\\theta \\pm \\delta) = \\int e^{\\langle \\theta, T(x)\\rangle \\pm \\langle \\delta, T(X)\\rangle}\\nu(dx)\\) et que \\(e^{|\\langle \\delta, T(x)\\rangle|}\\leqslant e^{\\langle \\delta, T(x)\\rangle} + e^{\\langle -\\delta, T(x)\\rangle}\\), on en déduit que \\[\\int e^{\\langle \\theta, T(x)\\rangle + |\\langle \\delta, T(x)\\rangle|}\\nu(dx)&lt;\\infty. \\] Le théorème d’interversion série-intégrale à termes positif montre que ce terme est aussi égal à \\[ \\sum_{n=0}^\\infty \\int e^{\\langle \\theta, T(x)\\rangle}\\frac{|\\langle \\delta, T(x)\\rangle|^n}{n!}\\nu(dx).\\] Tous les termes de cette somme sont donc finis, ce qui signifie précisément que \\(E_\\theta[|\\langle \\delta, T(X)\\rangle|^n]&lt;\\infty\\) pour tout \\(n\\). Ceci étant valable pour tout \\(\\delta\\) dans une boule autour de \\(\\theta\\), il est immédiat d’en déduire que \\(E_\\theta[|\\langle x, T(X)\\rangle|^n]\\) est fini pour tout \\(n\\) et pour tout \\(x\\). En prenant \\(x = \\delta_i\\), on voit donc que les coordonnées de \\(T\\) ont des moments finis à tous les ordres, et donc que \\(T\\) possède des moments finis à tous les ordres au sens où \\(E_\\theta[|T(X)|^n]\\).\nOn a \\(\\nabla \\ln Z(\\theta) = \\nabla Z(\\theta)/Z(\\theta)\\). Formellement, \\(\\nabla Z(\\theta)\\) est donc égal à \\[\\nabla \\int e^{\\langle \\theta, T\\rangle} = \\int \\nabla e^{\\langle \\theta, T\\rangle} = \\int T e^{\\langle \\theta, T\\rangle}. \\] Il est alors clair que \\(\\nabla \\ln Z(\\theta) = \\int p_\\theta T\\). Pour justifier la dérivation sous le signe intégral, notons \\(f(\\theta, x) = e^{\\langle \\theta, T(x)\\rangle}\\). Elle est infiniment différentiable en \\(\\theta\\) et son intégrale est finie dès que \\(\\theta\\) est dans \\(\\Theta\\). Son gradient en \\(\\theta\\) est égal à \\(e^{\\langle \\theta, T(x)\\rangle}T(x)\\) qui est d’intégrale finie d’après le premier point. En regardant bien la démonstration, on constate également qu’il y a une constante \\(c\\) telle que pour tout \\(\\delta\\) dans un voisinage de \\(\\theta\\), on a \\(E_{\\theta + \\delta}[|T(X)|]&lt;c\\). Tout cela permet d’appliquer le théorème de dérivation sous le signe intégral et la formule Équation 13.1.\nPour la formule Équation 13.2, c’est la même chose : \\(\\nabla^2 \\ln Z(\\theta) = \\nabla \\frac{\\nabla Z(\\theta)}{Z(\\theta)}\\). Les règles de dérivation des produits disent alors que ce terme est égal à \\[\\frac{Z(\\theta)\\nabla^2 Z(\\theta) - \\nabla Z(\\theta)\\nabla Z(\\theta)^\\top}{Z(\\theta)^2}. \\] Il suffit donc de calculer \\(\\nabla^2 Z(\\theta)\\), qui par un argument de dérivation sous l’intégrale similaire au précédent, est égal à \\[ \\int e^{\\langle \\theta, T(x)\\rangle}T(x)T(x)^\\top \\nu(dx).\\] La formule Équation 13.2 découle alors de la définition de la covariance."
  },
  {
    "objectID": "ch6_0.html#identifiabilité",
    "href": "ch6_0.html#identifiabilité",
    "title": "13  Modèles exponentiels",
    "section": "13.4 Identifiabilité",
    "text": "13.4 Identifiabilité\n\nThéorème 13.1 Dans un modèle exponentiel, les points suivants sont équivalents.\n\nLe modèle est identifiable.\nLa matrice hessienne de la fonction de log-partition (Équation 13.2) est inversible en tout \\(\\theta\\).\n\\(\\nabla \\ln Z(\\theta)\\) est un difféomorphisme.\n\n\n\nPreuve. Démontrons d’abord l’équivalence des deux premiers points.\nLa matrice hessienne de \\(\\ln Z_\\theta\\) est égale à \\(\\mathrm{Var}_\\theta(T(X))\\), donc elle est toujours positive. Si elle n’est pas inversible, alors elle n’est pas définie positive et il existe un \\(\\mu\\) tel que \\(\\mu^\\top \\mathrm{Var}_\\theta(T) \\mu\\) vaut zéro. Or, ce terme est aussi égal à \\(\\mathrm{Var}_\\theta(\\langle \\mu, T\\rangle)\\). Cela impliquerait que la variable aléatoire \\(\\langle \\mu, T(X)\\rangle\\) soit constante \\(P_\\theta\\)-presque sûrement, disons égale à un certain \\(\\alpha\\), et donc que \\(\\nu\\)-presque sûrement, \\(\\langle \\mu, T(x)\\rangle = \\alpha\\). Mais alors, \\(p_{\\theta + \\mu}(x)\\) peut s’écrire \\[\\frac{e^{\\langle\\theta + \\mu, T(x)\\rangle}}{Z(\\theta + \\mu)} = \\frac{e^{\\langle\\theta, T(x)\\rangle}e^{\\alpha}}{Z(\\theta + \\mu)}\\] c’est-à-dire \\[p_\\theta(x)\\times \\frac{e^\\alpha Z(\\theta)}{Z(\\theta + \\mu)}.\\] Or, comme \\(p_\\theta\\) est une densité de probabilité, son intégrale vaut 1 : la constante \\(e^\\alpha Z(\\theta) / Z(\\theta + \\mu)\\) vaut donc 1 et l’on a montré que \\(p_{\\theta + \\mu}\\) et \\(p_\\theta\\) sont égales partout. Le modèle n’est donc pas identifiable.\nPour l’autre sens, il suffit de reprendre toutes ces implications à l’envers : si \\(p_{\\theta + \\mu} = p_\\theta\\), alors pour \\(\\nu\\)-presque tout \\(x\\) on aura \\(\\langle \\theta + \\mu, T(x)\\rangle = \\langle \\theta, T(x)\\rangle\\), et donc \\(\\langle \\mu, T(x)\\rangle = 0\\), et in fine \\(\\mu^\\top \\mathrm{Var}_\\theta(T(X))\\mu = 0\\).\nComme iii) entraîne ii), il suffit donc de montrer que i) et ii) entraînent iii). Nous allons montrer la contraposée : si iii) n’est pas vrai et que ii) n’est pas vrai, c’est fini. On peut donc supposer que iii) n’est pas vrai et que ii) est vrai, et il faut montrer que i) est faux. Le point ii) entraîne que \\(\\nabla \\ln Z\\) est localement injective (théorème d’inversion locale), donc si cette application n’était pas un difféomorphisme, cela voudrait dire qu’elle n’est pas injective et qu’il y aurait donc \\(\\theta\\neq\\mu\\) tels que \\(\\nabla \\ln Z(\\theta) = \\nabla \\ln Z(\\mu)\\). Or, un calcul montre que la divergence de Kullback-Leibler (Définition 7.3) symétrisée, à savoir \\(\\dkl(P_\\theta \\mid P_\\mu) + \\dkl(P_\\mu \\mid P_\\theta)\\), est égale à \\[\\langle \\nabla \\ln Z(\\theta) - \\nabla \\ln Z(\\mu), \\theta - \\mu \\rangle \\] et ceci vaut donc zéro : c’est donc que chacune des deux \\(\\dkl\\) vaut zéro, puisque ces deux termes sont positifs. On en déduit alors que \\(P_\\theta = P_\\mu\\), et donc le modèle n’est pas identifiable."
  },
  {
    "objectID": "ch6_0.html#footnotes",
    "href": "ch6_0.html#footnotes",
    "title": "13  Modèles exponentiels",
    "section": "",
    "text": "On rappelle que la Hessienne d’une fonction \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\) est la matrice de ses dérivées secondes \\[\\frac{\\partial^2}{\\partial x_i \\partial x_j}f(x_1, \\dotsc, x_d).\\] Par abus de notation, on la note souvent \\(\\nabla^2\\), mais il serait plus juste de la noter \\(\\nabla^\\top \\nabla\\). ↩︎"
  },
  {
    "objectID": "ch6_1.html#définition",
    "href": "ch6_1.html#définition",
    "title": "14  Maximum de vraisemblance",
    "section": "14.1 Définition",
    "text": "14.1 Définition\nL’estimateur du maximum de vraisemblance (EMV) est le paramètre pour lequel la vraisemblance des observations est maximale : \\[\\emv = \\arg \\max_{\\theta \\in \\Theta} p_\\theta(x_1, \\dotsc, x_n). \\tag{14.1}\\] Il n’est pas évident que ce maximum existe, ni que le minimeur soit unique. C’est en règle générale le cas, mais des cas limites existent où ça ne l’est pas – parfois même dans des modèles exponentiels. Il existe un théorème général garantissant son existence et son unicité.\n\nProposition 14.1 Dans un modèle exponentiel identifiable dont l’espace des paramètres est un ouvert convexe avec \\(\\nu(\\partial \\Theta)=0\\), l’estimateur Équation 14.1 existe et il est unique.\n\nLa démonstration de ce théorème général repose sur des outils purement analytiques relativement simples : en particulier, sous les hypothèses ci-dessus, la fonction de log-vraisemblance \\(\\theta \\mapsto \\ln p_\\theta(x_1, \\dotsc, x_n)\\) est presque sûrement strictement concave et tend vers \\(0\\) au bord du domaine : son maximum \\(\\hat{\\theta}\\) est donc bien défini, grâce aux résultats généraux de convexité.\nTrouver le maximum d’une fonction positive \\(f(x)\\) et trouver le maximum de son logarithme \\(\\ln f(x)\\) reviennent au même : or, il est souvent bien plus facile dans les modèles exponentiels de maximiser le logarithme de la vraisemblance, qui dans un modèle exponentiel s’écrit \\[\\ell(\\theta | x_1, \\dotsc, x_n) := \\langle \\theta, T(x)\\rangle - \\ln Z_\\theta. \\] On omettra presque systématiquement le fait que la log-vraisemblance dépend des observations \\(x_i\\), et l’on notera simplement \\(\\ell(\\theta)\\). Parfois, pour indiquer quand même que l’échantillon comporte \\(n\\) éléments, on notera \\(\\ell_n(\\theta)\\). En règle générale, Équation 14.1 est donc équivalent au problème du maximum de log-vraisemblance, \\[ \\hat{\\theta} = \\arg \\max \\ell(\\theta)\\]"
  },
  {
    "objectID": "ch6_1.html#lemv-et-les-moments",
    "href": "ch6_1.html#lemv-et-les-moments",
    "title": "14  Maximum de vraisemblance",
    "section": "14.2 L’EMV et les moments",
    "text": "14.2 L’EMV et les moments\nL’EMV maximise la log-vraisemblance. Lorsqu’il existe et qu’il est unique, il est donc l’unique solution de \\(\\nabla_\\theta \\ell(\\theta) = 0\\). Cette équation s’écrit encore \\(T(x) = \\nabla \\ln Z_\\theta\\). Un calcul célèbre montre que \\[ \\nabla \\ln Z_\\theta = E_\\theta [T(X)].\\]\n\nPreuve. Le gradient que l’on cherche à calculer vaut \\(\\nabla Z_\\theta / Z_\\theta\\). Or, en intervertissant le gradient et l’intégrale, on voit que le numérateur est égal à \\(\\int T(x)e^{\\langle \\theta, T(x)\\rangle}\\nu(dx)\\), ce qui donne l’égalité recherchée.\n\nNotons \\(\\varphi(\\theta) = E_\\theta[T(X)]\\). Le maximum de vraisemblance vérifie précisément l’équation des moments, \\(\\varphi(\\emv) = T(X)\\).\n\nThéorème 14.1 Dans un modèle iid, l’estimateur du maximum de vraisemblance vérifie \\[\\emv = \\varphi^{-1}(\\bar{T}_n).\\]\n\nOr, \\(\\bar{T}_n\\) est asymptotiquement normal. Plus précisément, sous \\(P_\\theta\\), \\[ \\sqrt{n}(\\bar{T}_n - \\varphi(\\theta))\\to N(0,I(\\theta))\\] où l’on a défini \\[I(\\theta) = \\mathrm{Var}_\\theta(T).\\] En appliquant le théorème de la méthode des moments (Théorème 3.1), on voit donc que \\(\\emv\\) est asymptotiquement normal, au sens où \\(\\sqrt{n}(\\bar{T}_n - \\theta)\\) converge en loi vers une gaussienne centrée de matrice de variance \\[ D\\varphi(\\theta)^{-1}\\mathrm{Var}_\\theta(T)(D\\varphi(\\theta)^{-1})^\\top.\\] Il se trouve que cette matrice est aisément calculable, et centrale dans la théorie des statistiques : il s’agit de la matrice d’information de Fisher, que nous étudierons dans la prochaine section."
  },
  {
    "objectID": "ch6_2.html#définitions",
    "href": "ch6_2.html#définitions",
    "title": "15  L’information de Fisher",
    "section": "15.1 Définitions",
    "text": "15.1 Définitions\n\nDéfinition 15.1 (Fonction de score) Le score est la dérivée de la log-vraisemblance : \\[ s_\\theta(x) = \\nabla_\\theta \\ln p_\\theta(x).\\]\n\nDans un modèle exponentiel \\(p_\\theta(x) = \\exp(\\langle \\theta, T(x)\\rangle - \\ln Z_\\theta)\\), on voit immédiatement que \\[ s_\\theta(x) =  T(x).\\] Le score dépend des observations, et donc est une variable aléatoire. On peut calculer son espérance par rapport à \\(p_\\theta\\). Il s’avère que le score est centré :  \\[E_\\theta[s_\\theta(X)]=0.\\]\n\nPreuve. L’espérance du score est \\(\\int p_\\theta(x) \\nabla_\\theta \\ln p_\\theta(x) \\nu(dx)\\), c’est-à-dire \\(\\int \\nabla_\\theta p_\\theta(x)\\nu(dx)\\). Si l’on peut intervertir l’intégrale et la dérivée, on obtient \\(\\nabla_\\theta \\int p_\\theta d\\nu\\), qui est le gradient de la fonction constante égale à 1 (puisque \\(p_\\theta\\) est une densité de probabilité par rapport à \\(\\nu\\)) : c’est donc zéro.\n\n\nDéfinition 15.2 (Information de Fisher) L’information de Fisher est la matrice de covariance du score : \n\\[I(\\theta) = E_\\theta[s_\\theta(X) s_\\theta(X)^\\top].\\]\n\nL’information de Fisher possède de nombreuses expressions alternatives. Parmi elles, la suivante permet d’interpréter \\(I(\\theta)\\) comme la courbure moyenne de l’entropie. On note \\(\\nabla^2_\\theta f\\) la matrice hessienne d’une fonction \\(f\\).\n\nProposition 15.1 \\(I(\\theta) = - \\nabla^2_\\theta E_\\theta[ \\ln p_\\theta(X)].\\)\n\n\nPreuve. L’intégrale en question est égale à \\(\\int p_\\theta \\nabla_\\theta^2\\ln p_\\theta\\). Les règles de différentiation donnent \\[\\nabla^2_\\theta \\ln p_\\theta = \\frac{p_\\theta\\nabla^2_\\theta p_\\theta}{p_\\theta^2} -  \\frac{\\nabla_\\theta p_\\theta (\\nabla_\\theta p_\\theta)^\\top}{p_\\theta^2}\\] c’est-à-dire \\(\\nabla^2_\\theta p_\\theta / p_\\theta - s_\\theta s_\\theta^\\top\\). L’espérance de ceci par rapport à \\(p_\\theta\\) est donc égale à \\(\\int \\nabla^2_\\theta p_\\theta - I(\\theta)\\). Quant au premier terme, en intervertissant la dérivée seconde et l’intégrale, on voit qu’il est égal à la hessienne de la fonction constante égale à 1, donc il est nul."
  },
  {
    "objectID": "ch6_2.html#lemv-est-asymptotiquement-normal",
    "href": "ch6_2.html#lemv-est-asymptotiquement-normal",
    "title": "15  L’information de Fisher",
    "section": "15.2 L’EMV est asymptotiquement normal",
    "text": "15.2 L’EMV est asymptotiquement normal\n\nThéorème 15.1 (L’EMV est asymptotiquement normal) \\(\\sqrt{n}(\\emv - \\theta)\\) converge en loi vers \\(N(0,I(\\theta)^{-1})\\)."
  },
  {
    "objectID": "ch6_2.html#borne-de-cramér-rao",
    "href": "ch6_2.html#borne-de-cramér-rao",
    "title": "15  L’information de Fisher",
    "section": "15.3 Borne de Cramér-Rao",
    "text": "15.3 Borne de Cramér-Rao\n\nThéorème 15.2 (Borne de Cramér-Rao) Pour tout estimateur sans biais \\(\\hat{\\theta}\\) de \\(\\theta\\), on a1 \\(I(\\theta)^{-1} \\preceq \\mathrm{Cov}_\\theta(\\hat{\\theta})\\).\n\nLorsque le paramètre \\(\\theta\\) est réel, la borne de Cramér-Rao dit que le risque quadratique de n’importe quel estimateur sans biais ne peut pas être plus petit que \\(1/I(\\theta)\\). Les estimateurs sans biais qui atteignent cette borne sont appelés efficaces, ou asymptotiquement efficaces si leur risque quadratique converge vers cette borne.\n\nPreuve. Commençons par la dimension 1. Comme \\(T\\) est sans biais, \\(\\int p_\\theta(x)T(x)dx=\\theta\\). Comme \\(\\nabla p_\\theta = p_\\theta \\nabla_\\theta \\ln p_\\theta = p_\\theta s_\\theta\\), en intervertissant intégrale et dérivée, on obtient donc \\(1 = \\int p_\\theta (x)s_\\theta(x)T(x)dx = E_\\theta[s_\\theta(X)T(X)]\\). Nous avons déjà vu que le score est centré : ainsi, ce dernier terme vaut aussi \\(E_\\theta[s_\\theta(X)(T(X) - \\theta)]\\). L’inégalité de Cauchy-Schwarz donne alors \\[1 \\leqslant \\sqrt{E_\\theta[|T(X)-\\theta|^2]I(\\theta)}, \\] qui est le résultat voulu. Pour la dimension supérieure, il suffit d’appliquer ce résultat à \\(\\langle y, T(X)\\rangle\\), qui est un estimateur sans biais de \\(\\langle y, \\theta \\rangle\\) (ici, \\(y\\) est n’importe quel vecteur de \\(\\mathbb{R}^p\\)). L’inégalité ci-dessus, après quelques menues manipulations, devient \\[\\langle y, I(\\theta)^{-1}y\\rangle \\leqslant \\langle y, \\mathrm{Cov}_\\theta(T)y\\rangle, \\] qui montre bien que \\(I(\\theta)^{-1} \\preceq \\mathrm{Cov}_\\theta(T)\\)."
  },
  {
    "objectID": "ch6_2.html#footnotes",
    "href": "ch6_2.html#footnotes",
    "title": "15  L’information de Fisher",
    "section": "",
    "text": "Rappelons que, lorsque \\(A,B\\) sont des matrices définies positives, \\(A \\preceq B\\) équivaut à ce que \\(\\langle y, Ay\\rangle \\leqslant \\langle y, By\\rangle\\) pour tout \\(y\\). ↩︎"
  },
  {
    "objectID": "ch6_ex.html",
    "href": "ch6_ex.html",
    "title": "٭ Exercices",
    "section": "",
    "text": "$$\n\\newcommand{\\bx}{\\boldsymbol{x}}\n\\newcommand{\\bt}{\\boldsymbol{\\theta}}\n\\newcommand{\\dkl}{\\mathrm{d}_{\\mathrm{KL}}}\n\\newcommand{\\dtv}{\\mathrm{d}_{\\mathrm{TV}}}\n\\newcommand{\\emv}{\\hat{\\theta}_{\\mathrm{emv}}}\n$$\n\n\n\n\n\nSoit \\(\\hat{\\theta}\\) l’estimateur du maximum de vraisemblance d’un paramètre \\(\\theta\\) dans un modèle régulier. Montrer que \\(f(\\hat{\\theta})\\) est l’estimateur du maximum de vraisemblance de \\(f(\\theta)\\), pour n’importe quelle fonction \\(\\theta\\) raisonnable."
  },
  {
    "objectID": "ch7_0.html#la-répartition-empirique",
    "href": "ch7_0.html#la-répartition-empirique",
    "title": "16  Estimation de densité",
    "section": "16.1 La répartition empirique",
    "text": "16.1 La répartition empirique\nL’objet central est la fonction de répartition empirique, \\[ F_n(t) = \\frac{1}{n}\\#\\{i : X_i \\leqslant t\\}.\\] La loi des grands nombres montre immédiatement que, \\(\\mathbb{P}\\)-presque sûrement, \\(F_n(t)\\) converge vers \\(\\mathbb{P}(X_i \\leqslant t)=F(t)\\). On peut étendre ce résultat simultanément à une quantité dénombrable de \\(t\\) (par exemple, \\(\\mathbb{Q}\\)) mais pas à tous. De plus, ce résultat ne dit pas si la fonction \\(F_n\\) est proche de la fonction \\(F\\), au sens de la norme uniforme par exemple. Le théorème suivant, parfois appelé théorème fondamental de l’estimation, confirme que c’est le cas.\n\nThéorème 16.1 (Théorème de Glivenko-Cantelli) \\(\\mathbb{P}\\)-presque sûrement, lorsque \\(n \\to \\infty\\) on a \\(|\\hat{F}_n - F|_\\infty \\to 0\\).\n\nSoit \\((X_{(1)}, \\dotsc, X_{(n)})\\) l’échantillon trié en ordre croissant. Par convention, on pose \\(X_{(0)} = -\\infty\\). Alors, la quantité \\(|F_n - F|_\\infty\\) est aisément calculable grâce à la représentation suivante : \\[|F_n - F|_\\infty = \\sup_{i\\in \\{0, \\dotsc, n-1\\}}\\left|\\frac{i}{n} - F(X_{(i)})\\right| \\vee \\left| \\frac{i}{n} - F(X_{(i+1)})\\right|. \\]\n\nLemme 16.1 \\(|F_n - F|_\\infty\\) a la même loi que \\[ \\sup_{i\\in \\{0, \\dotsc, n-1\\}}\\left|\\frac{i}{n} - U_{(i)}\\right| \\vee \\left| \\frac{i}{n} - U_{(i+1)}\\right|\\] où les \\(U_{(i)}\\) sont des lois uniformes sur \\([0,1]\\), indépendantes, et triées dans l’ordre croissant.\n\nEn particulier, la loi de \\(|F_n - F|_\\infty\\) ne dépend pas de \\(F\\) : on dit que cette statistique est libre."
  },
  {
    "objectID": "ch7_0.html#inégalité-dkw",
    "href": "ch7_0.html#inégalité-dkw",
    "title": "16  Estimation de densité",
    "section": "16.2 Inégalité DKW",
    "text": "16.2 Inégalité DKW\nLe théorème de Glivenko-Cantelli possède une version beaucoup plus puissante car elle est entièrement quantitative, appelée inégalité DKW.\n\nThéorème 16.2 (Dvoretzky-Kiefer-Wolfowitz) Dans le même contexte, pour tout \\(t&gt;0\\) on a \\[\\mathbb{P}(|F_n - F|_\\infty &gt; t) \\leqslant 2e^{-2nt^2}.\\]"
  },
  {
    "objectID": "ch7_1.html",
    "href": "ch7_1.html",
    "title": "17  Test de Kolmogorov-Smirnov",
    "section": "",
    "text": "$$\n\\newcommand{\\bx}{\\boldsymbol{x}}\n\\newcommand{\\bt}{\\boldsymbol{\\theta}}\n\\newcommand{\\dkl}{\\mathrm{d}_{\\mathrm{KL}}}\n\\newcommand{\\dtv}{\\mathrm{d}_{\\mathrm{TV}}}\n\\newcommand{\\emv}{\\hat{\\theta}_{\\mathrm{emv}}}\n$$\n\n\n\n\n\nThéorème 17.1 (Kolmogorov-Smirnov) \\(\\sqrt{n}|F_n - F|_\\infty\\) converge en loi vers \\(|B|_\\infty\\), où \\((B_t)_{t\\in [0,1]}\\) est un mouvement Brownien standard. Cette variable aléatoire possède une loi connue, appelée loi de Kolmogorov-Smirnov, de fonction de répartition \\[\\mathbb{P}(|B|_\\infty \\leqslant x) = 1 - 2\\sum_{k=0}^\\infty (-1)^k e^{-2x^2 (k+1)^2}.\\]"
  },
  {
    "objectID": "ch7_ex.html",
    "href": "ch7_ex.html",
    "title": "٭ Exercices",
    "section": "",
    "text": "$$\n\\newcommand{\\bx}{\\boldsymbol{x}}\n\\newcommand{\\bt}{\\boldsymbol{\\theta}}\n\\newcommand{\\dkl}{\\mathrm{d}_{\\mathrm{KL}}}\n\\newcommand{\\dtv}{\\mathrm{d}_{\\mathrm{TV}}}\n\\newcommand{\\emv}{\\hat{\\theta}_{\\mathrm{emv}}}\n$$"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Et après ?",
    "section": "",
    "text": "$$\n\\newcommand{\\bx}{\\boldsymbol{x}}\n\\newcommand{\\bt}{\\boldsymbol{\\theta}}\n\\newcommand{\\dkl}{\\mathrm{d}_{\\mathrm{KL}}}\n\\newcommand{\\dtv}{\\mathrm{d}_{\\mathrm{TV}}}\n\\newcommand{\\emv}{\\hat{\\theta}_{\\mathrm{emv}}}\n$$"
  },
  {
    "objectID": "ref.html",
    "href": "ref.html",
    "title": "Références",
    "section": "",
    "text": "$$\n\\newcommand{\\bx}{\\boldsymbol{x}}\n\\newcommand{\\bt}{\\boldsymbol{\\theta}}\n\\newcommand{\\dkl}{\\mathrm{d}_{\\mathrm{KL}}}\n\\newcommand{\\dtv}{\\mathrm{d}_{\\mathrm{TV}}}\n\\newcommand{\\emv}{\\hat{\\theta}_{\\mathrm{emv}}}\n$$\n\n\n\n\n\nStatistics done Wrong\nThe earth is round (p&lt;.05)\nStatistiques mathématiques en action, pour ceux qui vont passer l’agrégation.\nIntroduction à l’économétrie de Brigitte Dormont est un excellent livre, écrit en français, sur les modèles linéaires.\nEn anglais, la référence sur les modèles linéaires est Econometric analysis de Greene.\nMéthodes statistiques de Philippe Tassi est un bon livre général.\nAll of statistics de Larry Wasserman est un ouvrage de référence."
  },
  {
    "objectID": "app1.html#multiplication-matricielle",
    "href": "app1.html#multiplication-matricielle",
    "title": "18  Algèbre linéaire",
    "section": "18.1 Multiplication matricielle",
    "text": "18.1 Multiplication matricielle\nLa pratique des régressions linéaires nécessite une certaine familiarité avec la multiplication des matrices. On rappelle que si \\(A\\) est une matrice à \\(\\ell\\) lignes et \\(m\\) colonnes, et que \\(B\\) est une matrice à \\(m\\) lignes et \\(n\\) colonnes, alors il est possible de les multiplier entre elles. Il en résulte une matrice \\(AB\\) avec \\(\\ell\\) lignes et \\(n\\) colonnes, dont le terme \\(i,j\\) est égal à \\[\\sum_{k=1}^m A_{i,k}B_{k,j}. \\] Ce terme peut aussi être vu comme \\(\\langle A_{i, \\cdot}, B_{\\cdot, j}\\rangle\\), le produit scalaire entre la \\(i\\)-ème ligne de \\(A\\) et la \\(j\\)-ème colonne de \\(B\\).\nDe façon générale, le produit scalaire entre deux vecteurs de même taille, \\(\\langle x, y\\rangle\\), est donc égal à la multiplication matricielle entre le vecteur ligne \\(x^\\top\\) et le vecteur colonne \\(y\\).\nIl est aussi possible de multiplier un vecteur ligne \\(x\\) de taille \\(n\\) et un vecteur colonne \\(y^\\top\\) de taille \\(m\\), mais ici on n’a plus besoin que \\(n\\) et \\(m\\) soient égaux. Il en résulte une matrice de taille \\(n\\times m\\), \\[xy^\\top = [x_i y_j]_{\\substack{i=1, \\dotsc, n\\\\ j=1,\\dotsc, m}}.\\] Si, comme tout à l’heure, \\(A\\) est une matrice \\(\\ell,n\\) et \\(B\\) une matrice \\(m,n\\), notons \\(a_i\\) les colonnes de \\(A\\) (vecteurs colonnes) et \\(b_i\\) les lignes de \\(B\\) (vecteurs lignes). Alors, on peut écrire \\[ AB = \\sum_{i=1}^m a_i b_i. \\] En particulier, pour n’importe quelle matrice \\(X\\) de taille \\(n,d\\) dont les lignes sont \\(\\bx_i\\) (et donc, les colonnes de \\(X^\\top\\) sont les \\(\\bx_i^\\top\\)), alors on peut écrire \\[X^\\top X = \\sum_{i=1}^n \\bx_i^\\top \\bx_i.\\]"
  },
  {
    "objectID": "app1.html#le-théorème-spectral",
    "href": "app1.html#le-théorème-spectral",
    "title": "18  Algèbre linéaire",
    "section": "18.2 Le théorème spectral",
    "text": "18.2 Le théorème spectral\nGrâce aux manipulations ci-dessus, le théorème de décomposition en vecteurs propres prend une forme légèrement différente. Ce théorème dit habituellement que toute matrice \\(M\\) symétrique réelle peut s’écrire \\(UDU^\\top\\), avec \\(U\\) la matrice de passage dans la base des vecteurs propres et \\(D = \\mathrm{diag}(\\lambda_i)\\) la matrice diagonale des valeurs propres. C’est donc la même chose que l’énoncé suivant.\n\nThéorème 18.1 Soit \\(M\\) une matrice symétrique réelle. Il existe une base orthonormale de vecteurs \\(u_1, \\dotsc, u_n\\) et des nombres réels \\(\\lambda_1, \\dotsc , \\lambda_n\\) tels que \\[ M = \\sum_{i=1}^n \\lambda_i u_i u_i^\\top.\\]"
  },
  {
    "objectID": "app1.html#projections-orthogonales",
    "href": "app1.html#projections-orthogonales",
    "title": "18  Algèbre linéaire",
    "section": "18.3 Projections orthogonales",
    "text": "18.3 Projections orthogonales\nSoit \\(v\\) un vecteur non nul de \\(\\mathbb{R}^n\\). L’espace vectoriel engendré par \\(v\\) est l’ensemble \\(\\mathscr{V}=\\{tv : t \\in \\mathbb{R}\\}\\), et son orthogonal est l’hyperplan \\(\\mathscr{V}^\\perp = \\{x : \\langle x, v \\rangle = 0\\}\\). Les résultats élémentaires d’algèbre linéaire disent que tout vecteur \\(x\\) se décompose de façon unique sous la forme \\[ x = y + z\\] avec \\(y\\) dans \\(\\mathscr{V}\\) et \\(z\\) dans \\(\\mathscr{V}^\\top\\). En particulier, il existe un \\(t\\) tel que \\(y = tv\\).\nConsidérons maintenant la matrice \\[P = \\frac{1}{|v|^2}vv^\\top \\in \\mathscr{M}_{n,n}. \\] Appliquons cette matrice à \\(x\\). Par linéarité, \\(Px = Px + Pz\\). Calculons ces deux termes.\n\n\\(Pz = |v|^{-1}v v^\\top z = |v|^{-1}v \\langle v, z\\rangle\\). Comme \\(z\\) est orthogonal à \\(v\\), cela vaut 0.\n\\(Py = tPv\\). Par définition de \\(P\\), ceci est donc égal à \\(t|v|^{-2}v v^\\top v = t|v|^{-2}v |v|^2 = tv\\), c’est-à-dire \\(y\\).\n\nNous avons montré plusieurs choses. D’abord, l’application qui à \\(x\\) associe \\(y\\) est effectivement linéaire, et une de ses matrices est \\(P\\). On dit que \\(P\\) est la matrice de projection sur \\(\\mathscr{V}\\). De même, comme \\((I - P)x = y+z - y = z\\), la matrice \\(I-P\\) est la matrice de projection sur \\(\\mathscr{V}^\\perp\\).\nLe cas d’un sous-espace vectoriel généré par plusieurs vecteurs \\(v_1, \\dotsc, v_d\\) linéairement indépendants se traite de la même façon. Soit \\(V = [v_1, \\dotsc, v_d]\\) la matrice \\(n \\times d\\) dont les colonnes sont les \\(v_i\\). Tout à l’heure, \\(|v|^{-2}\\) aurait pu s’écrire \\((v^\\top v)^{-1}\\). L’analogue avec \\(V\\) est donc naturellement \\((V^\\top V)^{-1}\\), donnant naissance au théorème suivant.\n\nThéorème 18.2 Soient \\(v_1, \\dotsc, v_d\\) des vecteurs non-colinéaires de \\(\\mathbb{R}^n\\), et soit \\(V = [v_1, \\dotsc, v_d]\\) la matrice \\(n \\times d\\) dont les colonnes sont les \\(v_i\\). La matrice de taille \\(n\\times n\\) \\[P_V = V (V^\\top V)^{-1}V^\\top \\] est la matrice de projection orthogonale sur le sous-espace \\(\\mathscr{V}\\) engendré par les \\(v_i\\). De plus, la matrice \\(I - P_V\\) est la matrice de projection orthogonale sur le sous-espace \\(\\mathscr{V}^\\perp\\).\n\n\nPreuve. Si \\(x=y+z\\) est la décomposition de \\(x\\) en somme d’un élément \\(y\\in\\mathscr{V}\\) et d’un élément \\(z \\in \\mathscr{V}^\\perp\\), alors \\(Px = Py + Pz\\) et \\[ Pz = V(V^\\top V)^{-1}V^\\top z.\\] Or, les \\(d\\) lignes de \\(V^\\top z\\) sont les produits scalaires \\(\\langle v_i, z\\rangle\\), qui sont tous nuls car \\(z\\) est orthogonal à tous les \\(v_i\\). Ainsi, \\(Pz = 0\\).\nD’autre part, comme \\(y\\) est dans l’espace engendré par les \\(v_i\\), il s’écrit sous la forme \\(t_1 v_1 + \\dotsc + t_d v_d\\). Cela peut se récrire en disant que \\(y = V t\\), où \\(t\\) est le vecteur colonne des \\(t_i\\). Mais alors, \\[ Py = V(V^\\top V)^{-1}V^\\top V t = Vt = y.\\] On conclut comme dans le cas \\(d=1\\) exposé ci-dessus. Il reste cependant un point de détail : nous devons nous assurer que \\(V^\\top V\\) est effectivement inversible ! C’est le cas, je le jure."
  },
  {
    "objectID": "app2.html#la-version-classique",
    "href": "app2.html#la-version-classique",
    "title": "19  50 nuances de TCL",
    "section": "19.1 La version classique",
    "text": "19.1 La version classique\nSoit \\((X_i)\\) une suite de variables aléatoires iid possédant une moyenne \\(\\mu\\) et une variance \\(\\sigma^2\\). On note \\(\\bar{X}_n\\) leur moyenne empirique, \\[\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i. \\tag{19.1}\\] Sous les hypothèses sur les \\(X_i\\), il est clair que \\(\\mathbb{E}[\\bar{X}_n] = \\mu\\), et que \\(\\mathrm{Var}(\\bar{X}_n) = \\sigma^2/n\\).\n\nThéorème 19.1 La variable aléatoire \\[\\frac{\\bar{X}_n - \\mu}{\\sqrt{\\sigma^2/n}}\\] converge en loi vers \\(N(0,1)\\).\n\n\nPreuve. Si \\(\\varphi\\) est la transformée de Fourier commune de la loi des \\(Y_i = (X_i - \\mu)/\\sigma\\) et \\(\\psi\\) celle de Équation 19.1, alors \\[ \\psi(t) = \\varphi(t/\\sqrt{n})^n.\\] Comme \\(\\varphi(x) \\sim 1 - x^2/2 + o(x^2)\\) par un développement de Taylor près de zéro, on voit que lorsque \\(n\\to\\infty\\), alors \\(\\psi(t) = (1 - t^2 / 2n + o(1/n))^n\\) et ceci tend vers \\(e^{-t^2/2}\\), qui est bien la transformée de Fourier de \\(N(0,1)\\)."
  },
  {
    "objectID": "app2.html#la-version-de-lindeberg-lévy",
    "href": "app2.html#la-version-de-lindeberg-lévy",
    "title": "19  50 nuances de TCL",
    "section": "19.2 La version de Lindeberg-Lévy",
    "text": "19.2 La version de Lindeberg-Lévy\nOn supposera maintenant les \\(X_i\\) indépendantes (mais pas forcément de même loi). On pose \\(\\bar{\\mu} = \\mathbb{E}[\\bar{X}_n]\\) et \\(s_n = \\mathrm{Var}(\\bar{X}_n)\\), c’est-à-dire \\[\\bar{\\mu}_n = \\frac{\\sum_{i=1}^n \\mu_i}{n} \\] \\[\\varsigma^2_n = \\frac{\\sum_{i=1}^n \\sigma_i^2}{n^2} \\] où \\(\\mu_i= \\mathbb{E}[X_i]\\) et \\(\\sigma_i^2 = \\mathrm{Var}(X_i)\\).\n\nThéorème 19.2 Si ces variables vérifient la condition de Lindeberg, à savoir que pour tout \\(\\delta &gt;0\\), \\[ \\frac{1}{\\varsigma_n^2}\\sum_{i=1}^n \\mathbb{E}[|X_i - \\mu_i|^2 \\mathbf{1}_{|X_i - \\mu_i| &gt; \\delta \\varsigma_n}] \\to 0 \\tag{19.2}\\] alors la variable aléatoire \\[ \\frac{\\bar{X}_n - \\bar{\\mu}_n}{\\varsigma_n}\\] converge en loi vers \\(N(0,1)\\)."
  },
  {
    "objectID": "app2.html#le-théorème-de-mann-wald",
    "href": "app2.html#le-théorème-de-mann-wald",
    "title": "19  50 nuances de TCL",
    "section": "19.3 Le théorème de Mann-Wald1",
    "text": "19.3 Le théorème de Mann-Wald1\nC’est un cas particulier du précédent.\nSoient \\((x_i)\\) une suite de nombres réels, pas forcément aléatoires, et soient \\(\\varepsilon_i\\) des variables aléatoires iid de variance \\(\\sigma^2\\) et vérifiant \\(\\mathbb{E}[|\\varepsilon_i|^4]&lt;c^2\\) pour une certaine constante \\(c^2\\). La moyenne pondérée \\[\\frac{1}{n}\\sum_{i=1}^n x_i \\varepsilon_i\\] est clairement une variable aléatoire centrée, et sa variance est égale à \\[\\sigma^2 \\frac{\\sum_{i=1}^n x_i^2}{n} = \\frac{\\sigma^2 s_n^2}{n}. \\] Peut-on dire que la moyenne réduite \\[ \\sqrt{n}\\frac{\\sum_{i=1}^n x_i \\varepsilon_i}{\\sigma s_n} \\tag{19.3}\\] converge en loi vers une \\(N(0,1)\\) ? La réponse est oui en général : cependant, en toute rigueur, on faire une hypothèse sur les \\(x_i\\). On demande à ce que la variance \\(s_n^2\\) ne soit pas dominée par un petit nombre de \\(x_i\\): \\[\\max_{i=1, \\dotsc, n} \\frac{|x_i|^2}{s_n^2} \\to 0.  \\tag{19.4}\\]\n\nThéorème 19.3 Sous les hypothèses précédentes, Équation 19.3 converge en loi lorsque \\(n\\to\\infty\\) vers une \\(N(0,1)\\).\n\n\nPreuve. La démonstration repose sur Théorème 19.2 appliqué aux \\(X_i = x_i \\varepsilon_i\\) : ces variables sont centrées, et leur variance est \\(\\sigma^2 x_i^2\\). En particulier, \\[s_n^2 = \\sigma^2 \\sum_{i=1}^n x_i^2.\\] Le terme \\(\\mathbb{E}[|X_i|\\mathbf{1}_{|X_i|&gt;\\delta s_n}]\\) vaut \\(x_i^2 \\mathbb{E}[\\varepsilon^2 \\mathbf{1}_{|\\varepsilon|&gt;\\delta s_n / |x_i|}]\\). Par l’inégalité de Cauchy-Schwarz, \\(\\mathbb{E}[\\varepsilon^2 \\mathbf{1}_{|\\varepsilon|&gt;\\delta s_n / |x_i|}]\\) est borné par \\(\\sqrt{\\mathbb{E}[\\varepsilon^4]\\mathbb{P}(|\\varepsilon|&gt;\\delta s_n / |x_i|)} = \\sigma^2 c \\sqrt{\\mathbb{P}(|\\varepsilon|&gt; \\delta s_n / |x_i|)}\\), qui est également plus petit que \\(\\sigma^2 c \\sqrt{\\mathbb{P}(|\\varepsilon|&gt;\\delta m_n)}\\) où \\(m_n\\) est le plus petit des nombres \\(s_n/|x_1|, \\dotsc, s_n/|x_n|\\), c’est-à-dire l’inverse de la racine carrée de Équation 19.4.\nEn regroupant tout ceci, on voit que Équation 19.2 devient plus petite que \\[\\frac{\\sigma^2 c}{s_n^2}\\sum_{i=1}^n x_i^2\\sqrt{\\mathbb{P}(|\\varepsilon|&gt;\\delta m_n)}\\] c’est-à-dire \\(c\\times \\sqrt{\\mathbb{P}(|\\varepsilon|&gt;\\delta m_n)}\\). Comme \\(m_n\\to \\infty\\) par Équation 19.4, ce terme tend vers zéro."
  },
  {
    "objectID": "app2.html#footnotes",
    "href": "app2.html#footnotes",
    "title": "19  50 nuances de TCL",
    "section": "",
    "text": "J’ai l’impression que ce nom n’est guère répandu dans la littérature, mais je l’ai trouvé dans le livre Introduction à l’économétrie de Brigitte Dormont. ↩︎"
  }
]