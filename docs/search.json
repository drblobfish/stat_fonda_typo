[
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "Statistiques Fondamentales",
    "section": "Organisation",
    "text": "Organisation\n\nLes CM ont lieu les jeudi à (8h30 - 10h30), et les vendredi (10h45 - 12h45) sauf le premier cours qui a lieu lundi 8 janvier à 10h45-12h45.\nLes TD ont lieu lundi (13h45 - 16h45) et vendredi (13h30 - 15h30), de lundi 8 janvier à vendredi 16 février.\nIl y aura deux contrôles de 2h, le vendredi 26 janvier et lundi 12 février.\nL’examen a lieu le 1er mars de 13h30 à 16h30.\nIl y aura une interro de 5 minutes chaque semaine le jeudi."
  },
  {
    "objectID": "index.html#utiliser-ce-site",
    "href": "index.html#utiliser-ce-site",
    "title": "Statistiques Fondamentales",
    "section": "Utiliser ce site",
    "text": "Utiliser ce site\nChaque chapitre de ce livre contient une page dédiée au cours théorique, et contiendra dans un futur proche une page d’exercices.\nLa saveur du cours est essentiellement mathématique et nous n’aurons pas de TP d’info ; cependant, je vous recommande vraiment d’essayer d’appliquer tout ça via votre langage de programmation favori, c’est-à-dire Python R SAS C++ Julia. J’essaierai autant que possible de fournir des mini-jeux de données avec des petits challenges pour appliquer ce que vous apprenez en cours.\nCes notes sont mises en lignes et totalement accessibles via Quarto. Si vous savez comment utiliser git, n’hésitez pas à corriger toutes les erreurs que vous pourriez voir (et Dieu sait qu’elles seront nombreuses) via des pull requests."
  },
  {
    "objectID": "ch1.html#un-exemple-pour-fixer-les-idées",
    "href": "ch1.html#un-exemple-pour-fixer-les-idées",
    "title": "1  Introduction",
    "section": "1.1 Un exemple pour fixer les idées",
    "text": "1.1 Un exemple pour fixer les idées\nUne grande enseigne de distribution possède \\(n=100\\) magasins identiques, qui génèrent chaque année un chiffre d’affaire annuel (CA, en millions d’euros). Ce chiffre oscille autour d’une valeur de référence \\(\\mu\\). Cette valeur n’est pas observée ; ce qui est observé, ce sont tous les chiffres d’affaires des \\(n\\) magasins, qui fluctuent tous autour de la vraie valeur \\(\\mu\\). Ces fluctuations proviennent de nombreuses sources : erreurs comptables, perturbations des ventes dues aux fournisseurs ou aux prix, etc. Ce qu’on observe, c’est donc des chiffres \\(x_1, \\dotsc, x_n\\) qui ne sont pas tous égaux ; comment avoir une idée de la véritable valeur de \\(\\mu\\) ?\nEstimation. Évidemment, la moyenne empirique \\[\\bar x_n = \\frac{x_1+\\dotsb + x_n}{n}\\] vient naturellement à l’esprit. En faisant le calcul, on trouve \\(\\bar{x}_n \\approx 21,6\\). Cette valeur est une estimation du CA moyen \\(\\mu\\). Ce chiffre peut être utilisé par l’enseigne, par exemple pour jauger la rentabilité d’un possible plan d’ouverture de nouveaux magasins.\nPrécision. On pourrait se demander à quel point cette estimation est précise ou, disons, essayer de quantifier l’erreur possible qu’on fait si l’on dit que \\(\\mu\\) est égal à 21,6 millions d’euros. Cela nécessite de faire quelques hypothèses sur le hasard qui génère les fluctuations des \\(x_i\\) autour de \\(\\mu\\). Ces fluctuations observées au cours de l’année proviennent de l’agrégation de toutes les fluctuations quotidiennes, lesquelles sont à peu près indépendantes, et pour cette raison on peut supposer (pour commencer) que ces fluctuations sont gaussiennes et ont à peu près la même variance, disons \\(\\sigma^2=1\\). Comme on a supposé que les \\(x_i\\) sont des réalisations d’une loi gaussienne \\(N(\\mu, 1)\\), alors on sait que \\(\\bar{x}_n\\) est la réalisation d’une loi \\(N(\\mu, 1/n)\\), ou encore que \\(\\bar{x}_n - \\mu\\) est la réalisation d’une gaussienne centrée de variance \\(1/n\\). Les lois gaussiennes sont bien connues ; par exemple, avec probabilité supérieure à 99%, une gaussienne \\(N(0, \\sigma^2)\\) est comprise entre les valeurs \\(-2,96\\sigma\\) et \\(2,96\\sigma\\). Autrement dit, il y a 99% de chances pour que le nombre \\(|\\bar{x}-\\mu|\\), qui représente l’erreur d’estimation, soit plus petite que \\(2,96/\\sqrt{n} = 2,96/10 \\approx 0,3\\).\nCe dernier raisonnement peut être vu d’une autre façon. Dire que \\(\\bar{x}_n\\) et \\(\\mu\\) ne diffèrent pas de plus de \\(0,3\\), c’est équivalent à dire que \\(\\mu\\) appartient à l’intervalle \\([\\bar{x} - 0,3, \\bar{x} +0,3]\\). En d’autres termes, avec une probabilité supérieure à 99%, le vrai CA \\(\\mu\\) de chaque magasin se situe entre \\(21,3\\) et \\(21,9\\). Cela laisse tout de même une chance de 1% que le paramètre \\(\\mu\\) ne soit pas dans cette région.\nTests. Il existe encore un autre point de vue sur ce problème. Par exemple, le conseil d’administration de la firme veut s’assurer que le dirigeant a bien tenu sa promesse selon laquelle le CA de chaque magasin était supérieur à 21 millions d’euros. La valeur exacte de \\(\\mu\\) n’est pas le plus important : ce qui nous intéresse maintenant, c’est plutôt d’être sûrs que \\(\\mu\\) n’est pas inférieur au seuil de 21. Le dirigeant, fin statisticien, effectue alors un raisonnement par l’absurde en probabilité. Supposons que le CA \\(\\mu\\) soit effectivement égal à 21 (ou même, inférieur). Alors, par les mêmes calculs que ci-dessus, cela voudrait dire qu’avec 99% de chances, \\(\\bar{x}_n\\) et \\(21\\) ne devraient pas différer de plus de \\(0,3\\) ; autrement dit, que \\(\\bar{x}_n\\) devrait se situer entre \\(20,7\\) et \\(21,3\\). Ce n’est pas le cas, puisque \\(\\bar{x}_n = 21,6\\). Si \\(\\mu\\) est réellement plus petit que 21, alors ce qu’on a observé est extrêmement peu probable. Par contraposée probabiliste, il est raisonnable de rejeter l’hypothèse selon laquelle \\(\\mu\\) est inférieur à 21.\n\nLes trois points de vue donnés ci-dessus sont en quelque sorte les piliers de l’analyse statistique. L’estimation consiste à deviner une valeur cachée dans du bruit ; les intervalles de confiance consistent à donner une région dans laquelle se trouve cette valeur ; les tests d’hypothèse permettent de raisonner de façon logique sur cette valeur.\n\nL’objectif du cours de statistiques de quantifier l’incertitude liée au hasard dans chacun de ces objectifs. Comme dans les exemples donnés ci-dessus, c’est un ensemble de méthodes scientifiques qui s’appuient sur la théorie des probabilités ; dans ce cours, on fera des hypothèses sur le hasard qui est en jeu, et on en tirera des conséquences probables sur le modèle sous-jacent. En théorie des probabilités, le jeu est plutôt inverse : partant d’un modèle probabiliste fixé, on essaie de déterminer quel sera le comportement des réalisations de ce modèle. Il semble difficile de faire l’un sans l’autre."
  },
  {
    "objectID": "ch1.html#quest-ce-quun-problème-statistique",
    "href": "ch1.html#quest-ce-quun-problème-statistique",
    "title": "1  Introduction",
    "section": "1.2 Qu’est-ce qu’un problème statistique ?",
    "text": "1.2 Qu’est-ce qu’un problème statistique ?\nIl n’y aurait pas de statistiques s’il n’y avait pas de monde réel, et comme chacun sait, le monde réel est principalement composé de quantités aléatoires.\nUn problème statistique tire donc toujours sa source d’un ensemble d’observations, disons \\(n\\) observations notées \\(x_1, \\dotsc, x_n\\) ; cet ensemble d’observations est appelé un échantillon. L’hypothèse de base de tout travail statistique consiste à supposer que cet échantillon suit une certaine loi de probabilité ; l’objectif est de trouver laquelle. Évidemment, on ne va pas partir de rien : il faut bien faire des hypothèse minimales sur cette loi. Ce qu’on appelle un modèle statistique est le choix d’une famille de lois de probabilités que l’on suppose pertinentes.\n\nDéfinition 1.1 Formellement, choisir un modèle statistique revient à choisir trois choses : \n\n\\(\\mathcal{X}\\), l’espace dans lequel vit notre échantillon ; \n\\(\\mathscr{F}\\), une tribu sur \\(\\mathcal{X}\\), pour donner du sens à ce qui est observable ou non ;\n\\((P_\\theta)_{\\theta \\in \\Theta}\\), une famille de mesures de probabilités sur \\(\\mathcal{X}\\) indexée par \\(\\theta \\in \\Theta\\), où \\(\\Theta\\) est appelé espace des paramètres. On écrira fréquemment \\(\\mathbb{E}_\\theta\\) ou \\(\\mathrm{Var}_\\theta\\) pour désigner des espérances, variances, etc., calculées avec la loi \\(P_\\theta\\).\n\n\nEn pratique, dans ce cours, on aura toujours un échantillon \\((x_1, \\dotsc, x_n)\\) où les \\(x_i\\) vivent dans un même espace, disons \\(\\mathbb{R}^d\\) pour simplifier. On devrait donc écrire \\(\\mathcal{X} = \\mathbb{R}^{d\\times n}\\) ; et l’on fera toujours l’hypothèse que ces observations sont indépendantes les unes des autres, et que ces observations ont la même loi de probabilité. Autrement dit, on se donnera toujours une mesure \\(p_\\theta\\) sur \\(\\mathbb{R}^d\\) et on supposera que la loi de notre échantillon est \\(P_\\theta = p_\\theta^{\\otimes n}\\). Dans ce cadre, les observations \\(x_i\\) sont des réalisations de variables aléatoires \\(X_i\\) iid de loi \\(p_\\theta\\).\nIl faut prendre garde à distinguer les variables aléatoires \\(X_i\\), qui sont des objets théoriques, de leurs réalisations \\(x_i\\), qui, elles, sont bel et bien observées.\n\nDéfinition 1.2 On dit qu’un modèle statistique est identifiable si \\(\\theta \\neq \\theta'\\) entraîne \\(P_\\theta \\neq P_{\\theta'}\\).\n\nSi l’on a bien choisi notre modèle statistique, alors il existe un « vrai » paramètre, disons \\(\\theta_\\star\\), tel que les observations \\(x_1, \\dotsc, x_n\\) sont des réalisations de loi \\(p_{\\theta_\\star}\\). L’objectif est alors de trouver \\(\\theta_\\star\\) ou quelque information que ce soit le concernant.\nDans un modèle identifiable, la statistique inférentielle (classique) permet de faire trois choses :\n\nTrouver une valeur approchée du vrai paramètre \\(\\theta_\\star\\) (estimation ponctuelle).\nDonner une zone de \\(\\Theta\\) dans laquelle le vrai paramètre \\(\\theta_\\star\\) a des chances de se trouver (intervalle de confiance).\nRépondre à des questions binaires sur \\(\\theta_\\star\\), par exemple « \\(\\theta_\\star\\) est-il positif ? »."
  },
  {
    "objectID": "ch1.html#quest-ce-quun-estimateur",
    "href": "ch1.html#quest-ce-quun-estimateur",
    "title": "1  Introduction",
    "section": "1.3 Qu’est-ce qu’un estimateur ?",
    "text": "1.3 Qu’est-ce qu’un estimateur ?\n\nDéfinition 1.3 Une statistique est une fonction mesurable des observations. Plus formellement, si le modèle statistique fixé est \\((\\mathcal{X}, \\mathscr{F}, P)\\), alors une statistique est n’importe quelle fonction mesurable de \\((\\mathcal{X}, \\mathscr{F})\\).\n\n\nLe premier point important est qu’une statistique ne peut pas prendre \\(\\theta\\) en argument. Ses valeurs ne doivent dépendre du paramètre \\(\\theta\\) qu’au travers de \\(P_\\theta\\).\nLe second point important est que, si \\(X\\) est une variable aléatoire et \\(T\\) une statistique, alors \\(T(X)\\) est une variable aléatoire. On peut donc définir des quantités théoriques liées à \\(T\\): typiquement, si \\(X\\) a pour loi \\(P_\\theta\\), on peut définir la valeur moyenne de \\(T\\) sous le modèle \\(P_\\theta\\) comme \\[\\mathbb{E}_\\theta[T(X)] = \\int_{\\mathcal{X}} T(x) P_\\theta(dx)\\] ou encore sa variance \\(\\mathbb{E}_\\theta[T(X)^2] - (\\mathbb{E}_\\theta[T(X)])^2\\), etc. On peut aussi calculer la valeur de cette statistique sur l’échantillon dont on dispose, c’est-à-dire \\(T(x_1, \\dotsc, x_n)\\). Par exemple, la moyenne empirique d’un \\(n\\)-échantillon réel est la fonction \\(T : (a_1, \\dots, a_n) \\to n^{-1}(a_1+\\dotsb + a_n)\\). Si les \\(x_i\\) sont des réalisations des variables aléatoires \\(X_i\\), alors \\(T(x_1, \\dotsc, x_n)\\) est une réalisation de la variable aléatoire \\(T(X_1, \\dotsc, X_n)\\).\nCe qui ne se voit pas dans la définition, c’est qu’une bonne statistique devrait être facilement calculable ; à la place de statistique, on peut penser à algorithme : une bonne statistique doit pouvoir être calculée facilement par un algorithme ne prenant en entrée que les échantillons \\(x_i\\).\n\nSi le but est de deviner la valeur de \\(\\theta\\) à partir des observations, il est naturel de considérer des statistiques à valeurs dans \\(\\Theta\\). C’est précisément la définition d’un estimateur.\n\nDéfinition 1.4 Dans le modèle \\((\\mathcal{X},\\mathcal{A}, (P_\\theta)_{\\theta \\in \\Theta})\\), un estimateur de \\(\\theta\\) est une statistique à valeurs dans \\(\\Theta\\).\n\nEn fait, on n’est pas obligés de vouloir estimer précisément \\(\\theta\\). Peut-être qu’on veut estimer quelque chose qui dépend de \\(\\theta\\), mais qui n’est pas \\(\\theta\\) ; disons, une fonction \\(\\varphi(\\theta)\\). Dans ce cas, un estimateur de \\(\\varphi(\\theta)\\) sera simplement une statistique à valeurs dans l’espace où vit \\(\\varphi(\\theta)\\)."
  },
  {
    "objectID": "ch1.html#points-de-vue",
    "href": "ch1.html#points-de-vue",
    "title": "1  Introduction",
    "section": "1.4 Points de vue",
    "text": "1.4 Points de vue\nInférence paramétrique. La plupart des expériences/modèles statistiques que nous rencontrerons dans ce cours, seront de nature dite paramétrique, autrement dit indexés par des parties de \\(\\mathbb{R}^d\\). Le mot “paramètre” est en lui-même trompeur : on parle souvent de paramètre d’une distribution pour désigner ce qui devrait plutôt s’appeler une fonctionnelle. Par exemple, la moyenne, la covariance d’une distribution sur \\(\\mathbb{R}^d\\) sont des paramètres de cette distribution. Les quantiles, l’asymétrie, la kurtosis sont d’autres paramètres.\nStatistique non paramétrique. Tous les modèles ne sont pas paramétriques au sens ci-dessus : dans de nombreux développements des statistiques, par exemple en estimation de densité, on travaille sur des modèles plus riches qui n’admettent pas de paramétrisation naturelle par une partie d’un espace euclidien de dimension finie. C’est ce qu’on appelle l’ estimation non-paramétrique. Nous y reviendrons au dernier chapitre.\nStatistique bayésienne. En statistique paramétrique, les paramètres \\(\\theta\\) déterminent le hasard qui génère les observations \\(x_i\\). La statistique bayésienne consiste à renverser le point de vue, et à rendre le paramètre \\(\\theta\\) lui-même aléatoire ; sa loi, appelée prior, mesure “le degré de connaissance a priori” qu’on en a. La règle de Bayes explique comment cette loi est modifiée par les observations. C’est un point de vue qui ne sera pas abordé dans ce cours."
  },
  {
    "objectID": "ch2.html#précision-dun-estimateur",
    "href": "ch2.html#précision-dun-estimateur",
    "title": "2  Estimation de paramètre",
    "section": "2.1 Précision d’un estimateur",
    "text": "2.1 Précision d’un estimateur\n\nDéfinition 2.1 (Biais , risque quadratique)  \n\nLe biais de \\(\\hat{\\theta}\\) est la quantité \\(\\mathbb{E}_\\theta[\\hat\\theta - \\theta]\\). L’estimateur est dit sans biais s’il est de biais nul.\nLe risque quadratique de \\(\\hat\\theta\\) est la quantité \\(\\mathbb{E}_{\\theta}[ |\\hat{\\theta}- \\theta|^2]\\).\n\n\nEn pratique, on peut vouloir estimer non pas \\(\\theta\\) lui-même, mais un paramètre \\(\\psi = \\psi_\\theta\\) qui dépend de \\(\\theta\\), comme \\(\\cos(\\theta)\\) ou \\(|\\theta|\\) par exemple. Dans ce cas, si \\(\\hat{\\psi}\\) est un estimateur de \\(\\psi\\) alors le biais est défini par \\(\\mathbb{E}_\\theta[\\hat{\\psi} - \\psi_\\theta]\\) et le risque quadratique par \\(\\mathbb{E}_\\theta [ |\\hat\\psi^2 - \\psi_\\theta|^2]\\).\nLa dépendance du risque quadratique vis à vis de la taille de l’échantillon est une question importante en statistique mathématique. Elle concerne la vitesse d’estimation (pour une suite d’expériences donnée, quelles sont les meilleures vitesses envisageables, et comment les obtenir ?).\n\nThéorème 2.1 (Décomposition biais-variance) \\[\n\\mathbb{E}_{\\theta} [|\\hat{\\theta}-\\theta|^2]\n= \\underbrace{\\operatorname{Var}_{\\theta} (\\hat{\\theta})}_{\\text{variance}} +\n\\underbrace{\\mathbb{E}_{\\theta}[\\hat{\\theta}-\\theta]^2}_{\\text{carré du biais}} \\, .\n\\]"
  },
  {
    "objectID": "ch2.html#consistances",
    "href": "ch2.html#consistances",
    "title": "2  Estimation de paramètre",
    "section": "2.2 Consistances",
    "text": "2.2 Consistances\nPour introduire la notion de consistance d’une suite d’estimateurs, nous aurons besoin des notions de convergence en probabilité et de convergence presque sûre.\n\nDéfinition 2.2 Une suite de variables aléatoires \\(X_n\\) à valeurs dans \\(\\mathbb{R}^k\\) converge en probabilité vers une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb{R}^k\\), vivant sur cet espace probabilisé si et seulement si, pour tout \\(\\epsilon &gt;0\\) \\[\n\\lim_{n\\to\\infty} \\mathbb{P} (| X_n -X|&gt; \\epsilon ) = 0 \\, .\n\\]\n\n\nDéfinition 2.3 (consistance d’un estimateur) Une suite d’estimateurs \\((\\widehat{\\theta}_n)\\) est consistante pour l’estimation de \\(\\theta\\) lorsque, pour tout \\(\\theta \\in \\Theta\\), \\[ \\forall \\varepsilon&gt;0, \\qquad \\lim_n     P_\\theta ( | \\widehat{\\theta}_n-\\theta| &gt; \\varepsilon ) =0.\n\\] La suite est fortement consistante si, pour tout \\(\\theta \\in \\Theta\\), \\[\n\\hat{\\theta}_n \\to \\theta \\quad P_\\theta-\\text{p.s.}\n\\]"
  },
  {
    "objectID": "ch2.html#normalité-asymptotique",
    "href": "ch2.html#normalité-asymptotique",
    "title": "2  Estimation de paramètre",
    "section": "2.3 Normalité asymptotique",
    "text": "2.3 Normalité asymptotique\nLorsqu’un estimateur est consistant, on peut se demander à quoi ressemblent ses fluctuations autour de sa valeur limite. Le théorème central limite indique que le comportement asymptotique gaussiens est relativement fréquent, et beaucoup d’estimateurs sont des sommes de réalisations de variables indépendantes.\n\nDéfinition 2.4 (normalité asymtotique) Soit \\(\\theta\\) un paramètre à estimer, et \\(\\hat{\\theta}_n\\) une suite d’estimateurs de \\(\\theta\\). On dit que ces estimateurs sont asymptotiquement gaussiens (ou normaux) si, après les avoir renormalisés convenablement, ils convergent en loi vers une loi gaussienne. Autrement dit, s’il existe une suite \\(a_n\\) de nombres réels tels que \\[ a_n(\\hat{\\theta}_n - \\theta) \\xrightarrow[n\\to \\infty]{\\text{loi}} N(0,\\Sigma)\\] où \\(\\Sigma\\) est une matrice de covariance qui dépend peut-être de \\(\\theta\\) — pour éviter les cas dégénérés, on demande à ce que \\(\\Sigma\\) soit non-nulle."
  },
  {
    "objectID": "ch2.html#deux-outils-sur-la-normalité-asymptotique",
    "href": "ch2.html#deux-outils-sur-la-normalité-asymptotique",
    "title": "2  Estimation de paramètre",
    "section": "2.4 Deux outils sur la normalité asymptotique",
    "text": "2.4 Deux outils sur la normalité asymptotique\nLa normalité asymptotique n’est pas intéressante en elle-même ; l’idée est plutôt de chercher le comportement asymptotique de la statistique recentrée pour pouvoir en déduire des garanties en terme de risque asymptotique ou d’intervalle de confiance. Nous utiliserons cela de nombreuses fois dans la suite ; la normalité asymptotique sera par exemple la clé de la construction de nombreux intervalles de confiance. Aussi, prouver que des estimateurs sont asymptotiquement normaux est une tache importante, qui est grandement simplifiée par les deux outils suivants.\n\nThéorème 2.2 (Lemme de Slutsky) Soit \\((X_n)\\) une suite de variables aléatoire qui converge en loi vers \\(X\\) et \\((Y_n)\\) une suite de variables aléatoires qui converge en probabilité (ou en loi) vers une constante \\(c\\). Alors, le couple \\((X_n, Y_n)\\) converge en loi vers \\((X,c)\\) ; autrement dit, pour toute fonction continue bornée \\(\\varphi\\), \\[\\mathbb{E}[\\varphi(X_n, Y_n)] \\to \\mathbb{E}[\\varphi(X,c)].\\]\n\n\nPreuve. Fixons une fonction test \\(\\varphi\\) continue à support compact, donc bornée par un certain \\(M\\). Il faut montrer que \\(\\mathbb{E}[\\varphi(X_n, Y_n) - \\varphi(X,c)]\\) tend vers zéro. L’intégrande est égal à la somme de \\(A = \\varphi(X_n, Y_n) - \\varphi(X_n, c)\\) et de \\(B=\\varphi(X_n, c) - \\varphi(X, c)\\).\nComme \\(X_n\\) tend en loi vers \\(X\\) et que \\(t\\to \\varphi(t,c)\\) est continue bornée, l’espérance de \\(B\\) tend vers zéro. Il faut donc montrer que l’espérance de \\(A\\) tend vers zéro. On fixe un \\(\\varepsilon&gt;0\\).\n\nPar le théorème de Heine, \\(\\varphi\\) est uniformément continue : il existe \\(\\delta&gt;0\\) tel que \\(|(x,y) - (x', y')|&lt;\\delta\\) entraîne que \\(|\\varphi(x,y) - \\varphi(x', y')|&lt; \\varepsilon/2\\).\nOn introduit l’événement \\(\\{|Y_n - c|\\leqslant \\delta\\}\\). Par le point précédent, sur cet événement on a \\(|A| &lt; \\varepsilon/2\\). Hors de cet événement, on peut toujours borner \\(|A|\\) par \\(2M\\). On a donc \\[|\\mathbb{E}A| \\leqslant \\mathbb{P}(|Y_n - c|\\leqslant \\delta)\\varepsilon/2 +  \\mathbb{P}(|Y_n - c| &gt; \\delta)2M.\\]\nComme \\(Y_n\\) converge en probabilité vers \\(c\\), lorsque \\(n\\) est assez grand on a \\(\\mathbb{P}(|Y_n - c| &gt; \\delta) &lt; \\varepsilon/4M\\).\nEn regroupant tout ce qui a été dit, on obtient bien \\(|\\mathbb{E}A| \\leqslant \\varepsilon\\) dès que \\(n\\) est assez grand, ce qui montre bien que \\(\\mathbb{E}A \\to 0\\).\n\n\n\n\nThéorème 2.3 (Delta-méthode) Soit \\((X_n)\\) une suite de variables aléatoires réelles telle que \\(\\sqrt{n}(X_n - \\alpha)\\) converge en loi vers \\(N(0,\\sigma^2)\\). Pour toute fonction \\(g : \\mathbb{R} \\to \\mathbb{R}\\) dérivable en \\(\\alpha\\) (de dérivée non nulle en \\(\\alpha\\)), on a \\[ \\sqrt{n}(g(X_n) - g(\\alpha)) \\xrightarrow[n \\to \\infty]{\\text{loi}} N(0, g'(\\alpha)^2 \\sigma^2).\\]\n\nPlus généralement, si les \\(X_n\\) sont à valeurs dans \\(\\mathbb{R}^d\\) et que \\(\\sqrt{n}(X_n - \\alpha) \\to N(0,\\Sigma)\\), alors pour toute application \\(g:\\mathbb{R}^d \\to \\mathbb{R}^k\\), la suite \\(\\sqrt{n}(g(X_n) - g(\\alpha))\\) converge en loi vers \\[N(0, Dg(\\alpha)\\Sigma Dg(\\alpha)^\\top)\\] où \\(Dg(x)\\) est la matrice jacobienne de \\(g\\) en \\(x\\).\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch2.html#la-méthode-des-moments",
    "href": "ch2.html#la-méthode-des-moments",
    "title": "2  Estimation de paramètre",
    "section": "2.5 La méthode des moments",
    "text": "2.5 La méthode des moments\nIl existe plusieurs techniques générales pour construire des estimateurs. La méthode des moments est la plus naturelle, et donne beaucoup des estimateurs avec de bonnes propriétés.\nDans un modèle statistique, supposons qu’on dispose d’une statistiques intégrable \\(T\\) (pas forcément réelle), dont la moyenne n’est pas le paramètre \\(\\theta\\) lui-même, mais plutôt une fonction de \\(\\theta\\) :\n\\[\\mathbb{E}_\\theta[T(X)] = \\varphi(\\theta).\\] C’est cette fonction \\(\\varphi\\) qu’on appelle moment. Typiquement,\n\nla moyenne d’une loi \\(\\mathscr{E}(\\theta)\\) n’est pas \\(\\theta\\) mais \\(1/\\theta\\).\nla moyenne d’une loi log-normale de paramètres \\((0, \\sigma^2)\\) est \\(e^{\\sigma^2/2}\\).\n\nPrenons la moyenne empirique associée à cet estimateur, \\(\\bar{T}_n\\). Par la loi des grands nombres, \\[\\bar{T}_n = \\frac{1}{n}\\sum_{i=1}^n T(X_i) \\to \\varphi(\\theta) \\qquad P_\\theta-ps, \\] ce qui permet d’estimer \\(\\varphi(\\theta)\\). Peut-on alors estimer \\(\\theta\\) ? Si la fonction \\(\\varphi\\) est inversible et si \\(\\bar{T}_n\\) appartient presque sûrement à l’ensemble de définition de \\(\\varphi^{-1}\\), alors \\(\\varphi^{-1}(\\bar{T_n})\\) est bien définie. Pour qu’en plus cette quantité converge presque sûrement vers \\(\\theta\\), il faut s’assurer que \\(\\varphi^{-1}\\) est continue. C’est par exemple le cas lorsque l’ensemble des paramètres \\(\\Theta\\) est un ouvert, et que \\(\\varphi\\) est un difféomorphisme sur son image — une situation si fréquente qu’elle mérite son propre théorème, et si agréable qu’elle garantit que l’estimateur associé est asymptotiquement normal.\n\nThéorème 2.4 (Estimation par moments) Sous l’hypothèse mentionnée ci-dessus (la fonction \\(\\varphi\\) est un difféomorphisme), l’estimateur \\[\\hat{\\theta}_n = \\varphi^{-1}(\\bar{T}_n)\\] est presque sûrement bien défini pour tout \\(n\\) suffisamment grand ; il est également consistant pour l’estimation de \\(\\theta\\). En outre, si \\(T\\) est de carré intégrable, cet estimateur est asymptotiquement normal, au sens où \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta)\\) converge en loi vers une gaussienne centrée de matrice de covariance \\[ (D\\varphi(\\theta))^{-1}\\mathrm{Cov}_\\theta(T)(D\\varphi(\\theta)^\\top)^{-1}.\\]\n\n\nPreuve. La première partie a essentiellement été démontrée un peu plus haut. Pour la seconde, il faut d’abord remarquer que si \\(T\\) est de carré intégrable, alors \\(\\sqrt{n}(\\bar{T}_n - \\varphi(\\theta))\\) converge vers une loi \\(N(0, \\mathrm{Cov}_\\theta(T))\\) par le TCL. Une simple application de la delta-méthode (Théorème 2.3) donne alors le résultat, puisque la matrice jacobienne de \\(\\varphi^{-1}\\) en \\(\\varphi(\\theta)\\) n’est autre que l’inverse de la matrice jacobienne de \\(\\varphi\\) en \\(\\theta\\)."
  },
  {
    "objectID": "ch2.html#deux-estimateurs-importants",
    "href": "ch2.html#deux-estimateurs-importants",
    "title": "2  Estimation de paramètre",
    "section": "2.6 Deux estimateurs importants",
    "text": "2.6 Deux estimateurs importants\nDeux estimateurs sont omniprésents en statistique : la moyenne empirique et la variance empirique. Ils sont pertinents dans n’importe quel modèle où les observations sont des réalisations de variables iid possédant une moyenne \\(\\mu\\) et une variance \\(\\sigma^2\\).\nLa moyenne empirique est définie par \\[ \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\\] Il est évident que \\(\\mathbb{E}[\\bar{X}_n] = \\mathbb{E}[X] = \\mu\\). Cet estimateur est donc toujours sans biais, et son risque quadratique est égal à sa variance, c’est-à-dire \\(\\frac{\\sigma^2}{n}\\).\nL’estimateur de la variance empirique est défini comme \\[\\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2.\\]\n\nThéorème 2.5 L’estimateur \\(\\hat{\\sigma}_n^2\\) est sans biais.\n\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch2_ex.html",
    "href": "ch2_ex.html",
    "title": "3  Exercices",
    "section": "",
    "text": "3.0.1 Questions\n\nMontrer que la convergence en loi vers une constante implique la convergence en probabilité.\nMontrer que, si un modèle statistique n’est pas identifiable, alors il ne peut exister aucun estimateur convergent.\nTrouver un couple de variables aléatoires \\((X_n, Y_n)\\) tel que \\(X_n\\) converge en loi et \\(Y_n\\) converge en loi, mais le couple ne converge pas en loi.\nOn observe un échantillon de lois de Poisson de paramètre \\(\\lambda\\), que l’on estime par la moyenne empirique. Calculer le risque quadratique de cet estimateur.\nQuelle est la loi d’une somme de lois de Bernoulli indépendantes ? L’écart-type ?\n\n\n\n3.0.2 Variance empirique\nOn se donne \\(Y_1, \\dots, Y_n\\), i.i.d de moyenne \\(\\mu\\) et variance \\(\\sigma^2\\).\n\nOn suppose \\(\\mu\\) connu. Donner un estimateur non biaisé de \\(\\sigma^2\\).\nOn suppose \\(\\mu\\) inconnu. Calculer l’espérance de \\(\\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2\\). En déduire un estimateur non biaisé de \\(\\sigma^2\\).\n\n\n\n3.0.3 Estimation de masse\nAu cours de la seconde guerre mondiale, l’armée alliée notait les numéros de série \\(X_1, \\dots, X_n\\) de tous les tanks nazis capturés ou détruits, afin d’obtenir un estimateur du nombre total \\(N\\) de tanks produits.\n\nProposer un modèle pour le tirage de \\(X_1, \\dots, X_n\\).\nCalculer l’espérance de \\(\\bar X_n\\). En déduire un estimateur non biaisé de \\(N\\). Indication: la loi de \\(n\\) tirages sans remise est échangeable.\nÉtudier la loi de \\(X_{(n)}\\) et en déduire un estimateur non biaisé de \\(N\\).\nProposer deux intervalles de confiance de niveau \\(\\alpha\\). %de la forme \\([aS, bS]\\) avec \\(a, b\\in\\mathbb{R}\\) et \\(S\\) une statistique résumée. On pourra utiliser le fait que l’inégalité de Hoeffding s’applique également aux tirages sans remise.\n\nSelon Ruggles et Broodie (1947, JASA), la méthode statistique a fourni comme estimation une production moyenne de 246 tanks/mois entre juin 1940 et septembre 1942. Des méthodes d’espionnage traditionnelles donnaient une estimation de 1400 tanks/mois. Les chiffres officiels du ministère nazi des Armements ont montré après la guerre que la production moyenne était de 245 tanks/mois.\n\n\n3.0.4 Uniforme (1)\nOn considère \\((X_1, \\dots, X_n)\\) un échantillon de loi uniforme sur \\(]\\theta, \\theta+1[\\).\n\nDonner la densité de la loi de la variable \\(R_n=X_{(n)} -X_{(1)}\\), où \\(X_{(1)}=\\min(X_1, \\dots, X_n)\\) et \\(X_{(n)}=\\max(X_1, \\dots, X_n)\\).\nÉtudier les différents modes de convergence de \\(R_n\\) quand \\(n\\to\\infty\\).\nÉtudier le comportement en loi de \\(n(1-R_n)\\) quand \\(n\\to\\infty\\).\n\n\n\n3.0.5 Uniforme (2)\nSoit \\(X_1,\\dots,X_n\\) un échantillon de loi \\(\\mathscr{U}([0,\\theta])\\), avec \\(\\theta &gt;0\\). On veut estimer \\(\\theta\\).\n\nDéterminer un estimateur de \\(\\theta\\) à partir de \\(\\bar{X}_n\\).\nOn considère l’estimateur \\(X_{(n)}= {\\max}_{1\\leq i \\leq n}X_i\\). Déterminer les propriétés asymotptiques de ces estimateurs.\nComparer les performances des deux estimateurs.\n\n\n\n3.0.6 Gamma\nLa loi Gamma \\(\\Gamma(\\alpha, \\beta)\\) de paramètres \\(\\alpha, \\beta&gt;0\\) a pour densité \\[ x\\mapsto \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x), \\quad x&gt;0.\\] On se donne un échantillon \\((X_1,\\dots,X_n)\\) de loi \\(\\Gamma(\\alpha, \\beta)\\) et on chercche à estimer les paramètres.\n\nOn suppose le paramètre \\(\\beta\\) connu. Proposer un estimateur de \\(\\alpha\\) par la méthode des moments.\nOn suppose à présent que les deux paramètres \\(\\alpha, \\beta\\) sont inconnus. Proposer un estimateur de \\((\\alpha,\\beta)\\) par la méthode des moments.\n\n\n\n3.0.7 Gumbel\nLa loi de Gumbel (centrale) de paramètre \\(\\beta\\) a pour fonction de répartition \\(F(x)= e^{-e^{-x/\\beta}}\\). On observe un échantillon de lois de Gumbel et l’on cherche à estimer \\(\\beta\\).\n\nCalculer la densité des lois de Gumbel, ainsi que leur moyenne et variance [indice : \\(0.57721…\\)]\nEn déduire un estimateur convergent dont on calculera le risque quadratique et les propriétés asymptotiques.\n\n\n\n3.0.8 Yule-Simon\nUne variable aléatoire \\(X\\) suit la loi de Yule-Simon de paramètre \\(\\rho&gt;0\\) lorsque \\(\\mathbb{P}(X = n) = \\rho B(n, 1+\\rho)\\), où \\(n\\geqslant 1\\) et \\(B\\) est la fonction beta.\n\nMontrer que si \\(\\rho&gt;1\\), alors \\(\\mathbb{E}[X] = \\rho/(\\rho-1)\\).\nTrouver un estimateur de \\(\\rho\\) et donner ses propriétés."
  },
  {
    "objectID": "ch3.html#principe",
    "href": "ch3.html#principe",
    "title": "4  Intervalles de confiance",
    "section": "4.1 Principe",
    "text": "4.1 Principe\nDans un modèle statistique, l’estimation du paramètre d’intérêt \\(\\theta\\) par intervalles de confiance consiste à spécifier un intervalle calculable à partir des données, et qui contient \\(\\theta\\) avec grande probabilité : en d’autres termes, une région de confiance pour \\(\\theta\\).\nPour simplifier, on supposera d’abord que \\(\\theta\\) est un paramètre réel.\n\nDéfinition 4.1 (intervalle de confiance) Un intervalle de confiance de niveau \\(1-\\alpha\\) est un intervalle \\(I = [A,B]\\) dont les bornes \\(A,B\\) sont des statistiques, et tel que pour tout \\(\\theta\\), \\[P_\\theta(\\theta \\in I) \\geqslant 1 - \\alpha.\\] Un intervalle de confiance de niveau asymptotique \\(1-\\alpha\\) est une suite d’intervalles \\(I_n = [A_n,B_n]\\) dont les bornes \\(A_n,B_n\\) sont des statistiques, et tels que pour tout \\(n\\), \\[ P_\\theta(\\theta \\in I_n) \\geqslant 1 - \\alpha.\\]\n\nLe terme « niveau » désigne \\(1-\\alpha\\) ; la vocation de ce nombre est d’être proche de 1, typiquement 99%. Le nombre \\(\\alpha\\) est parfois appelé « erreur » ou « marge d’erreur » ; la vocation de ce nombre est d’être proche de zéro, typiquement 1%.\nIl n’y a rien d’autre à savoir sur les intervalles de confiance ; tout l’art de la chose consiste à savoir les construire. Commençons par trois exemples essentiels à plusieurs titres."
  },
  {
    "objectID": "ch3.html#exemples-gaussiens",
    "href": "ch3.html#exemples-gaussiens",
    "title": "4  Intervalles de confiance",
    "section": "4.2 Exemples gaussiens",
    "text": "4.2 Exemples gaussiens\nOn dispose de variables aléatoires \\(X_1, \\dotsc, X_n\\) gaussiennes de loi \\(N(\\mu, \\sigma^2)\\). On va donner des intervalles de confiance pour l’estimation des paramètres \\(\\mu\\) et \\(\\sigma\\) dans plusieurs cas de figure. Le moment est idéal pour rappeler l’existence et le calcul des quantiles d’une loi — voir ci dessous.\n\n4.2.1 Estimation de \\(\\mu\\)\nLorsque \\(\\sigma\\) est connue. \nNous avons déjà vu que la moyenne empirique \\(\\bar{X}_n\\) est un estimateur sans biais de \\(\\mu\\). Or, nous savons aussi la loi exacte de \\(\\bar{X}_n\\), qui est \\(N(\\mu, \\sigma^2/n)\\). Autrement dit, \\[\\frac{\\sqrt{n}}{\\sigma}(\\bar{X}_n - \\mu) \\sim N(0,1). \\tag{4.1}\\]\nSi l’on se donne une marge d’erreur \\(\\alpha = 1\\%\\), alors \\[ \\mathbb{P}( (\\sqrt{n}/\\sigma)|\\bar{X}_n - \\mu| &gt; z_{0.99}) = 1\\%\\] où \\(z_{0.99} \\approx 2.32\\). Or, \\[ \\frac{\\sqrt{n}}{\\sigma}|\\bar{X}_n - \\mu| &gt; z_{0.99} \\tag{4.2}\\] est équivalent à \\[ \\bar{X}_n - \\frac{z_{0.99}\\sigma}{\\sqrt{n}} \\leqslant \\mu \\leqslant \\bar{X}_n + \\frac{z_{0.99}\\sigma}{\\sqrt{n}}. \\tag{4.3}\\] Le passage de Équation 4.2 à Équation 4.3 est souvent appelé pivot.\nNous avons donc les deux bornes de notre intervalle de confiance : \\[ A = \\bar{X}_n - \\frac{z_{0.99}\\sigma}{\\sqrt{n}}\\] \\[ B = \\bar{X}_n + \\frac{z_{0.99}\\sigma}{\\sqrt{n}} .\\] Ces deux quantités sont bien des statistiques, car \\(\\sigma\\) est connu. De plus, nous venons de montrer que \\(P_\\mu(\\mu \\in [A,B]) = 99\\%\\). Ici, le choix de la marge d’erreur \\(\\alpha = 1\\%\\) ne jouait aucun rôle particulier ; ainsi, un intervalle de confiance de niveau \\(1-\\alpha\\) pour l’estimation de \\(\\mu\\) est donné par \\[\\left[\\bar{X}_n - \\frac{z_{1-\\alpha}\\sigma}{\\sqrt{n}}~~;~~\\bar{X}_n + \\frac{z_{1-\\alpha}\\sigma}{\\sqrt{n}} \\right]. \\tag{4.4}\\]\nLorsque \\(\\sigma\\) est inconnue. \nLorsque \\(\\sigma\\) n’est pas connu, les bornes \\(A,B\\) ci-dessus ne sont pas des statistiques, car elles dépendent de \\(\\sigma\\). Heureusement, on peut estimer \\(\\sigma\\) sans biais via l’estimateur \\[ \\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2.\\] Que se passe-t-il si, dans la statistique Équation 4.1, on remplace \\(\\sigma\\) par son estimation \\(\\hat{\\sigma}_n^2\\) ? On obtient la statistique dite de Student, \\[T_n = \\frac{\\sqrt{n}}{\\sqrt{\\hat{\\sigma}_n^2}}(\\bar{X}_n - \\mu). \\tag{4.5}\\] Sa loi n’est plus une loi gaussienne, mais une loi de Student à \\(n-1\\) paramètres de liberté : le calcul de la densité est fait en détails dans Section 5.2.3 - Section 5.2.4. Les quantiles des lois de Student ont été calculés avec précision. On notera \\(t_{k,\\alpha}\\) le quantile symétrique de niveau \\(\\alpha\\) d’une loi de Student de paramètre \\(k\\). Alors, \\[ P_{\\mu, \\sigma^2}(|T_n|&gt; t_{n-1,\\alpha})\\leqslant \\alpha .\\] Par le même raisonnement que tout à l’heure, l’inégalité \\[ \\left|\\frac{\\sqrt{n}}{\\hat{\\sigma}^2_n}(\\bar{X}_n - \\mu)\\right| &gt; t_{n-1,\\alpha}\\] est équivalente à \\[ \\bar{X}_n - \\frac{t_{n-1,\\alpha}\\hat{\\sigma}^2_n}{\\sqrt{n}} \\leqslant \\mu \\leqslant \\bar{X}_n + \\frac{t_{n-1,\\alpha}\\hat{\\sigma}^2_n}{\\sqrt{n}}.\\] et les deux côtés de ces inégalités sont des statistiques; en les notant \\(A,B\\), on a bien trouvé un intervalle de confiance de niveau \\(\\alpha\\), c’est-à-dire tel que \\(P_{\\mu,\\sigma^2}(\\mu \\in [A,B]) = \\alpha\\). Cet intervalle de confiance est d’une grande importance en pratique et mérite son propre théorème. Il est dû à William Gosset.\n\nThéorème 4.1 (Intervalle de Student) Un intervalle de confiance de niveau \\(1-\\alpha\\) pour l’estimation de \\(\\mu\\) lorsque \\(\\sigma\\) n’est pas connue est donné par\n\\[\\left[\\bar{X}_n - \\frac{t_{1-\\alpha}\\hat{\\sigma}_n}{\\sqrt{n}}~~;~~\\bar{X}_n + \\frac{t_{1-\\alpha}\\hat{\\sigma}_n}{\\sqrt{n}} \\right].\\]\n\n\n\n4.2.2 Estimation de \\(\\sigma\\)\nSupposons maintenant qu’on désire estimer la variance \\(\\sigma^2\\).\nLorsque \\(\\mu\\) est connue.\nEn supposant que \\(\\mu\\) est connue, l’estimateur des moments le plus naturel pour estimer \\(\\sigma^2\\) est évidemment \\[ \\tilde{\\sigma}^2_n = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2.\\] Comme les \\((X_i - \\mu)/\\sigma\\) sont des variables aléatoires gaussiennes centrées réduites, l’estimateur \\(\\tilde{\\sigma}^2_n \\times (n/ \\sigma^2)\\) est une somme de \\(n\\) gaussiennes standard indépendantes. La loi de cette statistique est connue : c’est une loi du chi-deux à \\(n\\) paramètres de liberté comme démontré dans Section 5.2.2. Cette loi n’est pas symétrique, puisqu’elle est supportée sur \\([0,\\infty[\\). On note souvent \\(k^-_{n,\\alpha}\\) et \\(k^+_{n,\\alpha}\\) les nombres les plus éloignées possible (ils exisent) tels que \\(\\mathbb{P}(k^-_{n,\\alpha}&lt; \\chi^2(n)&lt;k^+_{n,\\alpha}) = 1-\\alpha\\). Ainsi, \\[P_{\\sigma^2}(k^-_{n,\\alpha}&lt; \\frac{n \\tilde{\\sigma}^2_n}{\\sigma^2} &lt; k^+_{n,\\alpha}) = \\alpha.\\] En pivotant comme dans les exemples précédents, on obtient que l’intervalle \\[\\left[\\frac{n\\tilde{\\sigma}_n^2}{k^{+}_{n,\\alpha}} ~~;~~ \\frac{n\\tilde{\\sigma}_n^2}{k^-_{n,\\alpha}} \\right] \\] est un intervalle de confiance de niveau \\(\\alpha\\) pour \\(\\sigma^2\\).\nLorsque \\(\\mu\\) est inconnue.\nCette fois, on utilise l’estimateur déjà évoqué plus tôt, à savoir \\[ \\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2.\\] La loi de \\((n-1)\\hat{\\sigma}^2_n / \\sigma^2\\) est encore une loi du chi-deux, mais à \\(n-1\\) paramètres de liberté. Ainsi, le même raisonnement que ci-dessus donne l’intervalle de confiance de niveau \\(\\alpha\\) suivant :  \\[\\left[\\frac{(n-1)\\hat{\\sigma}_n^2}{k^+_{n-1,\\alpha}} ~~;~~ \\frac{(n-1)\\hat{\\sigma}_n^2}{k^-_{n-1,\\alpha}} \\right]. \\]"
  },
  {
    "objectID": "ch3.html#exemples-asymptotiques",
    "href": "ch3.html#exemples-asymptotiques",
    "title": "4  Intervalles de confiance",
    "section": "4.3 Exemples asymptotiques",
    "text": "4.3 Exemples asymptotiques\nÀ écrire.\n\n4.3.1 Estimation du paramètre \\(p\\) dans un modèle de Bernoulli.\n\n\n4.3.2 Estimation de moyenne dans un modèle non-gaussien."
  },
  {
    "objectID": "ch31_outils.html#quantiles",
    "href": "ch31_outils.html#quantiles",
    "title": "5  Outils pour construire des intervalles de confiance",
    "section": "5.1 Quantiles",
    "text": "5.1 Quantiles\nSi \\(X\\) est une variable aléatoire sur \\(\\mathbb{R}\\), un quantile d’ordre \\(\\alpha \\in ]0,1[\\), noté \\(q_\\alpha\\), est un nombre tel que \\(\\mathbb{P}(X \\leqslant q_\\alpha) = \\alpha\\). Lorsque \\(X\\) est continue, un tel nombre existe forcément, car la fonction de répartition \\(F(x) = \\mathbb{P}(X\\leqslant x)\\) est une surjection continue. Les quantiles symétriques \\(z_\\alpha\\) sont, eux, définis par \\(\\mathbb{P}(|X|\\leqslant z_\\alpha) = \\alpha\\).\nSi la loi de \\(X\\) est de surcroît symétrique, les quantiles symétriques s’expriment facilement en fonction des quantiles classiques. En effet, \\(\\mathbb{P}(|X|\\leqslant z)\\) est égal à \\(\\mathbb{P}(X \\leqslant z) - \\mathbb{P}(X \\leqslant -z)\\). Or, si la loi de \\(X\\) est symétrique, alors \\(\\mathbb{P}(X \\leqslant -z) = 1 - \\mathbb{P}(X \\leqslant z)\\), et donc \\[ \\mathbb{P}(|X|\\leqslant z) = 2\\mathbb{P}(X \\leqslant z) - 1.\\] Il suffit alors de choisir pour \\(z\\) le quantile \\(q_{\\frac{1+\\alpha}{2}}\\) pour obtenir \\(\\mathbb{P}(|X|\\leqslant z) = \\alpha\\).\nLes quantiles s’obtiennent en inversant la fonction de répartition : lorsque celle-ci est une bijection sur \\(]0,1[\\), alors \\(q_\\alpha = F^{-1}(\\alpha)\\). En règle générale, il n’y a pas de forme fermée. Par exemple, pour une loi gaussienne standard, \\[F(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-u^2/2}du\\] qui elle-même n’a pas d’écriture plus simple. Fort heureusement, les outils de calcul numérique permettent d’effectuer ces calculs avec une grande précision. La table suivante donne les quantiles symétriques de la gaussienne.\n\n\n\n\\(\\alpha\\)\n90%\n95%\n98%\n99%\n99.9%\n99.99999%\n\n\n\n\n\\(z_\\alpha\\)\n1.64\n1.96\n2.32\n2.57\n3.2\n5.32\n\n\n\nVoir aussi la règle 1-2-3.\n\nThéorème 5.1 (Queues de distribution de la gaussienne) Si \\(x\\) est plus grand que 1, \\[  \\left(\\frac{1}{x} - \\frac{1}{x^3}\\right) \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}}\\leqslant \\mathbb{P}(X &gt; x) \\leqslant \\frac{1}{x}\\frac{e^{-x^2/2}}{\\sqrt{2\\pi}} \\] En particulier, si \\(x\\) est grand, \\(\\mathbb{P}(X \\geqslant x) \\sim e^{-x^2/2}/x\\sqrt{2\\pi}\\) avec une erreur d’ordre \\(O(e^{-x^2/2}/x^3)\\).\n\nÀ titre d’exemple, pour \\(x=2.32\\) cette approximation donne 98.83%, ce qui est remarquablement proche de 98%. Pour \\(x = 2.57\\) on trouve 99.42%.\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch31_outils.html#calculs-de-lois",
    "href": "ch31_outils.html#calculs-de-lois",
    "title": "5  Outils pour construire des intervalles de confiance",
    "section": "5.2 Calculs de lois",
    "text": "5.2 Calculs de lois\n\n5.2.1 Lois Gamma\nUne variable aléatoire suit une loi Gamma de paramètres \\(\\lambda&gt;0, \\alpha&gt;0\\) lorsque sa densité est donnée par \\[\\gamma_{r,\\alpha}(x) =  \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}e^{-\\lambda x}x^{\\alpha -1}\\mathbf{1}_{x&gt;0}.\\] Les lois Gamma rassemblent les lois exponentielles (\\(\\Gamma(\\lambda, 1) = \\mathscr{E}(\\lambda)\\)) et les lois du chi-deux qu’on verra ci-dessous \\((\\Gamma(1/2, n/2) = \\chi_2(n)\\)). La transformée de Fourier \\(\\varphi_{\\lambda, \\alpha}\\) d’une loi \\(\\Gamma(\\lambda, \\alpha)\\) se calcule facilement par un changement de variables : \\[\\varphi_{\\lambda, \\alpha}(t) = \\left(1 - \\frac{it}{\\lambda}\\right)^{-\\alpha}.\\]\n\n\n5.2.2 Loi du chi-deux\nSoit \\(X\\) une loi gaussienne standard. Calculons la densité de \\(X^2\\) ; pour toute fonction-test \\(\\varphi\\), \\(\\mathbb{E}[\\varphi(X^2)]\\) est donné par \\[\\frac{1}{\\sqrt{2\\pi}}\\int e^{-x^2/2}\\varphi(x^2)dx.\\] Cette intégrale est symétrique, donc on peut ajouter un facteur 2 et intégrer sur \\([0,\\infty[\\). En posant \\(u=x^2\\), on obtient alors la valeur \\[ \\frac{2}{\\sqrt{2\\pi}}\\int_0^\\infty e^{-u/2}\\varphi(u)\\frac{1}{2\\sqrt{u}}du.\\] On reconnaît la densité d’une loi Gamma de paramètres \\((1/2, 1/2)\\). Cette loi s’appelle loi du chi-deux et on la note \\(\\chi_2(1)\\). Sa tranformée de Fourier est donnée par \\[\\mathbb{E}[e^{itX^2}] = \\frac{1}{\\sqrt{1 - 2it}}. \\]\nSoient maintenant \\(X_1,\\dotsc, X_n\\) des lois gaussiennes standard indépendantes. Chaque \\(X_i^2\\) est une \\(\\chi_2(1)\\) ; leur somme a donc pour loi la convolée \\(n\\) fois de \\(\\chi_2\\). Calculons sa tranformée de Fourier :  \\[\\begin{align}\\mathbb{E}[e^{it(X_1^2 + \\dotsb + X_n^2)}] &= \\mathbb{E}[e^{itX_1^2}]^n \\\\ &= (1-2it)^{-\\frac{n}{2}} .\\end{align}\\] On reconnaît la transformée de Fourier d’une loi \\(\\Gamma(n/2, 1/2)\\) ; cette loi s’appelle loi du chi-deux à \\(n\\) paramètres de liberté et elle est notée \\(\\chi_2(n)\\). Sa densité est donnée par \\[ \\frac{1}{2^{n/2}\\Gamma(n/2)}e^{-x/2}x^{n/2 - 1}\\mathbf{1}_{x&gt;0}.\\]\n\n\n5.2.3 Loi de Student\nSoit \\(X\\) une variable aléatoire gaussienne standard et \\(Y_n\\) une variable aléatoire suivant une loi \\(\\chi_2(n)\\) indépendante de \\(X\\). On va calculer la loi de \\(T_n = X/\\sqrt{Y_n/n}\\). Soit \\(f\\) une fonction test ; l’espérance \\(\\mathbb{E}[f(T_n)]\\) est égale à \\[\\frac{1}{Z_n\\sqrt{2\\pi}}\\int_0^\\infty \\int_{-\\infty}^{\\infty}  f\\left(\\frac{x}{\\sqrt{y/n}}\\right) e^{-\\frac{x^2}{2}}e^{-\\frac{y}{2}}y^{\\frac{n}{2} - 1}dxdy \\] où \\(Z_n = 2^{n/2}\\Gamma(n/2)\\). Dans l’intégrale en \\(x\\), on effectue le changement de variable \\(u = x/\\sqrt{y/n}\\) afin d’obtenir \\[\\frac{1}{Z_n\\sqrt{2\\pi}}\\int_0^\\infty \\int_{-\\infty}^{\\infty}  f(u) e^{-\\frac{yu^2}{2n}}e^{-\\frac{y}{2}}y^{\\frac{n}{2} - 1}\\sqrt{\\frac{y}{n}} dxdy. \\] La densité de \\(T_n\\) est donc donnée par \\[t_n(u)= \\frac{1}{Z_n\\sqrt{2\\pi n}}\\int_0^\\infty  e^{-\\frac{yu^2}{2n}-\\frac{y}{2}}y^{\\frac{n+1}{2}-1} dy. \\] Le changement de variables \\(z = y(1+u^2/n)/2\\) nous ramène à une fonction Gamma :  \\[t_n(u) = \\frac{1}{Z_n\\sqrt{2\\pi n}}\\left(\\frac{2}{1+\\frac{u^2}{n}}\\right)^{\\frac{n+1}{2}}\\int_0^\\infty  e^{-z}z^{\\frac{n+1}{2}- 1} dz.\\] On reconnaît \\(\\Gamma((n+1)/2)\\) à droite. La densité \\(t_n(x)\\) est donc égale à \\[t_n(x) = \\frac{1}{\\sqrt{n\\pi}}\\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)}\\left(\\frac{1}{1 + \\frac{u^2}{n}}\\right)^{\\frac{n+1}{2}}.\\]\nLa loi de Student de paramètre \\(n=1\\) est tout simplement une loi de Cauchy.\n\n\n5.2.4 Loi de la statistique de Student\nSoient \\(X_1, \\dotsc, X_n\\) des variables gaussiennes \\(N(\\mu, \\sigma^2)\\) indépendantes, et soit \\(T_n = (\\bar{X}_n-\\mu)/\\sqrt{\\hat{\\sigma}^2_n}\\), où \\[\\hat{\\sigma}^2_n = \\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}. \\]\n\nThéorème 5.2 La variable aléatoire \\(T_n\\) suit une loi de Student de paramètre \\(n-1\\).\n\n\nPreuve. On va montrer 1° que \\(\\bar{X}_n\\) et \\(\\sqrt{\\hat{\\sigma}^2_n / \\sigma^2}\\) sont indépendantes, et 2° que \\(\\sqrt{\\hat{\\sigma}^2_n / \\sigma^2}\\) a bien la même loi que \\(\\sqrt{Y_{n-1}/(n-1)}\\) où \\(Y_{n-1}\\) est une \\(\\chi_2(n-1)\\). Dans la suite, on supposera que \\(\\mu=0\\) et \\(\\sigma=1\\), ce qui n’enlève rien en généralité.\nPremier point.  Le vecteur \\(X=(X_1, \\dotsc, X_n)\\) est gaussien. Posons \\(Z = (X_1 - \\bar{X}_n, \\dotsc, X_n - \\bar{X}_n)\\). Le couple \\((\\bar{X}_n, Z_n)\\) est linéaire en \\(X\\), donc ce couple est aussi un vecteur gaussien. Or, la covariance de ses deux éléments est nulle. Par exemple, \\(\\mathrm{Cov}(\\bar{X}_n, Z_1)\\) est égale à \\(\\mathrm{Cov}(\\bar{X}_n, X_1) - \\mathrm{Var}(\\bar{X}_n)\\), ce qui par linéarité donne \\(1/n - 1/n = 0\\). Ainsi, \\(\\bar{X}_n\\) et \\(Z\\) sont deux variables conjointement gaussiennes et décorrélées : elles sont donc indépendantes. Comme \\(\\hat{\\sigma}_n\\) est une fonction de \\(Z\\), elle est aussi indépendante de \\(\\bar{X}_n\\).\nSecond point.  \\(Z\\) est la projection orthogonale de \\(X\\) sur le sous-espace vectoriel \\(\\mathscr{V}=\\{x \\in \\mathbb{R}^n : x_1 + \\dotsc + x_n = 0\\}\\). Soit \\((f_i)_{i=2, \\dotsc, n}\\) une base orthonormale de \\(\\mathscr{V}\\), de sorte que \\(Z = \\sum_{i=2}^n \\langle f_i, X\\rangle f_i\\). Par l’identité de Parseval, \\[|Z|^2 = \\sum_{i=2}^n |\\langle f_i, X \\rangle|^2.\\] Or, les \\(n-1\\) variables aléatoires \\(G_i = \\langle f_i, X\\rangle\\) sont des gaussiennes standard iid. En effet, on vérifie facilement que \\(\\mathrm{Cov}(G_i, G_j) = \\langle f_i, f_j\\rangle = \\delta_{i,j}\\). On en déduit donc que \\(|Z|^2\\) suit une loi \\(\\chi_2(n-1)\\)."
  },
  {
    "objectID": "ch31_outils.html#inégalités-de-concentration",
    "href": "ch31_outils.html#inégalités-de-concentration",
    "title": "5  Outils pour construire des intervalles de confiance",
    "section": "5.3 Inégalités de concentration",
    "text": "5.3 Inégalités de concentration\nLes outils de base pour construire des intervalles de confiance dans des circonstances générales (non gaussiennes) sont les inégalités de concentration. Une inégalité de concentration pour une variable aléatoire intégrable \\(X\\) consiste à borner \\(\\mathbb{P}(|X - \\mathbb{E}[X]|&gt;x)\\) par quelque chose de petit quand x est grand : on cherche à contrôler la probabilité pour que les réalisations de la variable aléatoire \\(X\\) soient éloignées de leur valeur moyenne \\(\\mathbb{E}[X]\\) de plus de \\(x\\).\n\nThéorème 5.3 Soit \\(X\\) une variable aléatoire de carré intégrable. Alors, \\[ \\mathbb{P}(|X - \\mathbb{E}[X]|\\geqslant x)\\leqslant \\frac{\\mathrm{Var}(X)}{x^2}.\\]\n\n\nPreuve. Élever au carré les deux membres de l’inégalité, puis appliquer l’inégalité de Markov à la variable aléatoire positive \\(|X - \\mathbb{E}X|^2\\) dont l’espérance est \\(\\mathrm{Var}(X)\\).\n\n\nThéorème 5.4 (Inégalité de Hoeffding) Soient \\(X_1, \\dotsc, X_n\\) des variables aléatoires indépendantes, pas forcément de même loi. On suppose que chaque \\(X_i\\) est à valeurs dans un intervalle borné \\([a_i, b_i]\\) et on pose \\(S_n = X_1 + \\dotsc + X_n\\).\n\\[\\mathbb{P}(S_n - \\mathbb{E}[S_n] \\geqslant x) \\leqslant e^{-\\frac{2x^2}{\\sum_{i=1}^n}(b_i - a_i)^2}.\\]\n\n\nPreuve. À écrire."
  },
  {
    "objectID": "ch3_ex.html#questions",
    "href": "ch3_ex.html#questions",
    "title": "6  Exercices",
    "section": "6.1 Questions",
    "text": "6.1 Questions\n\nSoit \\(X_n\\) une variable aléatoire de loi de Student de paramètre \\(n\\). Montrer que \\(X_n\\) converge en loi vers \\(N(0,1)\\).\nConstruire un intervalle de confiance pour le paramètre d’une loi de Poisson.\nDonner un intervalle de confiance de la forme \\([A,+\\infty[\\) pour la moyenne d’un échantillon gaussien.\nMême question pour la variance dans un modèle gaussien centré.\nDans l’estimation de la moyenne \\(\\mu\\) d’un modèle gaussien où la variance \\(\\sigma^2\\) est connue, montrer que l’intervalle de confiance obtenu (Équation 4.4) est le plus grand possible de niveau \\(1-\\alpha\\).\nDémontrer le théorème Théorème 5.1 sur l’asymptotique des queues de distribution de la loi gaussienne."
  },
  {
    "objectID": "ch3_ex.html#poisson",
    "href": "ch3_ex.html#poisson",
    "title": "6  Exercices",
    "section": "6.2 Poisson",
    "text": "6.2 Poisson\nOn suppose que l’on observe \\(X_1, \\dots, X_n\\) i.i.d de loi \\(\\mathscr{P}(\\theta)\\).\n\nÉtudier \\(\\bar{X}_n\\).\nMontrer que \\(\\sqrt{\\bar{X}_n} \\underset{n \\rightarrow \\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}} \\sqrt{\\theta}\\).\nDonner deux intervalles de confiance au niveau \\(98 \\%\\) pour \\(\\sqrt{\\theta}\\), et les comparer."
  },
  {
    "objectID": "ch3_ex.html#uniforme",
    "href": "ch3_ex.html#uniforme",
    "title": "6  Exercices",
    "section": "6.3 Uniforme",
    "text": "6.3 Uniforme\nSoit \\(X_1, \\dotsc, X_n\\) un échantillon iid de loi \\(\\mathscr{U}[0,\\theta]\\). Donner un intervalle de confiance non asymptotique pour \\(\\theta\\)."
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "7  Test d’hypothèse",
    "section": "",
    "text": "Cours 3-4"
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "8  Économétrie",
    "section": "",
    "text": "Cours 5-6-7"
  },
  {
    "objectID": "ch6.html",
    "href": "ch6.html",
    "title": "9  Théorie de l’information",
    "section": "",
    "text": "Cours 8-9-10"
  },
  {
    "objectID": "ch7.html",
    "href": "ch7.html",
    "title": "10  Estimation de densité",
    "section": "",
    "text": "Cours 11-12"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Et après ?",
    "section": "",
    "text": "nasuitenasute"
  }
]