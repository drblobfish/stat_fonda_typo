# Intervalles de confiance

## Principe 

Dans un modèle statistique, l'estimation du paramètre d'intérêt $\theta$ par intervalles de confiance consiste à spécifier un intervalle calculable à partir des données, et qui contient $\theta$ avec grande probabilité : en d'autres termes, une *région de confiance* pour $\theta$. 

Pour simplifier, on supposera d'abord que $\theta$ est un paramètre réel. 

:::{#def-ic}

## intervalle de confiance

Un intervalle de confiance de niveau $1-\alpha$ est un intervalle $I = [A,B]$ dont les bornes $A,B$ sont des statistiques, et tel que pour tout $\theta$, 
$$P_\theta(\theta \in I) \geqslant 1 - \alpha.$$
Un intervalle de confiance de niveau asymptotique $1-\alpha$ est une *suite* d'intervalles $I_n = [A_n,B_n]$ dont les bornes $A_n,B_n$ sont des statistiques, et tels que pour tout $n$,
$$ P_\theta(\theta \in I_n) \geqslant 1 - \alpha.$$
:::

Le terme « niveau » désigne $1-\alpha$ ; la vocation de ce nombre est d'être proche de 1, typiquement 99%. Le nombre $\alpha$ est parfois appelé « erreur » ou « marge d'erreur » ; la vocation de ce nombre est d'être proche de zéro, typiquement 1%. 

Il n'y a rien d'autre à savoir sur les intervalles de confiance ; tout l'art de la chose consiste à savoir les construire. Commençons par trois exemples essentiels à plusieurs titres. 

## Exemples gaussiens

On dispose de variables aléatoires $X_1, \dotsc, X_n$ gaussiennes de loi $N(\mu, \sigma^2)$. On va donner des intervalles de confiance pour l'estimation des paramètres $\mu$ et $\sigma$ dans plusieurs cas de figure. Le moment est idéal pour rappeler l'existence et le calcul des *quantiles* d'une loi --- voir ci dessous. 

### Estimation de $\mu$

**Lorsque $\sigma$ est connue. **

Nous avons déjà vu que la moyenne empirique $\bar{X}_n$ est un estimateur sans biais de $\mu$. Or, nous savons aussi la loi *exacte* de $\bar{X}_n$, qui est $N(\mu, \sigma^2/n)$. Autrement dit, 
$$\frac{\sqrt{n}}{\sigma}(\bar{X}_n - \mu) \sim N(0,1).$${#eq-pivg}

Si l'on se donne une marge d'erreur $\alpha = 1\%$, alors 
$$ \mathbb{P}( (\sqrt{n}/\sigma)|\bar{X}_n - \mu| > z_{0.99}) = 1\%$$
où $z_{0.99} \approx 2.32$. Or, 
$$ \frac{\sqrt{n}}{\sigma}|\bar{X}_n - \mu| > z_{0.99}$${#eq-piv1}
est équivalent à 
$$ \bar{X}_n - \frac{z_{0.99}\sigma}{\sqrt{n}} \leqslant \mu \leqslant \bar{X}_n + \frac{z_{0.99}\sigma}{\sqrt{n}}.$${#eq-piv2}
Le passage de @eq-piv1 à @eq-piv2 est souvent appelé *pivot*. 

Nous avons donc les deux bornes de notre intervalle de confiance : 
$$ A = \bar{X}_n - \frac{z_{0.99}\sigma}{\sqrt{n}}$$
$$ B = \bar{X}_n + \frac{z_{0.99}\sigma}{\sqrt{n}} .$$
Ces deux quantités sont bien des statistiques, car $\sigma$ est connu. De plus, nous venons de montrer que $P_\mu(\mu \in [A,B]) = 99\%$. Ici, le choix de la marge d'erreur $\alpha = 1\%$ ne jouait aucun rôle particulier ; ainsi, un intervalle de confiance de niveau $1-\alpha$ pour l'estimation de $\mu$ est donné par
$$\left[\bar{X}_n - \frac{z_{1-\alpha}\sigma}{\sqrt{n}}~~;~~\bar{X}_n + \frac{z_{1-\alpha}\sigma}{\sqrt{n}} \right].$$


**Exercice.** Montrer que cet intervalle de confiance est plus grand possible. 


**Lorsque $\sigma$ est inconnue. **


Lorsque $\sigma$ n'est pas connu, les bornes $A,B$ ci-dessus ne sont pas des statistiques, car elles dépendent de $\sigma$. Heureusement, on peut estimer $\sigma$ sans biais via l'estimateur 
$$ \hat{\sigma}_n^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X}_n)^2.$$
Que se passe-t-il si, dans la statistique @eq-pivg, on remplace $\sigma$ par son estimation $\hat{\sigma}_n^2$ ? La loi n'est plus gaussienne. Il se trouve que la loi de 
$$\frac{\sqrt{n}}{\hat{\sigma}^2_n}(\bar{X}_n - \mu)$${#eq-pivst}
s'appelle [*loi de Student*](https://fr.wikipedia.org/wiki/Loi_de_Student) à $n-1$ paramètres de liberté. Cette loi de référence est bien connue ; sa densité est 
$$ \rho_k(x) = \frac{1}{Z_k}\left(\frac{1}{1+\frac{x^2}{k}}\right)^{\frac{k+1}{2}}, $$
où $Z_k$ est la constante de normalisation donnée par 
$$ Z_k = \sqrt{k\pi}\frac{\Gamma((k+1)/2)}{\Gamma(k/2)}.$$
Ses quantiles ont également été calculés avec une grande précision. On notera $t_{k,\alpha}$ le quantile symétrique de niveau $\alpha$ d'une loi de Student de paramètre $k$. Alors, 
$$ P_{\mu, \sigma^2}(|T_n|> t_{n-1,\alpha})\leqslant \alpha .$$
Par le même raisonnement que tout à l'heure, l'inégalité 
$$ \left|\frac{\sqrt{n}}{\hat{\sigma}^2_n}(\bar{X}_n - \mu)\right| > t_{n-1,\alpha}$$
est équivalente à 
$$ \bar{X}_n - \frac{t_{n-1,\alpha}\hat{\sigma}^2_n}{\sqrt{n}} \leqslant \mu \leqslant \bar{X}_n + \frac{t_{n-1,\alpha}\hat{\sigma}^2_n}{\sqrt{n}}.$$
et les deux côtés de ces inégalités sont des statistiques; en les notant $A,B$, on a bien trouvé un intervalle de confiance de niveau $\alpha$, c'est-à-dire tel que $P_{\mu,\sigma^2}(\mu \in [A,B]) = \alpha$. 

:::{#thm-icg}

## Intervalle de Student

Un intervalle de confiance de niveau $1-\alpha$ pour l'estimation de $\mu$ lorsque $\sigma$ n'est pas connue est donné par

$$\left[\bar{X}_n - \frac{t_{1-\alpha}\hat{\sigma}_n}{\sqrt{n}}~~;~~\bar{X}_n + \frac{t_{1-\alpha}\hat{\sigma}_n}{\sqrt{n}} \right].$$
:::

### Estimation de $\sigma$

Supposons maintenant qu'on désire estimer la variance $\sigma^2$. 

**Lorsque $\mu$ est connue.**

En supposant que $\mu$ est connue, l'estimateur des moments le plus naturel pour estimer $\sigma^2$ est évidemment 
$$ \tilde{\sigma}^2_n = \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2.$$
Comme les $(X_i - \mu)/\sigma$ sont des variables aléatoires gaussiennes centrées réduites, l'estimateur $\tilde{\sigma}^2_n \times (n/ \sigma^2)$ est une somme de $n$ gaussiennes standard indépendantes. La loi de cette statistique est connue : c'est une [loi du chi-deux](https://fr.wikipedia.org/wiki/Loi_du_%CF%87%C2%B2) à $n$ paramètres de liberté. Cette loi n'est pas symétrique, puisqu'elle est supportée sur $[0,\infty[$. On note souvent $k^-_{n,\alpha}$ et $k^+_{n,\alpha}$ les nombres les plus éloignées possible (ils exisent) tels que $\mathbb{P}(k^-_{n,\alpha}< \chi^2(n)<k^+_{n,\alpha}) = 1-\alpha$. Ainsi, 
$$P_{\sigma^2}(k^-_{n,\alpha}< \frac{n \tilde{\sigma}^2_n}{\sigma^2} < k^+_{n,\alpha}) = \alpha.$$
En pivotant comme dans les exemples précédents, on obtient que l'intervalle 
$$\left[\frac{n\tilde{\sigma}_n^2}{k^{+}_{n,\alpha}} ~~;~~ \frac{n\tilde{\sigma}_n^2}{k^-_{n,\alpha}} \right] $$
est un intervalle de confiance de niveau $\alpha$ pour $\sigma^2$. 

**Lorsque $\mu$ est inconnue.**

Cette fois, on utilise l'estimateur déjà évoqué plus tôt, à savoir 
$$ \hat{\sigma}_n^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X}_n)^2.$$
La loi de $(n-1)\hat{\sigma}^2_n / \sigma^2$ est encore une loi du chi-deux, mais à $n-1$ paramètres de liberté. Ainsi, le même raisonnement que ci-dessus donne l'intervalle de confiance de niveau $\alpha$ suivant : 
$$\left[\frac{(n-1)\tilde{\sigma}_n^2}{k^+_{n-1,\alpha}} ~~;~~ \frac{(n-1)\tilde{\sigma}_n^2}{k^-_{n-1,\alpha}} \right]. $$

## Exemples asymptotiques

- Estimation du paramètre $p$ dans un modèle de Bernoulli. 
- Estimation de moyenne dans un modèle non-gaussien. 
