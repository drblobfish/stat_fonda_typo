# Moindres carrés

Les modèles *linéaires* sont les modèles les plus simples dans lesquels on raisonne en termes d'*entrées* et de *sorties*. Dans ces modèles, on dispose de variables $x_i$, dites *explicatives*, et de variables $y_i$, dites *à expliquer*, et l'on suppose qu'il existe une fonction inconnue $f$ telle que 
$$y_i \approx f(x_i),$$
et que l'on voudrait estimer. Les modèles linéaires consistent à supposer que $f$ est affine. Les modèles plus complexes, commes les réseaux de neurones, placent $f$ dans des classes plus riches. L'objectif de la méthode des moindres carrés est de trouver la meilleure approximation de $f$ possible dans la classe des fonctions affines. 

## Ajustement affine en une dimension. 

 On suppose qu'il existe entre les données $x_i$ et $y_i$ une relation de la forme $y_i \approx \alpha + \beta x_i$
où $\alpha, \beta$ sont deux nombres réels. Ici, $\approx$ signifie que la relation n'est pas parfaite : peut-être par exemple que les sorties sont bien égales à $\alpha+\beta x_i$, mais que les observations $y_i$ ont été polluées par du bruit ou des erreurs. Nous verrons cela plus tard. 

Pour l'heure, nous voulons chercher les meilleurs $\alpha, \beta$ possibles. On calcule la distance entre le nuage de points $(x_i, y_i)$ et la droite d'équation $y = \alpha + \beta x$. Cette distance au carré est donnée par 
$$ L(\alpha, \beta) = \sum_{i=1}^n (y_i - \alpha - \beta x_i)^2.$$

On cherchera donc les $(\hat\alpha, \hat\beta)$ qui  minimisent cette distance. La fonction $L$ est manifestement une fonction quadratique positive, par conséquent cette fonction possède au moins un minimum local $(\hat{\alpha}, \hat{\beta})$, et ce minimiseur est un point en lequel les dérivées partielles s'annulent (*conditions de premier ordre*) : $\partial_\alpha L(\hat{\alpha}, \hat{\beta}) = 0$ et $\partial_\beta L(\hat{\alpha}, \hat{\beta})=0$. Or, 
$$\partial_\alpha L(\alpha, \beta)  = 2\sum_{i=1}^n (\alpha + \beta x_i - y_i)$$
$$ \partial_\beta L(\alpha, \beta) = 2\sum_{i=1}^n x_i(\alpha + \beta x_i - y_i).$$
En manipulant ces équations et en introduisant $\bar{x}, \bar{x}, \bar{xx}, \bar{xy}$ les moyennes empiriques respectives des $x_i, y_i, x_i^2, x_i y_i$, on voit que les conditions de premier ordre deviennent $\alpha + \beta \bar{x} - \bar{y}=0$, et $\alpha (x_1 + \dotsb + x_n) + \beta(x_1^2 + \dotsb + x_n^2) - (x_1y_1 + \dotsb + x_ny_n) = 0$, soit $\alpha \bar{x}+ \beta \overline{xx} - \overline{xy} = 0$, où $\overline{xx}$ est la moyenne des carrés des $x_i$ et $\overline{xy}$ la moyenne des $x_iy_i$. En résolvant ces équations, on trouve d'abord $\alpha$ puis $\beta$ : 
$$\beta = \frac{\overline{xy}-\bar{x}\bar{y}}{\overline{xx} - \bar{x}\bar{x}} ,~~\quad\alpha = \bar{y} - \hat{\beta}\bar{x}.$$
Le coefficient $\beta$ n'est rien d'autre que la covariance empirique des $x_i$ et des $y_i$, normalisé par la variance empirique des $x_i$. 



L'inégalité de Cauchy-Schwartz dit que $\left|\overline{xy} - \bar{x}{\bar{y}}\right| \leqslant \tilde{\sigma}_x \tilde{\sigma}_y$, où l'on a noté
$$\tilde{\sigma}_x^2 = \frac{1}{n}\sum_{i=1}^n x_i^2 - \left(\frac{1}{n}\sum_{i=1}^n x_i\right)^2$$
l'estimateur naïf de la variance. L'inégalité n'est une égalité que si $x$ et $y$ sont effectivement colinéaires, c'est-à-dire si $y_i = \hat{\alpha} + x_i \hat{\beta}$ pour tous les $i$. La qualité de l'ajustement affine est donc bien mesurée par la quantité 
$$ R^2 = \frac{\overline{xy}-\bar{x}\bar{x}}{\tilde{\sigma}_x \tilde{\sigma}_y}.$$


## Moindres carrés ordinaires


Dans le cadre général le nombre $d$ de variables explicatives est plus grand que 1. On notera $\bx = (x_1, \dotsc, x_d)$
un élément de $\mathbb{R}^d$ ; les variables explicatives seront alors $\bx_1,\dotsc, \bx_n$. Avec mes notations, ces vecteurs sont **des vecteurs lignes**^[Ils sont de dimension $(1,d)$ si on les voit comme des matrices]. 

On cherchera donc des nombres $\theta_i$ tels que $y_i$ est aussi proche que possible de $$\theta_1 \bx_{i,1} + \dotsb + \theta_d \bx_{i,d} = \bx_i \bt,  $$
oùe paramètre $\bt$, sera toujours vu **comme le vecteur colonne**^[Donc, de dimension $(d,1)$ cette fois. ] des $\theta_i$. 


**Remarque : où est passée la constante ?** *Dans l'équation ci-dessus, on a l'impression que le terme constant, qui correspondait à $\alpha$ dans l'exemple en dimension 1, a disparu. Ce n'est pas le cas : intégrer la constante au modèle revient à considérer que la variable constante égale à 1 fait partie des variables explicatives. En pratique, cela revient à poser, par exemple, $\bx_{i,1} = 1$ pour tout $i$. Ainsi, la constante correspondra toujours à $\theta_1$.* 


On pose $X$ la matrice $n\times d$ dont la $i$-ème ligne est $\bx_i$ et $Y$ le vecteur colonne des $y_i$.  La matrice dont les lignes sont composées des nombres réels $\bx_i \bt$ n'est autre que la matrice $X\bt$. De façon générale, pour n'importe quel $\theta \in \mathbb{R}^d$, la distance entre le nuage de points $(X,Y)$ et la droite d'équation $Y = X\theta$ est alors $|Y - X\theta|$. On pourrait reproduire la méthode analytique ci-dessus pour trouver les paramètres optimaux, à savoir 
$$\hat{\bt} = \arg \min_{\theta} |Y - X\theta|^2. $${#eq-mcoargmin}
 Cependant, une interprétation géométrique simplifie la tâche : le $\hat{\bt}$ qui minimise @eq-mcoargmin est précisément celui qui garantit que $X\hat{\bt}$ est la projection orthogonale de $Y$ sur le sous-espace vectoriel $\mathscr{V}_X = \{X\theta : \theta \in \mathbb{R}^d\}$. 


:::{#thm-reg}

Si $d\leqslant n$ et si $X$ est de rang $d$, alors

$$\hat{\bt} = (X^\top X)^{-1}X^\top Y.$${#eq-mco}

:::

:::{.proof}

La projection orthogonale sur le sous-espace vectoriel engendré par les colonnes d'une matrice $X$ est la matrice $X(X^\top X)^{-1}X^\top$, comme démontré dans l'appendice @sec-al. Ainsi, la projection de $Y$ sur ce sous-espace est $X(X^\top X)^{-1}X^\top Y$, et c'est aussi (par définition de l'argmin) $X \hat{\bt}$. Comme $X$ est injective en vertu du théorème du rang, on en déduit le résultat. 

:::


L'expression @eq-mco possède de nombreuses expressions alternatives. Parmi elles, on pourra noter que $\hat{\bt} - \bt$ est égal à 
$$ \left(\frac{\sum_{i=1}^n \bx_i^\top\bx_i}{n} \right)^{-1}\frac{\sum_{i=1}^n \bx_i^\top \varepsilon_i}{n}. $${#eq-mco-alt}

**Remarque générale.** Si, en dimension 1, on cherche à trouver le $\theta$ qui résout l'équation $y = x\theta$, on trouve évidemment $\theta = y/x$, c'est-à-dire qu'on *divise* $y$ par $x$, ou encore qu'on multiplie $y$ par l'inverse de $x$. En dimension supérieure, quand on veut résoudre en $\theta$ l'équation $Y = X\theta$, c'est pareil. Le problème, c'est qu'on ne sait pas forcément *inverser* $X$. La formule @eq-mco dit que même si $X$ n'est pas inversible, on peut quand même "diviser par $X$" : c'est pour cela que la matrice $(X^\top X)^{-1} X^\top$ est appelée *pseudo-inverse à gauche* de $X$ -- parfois associée au nom de Moore-Penrose. Multiplier $Y$ par $(X^\top X)^{-1}X^\top$ donne $\hat{\bt}$ : ce vecteur ne vérifie par forcément $X{\hat{\bt}} = Y$, mais parmi tous les vecteurs possibles, c'est celui qui rend $X\bt$ le plus proche possible de $Y$.


## Résidus et $R^2$

Le vecteur $\hat{Y} = X\hat{\bt}$ est appelé *vecteur des prédictions*. Le vecteur $\hat{\varepsilon} = Y-\hat{Y} = Y - X\hat{\bt}$ est appelé *vecteur des résidus*. Si ce dernier est nul ou très petit, cela veut dire que les $Y$ sont presque parfaitement des fonctions linéaires des $X$. 



:::{#def-rdeux}

Dans le cas d'une régression @eq-mco *avec constante*, le coefficient de détermination est défini par 
$$R^2 = \frac{\sum_{i=1}^n |\hat{y}_i - \bar{y}|^2}{\sum_{i=1}^n |y_i - \bar{y}|^2}. $$
C'est un nombre entre 0 et 1. 
:::

Le numérateur est la variance empirique des prédictions $\hat{y}_i$. Le dénominateur est la variance empirique des observations. Dans les deux cas, il s'agit de la norme carrée d'un vecteur ($\hat{Y}$ et $Y$) projeté sur l'espace des vecteurs de moyenne nulle. Comme $\hat{Y}$ est déjà une projection de $Y$ sur un certain sous-espace, on a forcément $|\hat{Y} - \bar{y}| \leqslant |Y - \bar{y}|$, donc le coefficient $R^2$ est toujours entre 0 et 1. 

Plus le coefficient de détermination est proche de 1, meilleure est la régression -- attention, cet indicateur possède de nombreuses limites. Comme tous les outils statistiques puissants, la régression linéaire et le coefficient R2 donnent toujours lieu à des utilisations fantastiques : la régression suivante, par exemple, possède un $R^2$ respectable. 

::: {.content-visible when-format="html"} 

![Cet ajustement affine est vraiment très très mauvais.](/images/bad_regression.png)

:::

::: {.content-visible when-format="pdf"}

![Cet ajustement affine est vraiment très très mauvais.](/images/bad_regression.png){width=70%}

:::


## Théorème de cohérence

On cherche à faire une régression linéaire de $Y$ sur deux ensembles de variables explicatives $X\in \mathbb{R}^{n,d}$ et $Z \in \mathbb{R}^{n,k}$. L'estimateur des MCO qui utilise toutes ces variables est 
$$(\hat{\bt}, \hat{\bmu}) \in \arg\min_{\bt, \bmu}|Y - X \bt - Z\bmu|^2.$${#eq-frishwaugh}


Si j'avais omis la variable explicative $Z$ et que j'avais effectué la régression de $Y$ vers $X$, j'aurais obtenu 
$$\hat{\boldsymbol \beta} = \arg \min_{\boldsymbol \beta} |Y - X{\boldsymbol \beta}|^2.$$
Les deux estimateurs $\hat{\bt}$ et $\hat{\boldsymbol \beta}$ capturent l'effet de $X$ sur $Y$ : ils devraient donc être égaux.  

*C'est faux en général*. 

Si l'on omet les variables $Z$, il faut « enlever $Z$ de $Y$ et de $X$ », c'est-à-dire projeter sur l'orthogonal de l'espace des colonnes de $Z$. On notera $Y_Z$ et $X_Z$ les projections de $Y$ et $X$ sur cet espace. 

:::{#thm-fw}

## Théorème de Frish-Waugh


L'estimateur $\hat{\bt}$ obtenu dans la régression @eq-frishwaugh de $Y$ sur $X$ et $Z$ est égal au $\hat{\bt}_X$ obtenu dans la régression de $Y_Z$ sur $X_Z$, 
$$\hat{\bt}_X = \arg \min_{\bt} |Y_Z - X_Z \bt |^2.$$

:::
