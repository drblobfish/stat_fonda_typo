# Maximum de vraisemblance

Dans cette section, on a fixé un modèle exponentiel^[On se restreindra toujours aux modèles exponentiels qui satisfont les propriétés de la section précédente. ] associé au moment $T$, et l'on dispose d'observations indépendantes $x_1, \dotsc, x_n$ distribuées selon ce modèle. La densité de chaque observation par rapport à la mesure de référence $\nu$ est donc par $e^{\langle \theta, T(x_i)\rangle} / Z(\theta)$. En particulier, la densité de l'échantillon $x=(x_1, \dotsc, x_n)$ est $p_\theta(x):=p_\theta(x_1)\dotsb p_\theta(x_n)$, c'est-à-dire 
$$ \frac{e^{\langle \theta, \sum_{i=1}^n T(x_i)\rangle}}{Z(\theta)^n}.$${#eq-expo-prod}
Cela reste un modèle exponentiel associé à la fonction de moment $(x_1, \dotsc, x_n)\to T(x_1) + \dotsc + T(x_n)$ et à la fonction de partition $Z(\theta)^n$. 




## Définition

:::{#def-emv}

L'estimateur du maximum de vraisemblance (EMV) est le paramètre pour lequel la vraisemblance des observations est maximale : 
$$\emv = \arg \max_{\theta \in \Theta} p_\theta(x).$${#eq-MV}

:::

Il n'est pas évident que ce maximum existe, ni que le minimiseur est unique. Il existe un théorème général garantissant son existence et son unicité. 

:::{#prp-maxvraisex}

Dans un modèle exponentiel identifiable dont l'espace des paramètres $\Theta \subset \mathbb{R}^p$ est un ouvert convexe, sous certaines hypothèses, l'estimateur @eq-MV existe. Dans tous les cas, s'il existe, il est unique. 

:::



Trouver le maximum d'une fonction positive $f(x)$ et trouver le maximum de son logarithme $\ln f(x)$ reviennent au même : or, il est souvent plus facile dans les modèles exponentiels de maximiser le *logarithme* de la vraisemblance $\ell(\theta)$, qui dans un modèle de la forme @eq-expo-prod s'écrit 
$$\sum_{i=1}^n \langle \theta, T(x_i)\rangle - n\ln Z(\theta). $${#eq-logv-expo-prod}

:::{.proof}

La démonstration du théorème ci-dessus repose sur des outils analytiques simples. Dans @eq-logv-expo-prod, le premier terme est une fonction linéaire. Quant au terme $\ln Z(\theta)$, sa matrice Hessienne n'est autre (@eq-hess-exp) qu'une matrice de variance, donc positive : $\ln Z(\theta)$ est donc convexe, et même *strictement* si le modèle est identifiable (@thm-iden-expo). Ainsi, @eq-logv-expo-prod est presque sûrement strictement concave. Cela suffit à assurer que le maximum, *s'il existe*, est unique. Quant à son existence, elle nécessite des hypothèses sur $\ell$ ou sur $\Theta$ et je ne vois pas l'intérêt d'en énoncer de générales : ce sera au cas par cas. Mais typiquement, on peut demander à ce que $\ell(\theta) \to -\infty$ lorsque $\theta$ tend vers le bord de $\Theta$, ce qui revient à demander que $p_\theta(x)\to 0$. 

:::

On omettra presque systématiquement le fait que la log-vraisemblance dépend des observations $x_i$, mais **il faut garder en tête que la vraisemblance et la log-vraisemblance sont des variables aléatoires car elles dépendent de l'échantillon**. Parfois, pour indiquer quand même que l'échantillon comporte $n$ éléments, on notera $\ell_n(\theta)$. 
 En règle générale, @eq-MV est donc équivalent au problème du maximum de log-vraisemblance, 
 $$ \emv = \arg \max \ell(\theta).$$

## L'EMV et les moments

L'EMV maximise la log-vraisemblance. Lorsqu'il existe et qu'il est unique, il est donc l'unique solution de $\nabla_\theta \ell(\theta) = 0$. En dérivant @eq-logv-expo-prod, cette équation s'écrit encore 
$$\frac{1}{n}\sum_{i=1}^n T(x_i) = \nabla \ln Z(\theta).$$ 
Or, nous avons vu (@thm-iden-expo) que si le modèle est identifiable, le terme de droite, noté $\varphi(\theta)$, est un difféormorphisme. Le maximum de vraisemblance vérifie donc l'équation des moments, $\varphi(\emv) = \bar{T}_n$, où $\bar{T}_n = (T(x_1) + \dotsc + T(x_n))/n$. On peut donc appliquer le théorème des moments @thm-mom. L'hypothèse selon laquelle $T$ est de carré intégrable vient directement de @prp-reg. 

:::{#thm-emv-mom}

Dans un modèle iid, l'estimateur du maximum de vraisemblance vérifie 
$$\emv = \varphi^{-1}(\bar{T}_n)$$
où $\varphi(\theta) = \nabla \ln Z(\theta)= E_\theta[T(X)]$. Par ailleurs, cet estimateur est convergent et asymptotiquement normal : $\sqrt{n}(\emv - \theta)$ converge en loi vers $N(0,I(\theta)^{-1})$ où
$$I(\theta) = \mathrm{Var}_\theta(T).$$

:::


:::{.proof}

L'application du théorème des moments ayant été justifiée plus haut, il suffit de vérifier que l'expression de la variance asymptotique coïncide avec $I(\theta)^{-1}$. Le @thm-mom dit que $\sqrt{n}(\emv - \theta)$ converge vers une gaussienne centrée de variance 
$$D\varphi(\theta)^{-1}\mathrm{Var}_\theta(T)(D\varphi(\theta)^{-1})^\top.
$$
Or, @eq-hess-exp montre que $D\varphi(\theta) = \nabla^2 \ln Z(\theta)$ vaut également $\mathrm{Var}_\theta(T)$, d'où la simplification. 

:::

Il se trouve que la matrice $\mathrm{Var}_\theta(T)$ est centrale dans la théorie des statistiques : il s'agit de la *matrice d'information de Fisher*, que nous étudierons dans la prochaine section. 

Le maximum de vraisemblance ("maximum likelihood") est un bon prétendant au titre de concept le plus important des statistiques modernes. Son histoire est intéressante : [voir cet article](https://arxiv.org/abs/0804.2996).


## Problème d'optimisation

Dans les modèles exponentiels usuels où les paramètres ont peu de dimensions, il est aisé de maximiser la vraisemblance en résolvant l'équation $\nabla \ell(\theta)=0$ par des méthodes analytiques simples. Mais hors du giron des modèles classiques, on n'utilise presque jamais la formulation abstraite de @thm-emv-mom. La raison principale est que, *même dans les modèles exponentiels*, la fonction de partition $Z(\theta)$ peut être très difficile à inverser -- et parfois n'est même pas connue. Par exemple, un choix aussi simple que
$$T(x) = -\begin{bmatrix}x^2 \\ x^4 \end{bmatrix}$$
donne naissance à $Z(\theta) = \int e^{-\theta_1 x^2 - \theta_2 x^4}dx$ dont [la formule exacte](https://math.stackexchange.com/questions/471496/exponential-of-a-quartic) qui s'exprime via des fonctions hypergéométriques. Même si l'on accède à $\nabla \ln Z(\theta)$, il faut encore savoir en calculer l'inverse ! 


Dans ces cas, on maximise directement la vraisemblance en utilisant un algorithme d'optimisation, qui fournira donc une approximation de $\emv$ : typiquement, une variante des algorithmes de montée de gradient^[Le monde de l'optimisation ayant été habitué à minimiser des fonctions, les statisticiens ont pris l'habitude d'utiliser des *descentes* de gradient pour minimiser l'opposé de la log-vraisemblance. ], dont la version la plus simple est 
$$ \theta_{t+1} - \theta_t = \eta \nabla \ell(\theta_t)$$
où $\eta$ est le *pas* de la montée de gradient. 

## Exemple d'EMV

Pour illustrer le propos, regardons l'exemple classique de l'estimation de $\mu$ dans un modèle $N(\mu, 1)$, à partir de $n$ observations indépendantes. La log-vraisemblance $\ell(\mu)$ du modèle est 
$$\sum_{i=1}^n -\frac{(x_i - \mu)^2}{2} - \frac{n}{2}\ln(2\pi).$$
Sa dérivée $\ell'(\mu)$ est égale à 
$$ \sum_{i=1}^n (x_i - \mu).$$
Le maximum de vraisemblance existe et il est unique, car le modèle est exponentiel et identifiable. Il n'y a donc qu'un seul point critique (qui vérifie $\ell'(\mu)=0$) et celui-ci est donné par 
$$ \hat{\mu}_{\mathrm{emv}} = \frac{\sum_{i=1}^n x_i}{n} = \bar{x}_n.$$
Sans surprise, l'EMV est donc bien la moyenne empirique. 



## Tests fondés sur l'EMV

L'idée principale de l'EMV (maximiser la vraisemblance) est utilisable pour effectuer des tests. Typiquement, on peut vouloir tester l'hypothèse nulle 
$$ H_0 : \theta \in \Theta_0$$
contre l'hypothèse alternative 
$$ H_1 : \theta \in \Theta_1$$
où $\Theta_0,\Theta_1$ sont deux régions distinctes de l'espace des paramètres $\Theta$. Dans ces cas, nous pouvons définir deux maximums de vraisemblance, un par hypothèse : par exemple, 
$$ L_0 =  \sup_{\theta \in \Theta_0} p_\theta(x).$$

Dans le cas où les régions $\Theta_0,\Theta_1$ sont constituées d'un seul élément, disons $\theta_0,\theta_1$, ces maximums de vraisemblance sont simplement $p_{\theta_0}(x)$ et $p_{\theta_1}(x)$. Dans tous les cas, on peut associer à chaque hypothèse un EMV, par exemple 
$$ \hat{\theta}_0 = \arg \max_{\theta \in \Theta_0} p_\theta(x),$$
qui s'il existe vérifie $L_0 = p_{\hat{\theta}_0}(x)$. 


:::{#def-testRVgen}

Les *tests du rapport de vraisemblance* pour les hypothèses qui ne sont pas forcément simples sont les tests dont la région de rejet est de la forme 
$$ \left\lbrace\frac{\sup_{\theta \in \Theta_1}p_\theta(X)}{\sup_{\theta\in\Theta_0}p_\theta(X)} > z \right\rbrace. $$ 
:::

Lorsque les EMV pour chaque hypothèse existent, cette région de rejet s'écrit donc également 
$$ \left\lbrace \frac{p_{\hat{\theta}_0}(X)}{p_{\hat{\theta}_1}(X)} > z \right\rbrace. $$
Malheureusement, il n'y a pas d'équivalent du théorème de Neyman-Pearson (@thm-affinity, @thm-jerzy) lorsque les hypothèses ne sont pas simples. 

## Limitations de l'EMV

L'estimation par maximum de vraisemblance, par sa portée théorique autant que pratique, est une référence difficilement contournable. Au vu de son importance, il est de bon aloi d'en cerner les limites.

1. Si la loi $P$ qui a généré les observations n'appartient pas au modèle, l'estimateur n'a aucune chance d'être convergent, même s'il constitue quand même la meilleure estimation possible *dans ce modèle*. Le choix du modèle statistique reste donc un problème fondamental. 

2. L'apparente optimalité (au sens de Cramér-Rao, que l'on verra dans le chapitre suivant) de l'EMV n'est qu'asymptotique. À distance finie, il peut y avoir des estimateurs biaisés ayant un meilleur risque quadratique. Pire, dans des modèles exponentiels élémentaires comme le modèle gaussien $N(\mu, I_p)$ où l'on cherche à estimer une moyenne $\mu \in \mathbb{R}^p$ à partir d'une réalisation, il existe un estimateur dont le risque quadratique est strictement meilleur que l'EMV **quel que soit $\mu$** : c'est le [paradoxe de James-Stein](https://fr.wikipedia.org/wiki/Paradoxe_de_Stein) sur lequel nous reviendrons peut-être. 

3. Tous les modèles ne sont pas exponentiels. Même si l'estimation par maximum de vraisemblance reste pertinente en général, elle peut aussi donner des résultats peu cohérents, surtout lorsqu'elle est utilisée pour faire des tests (voir par exemple @exr-testunif). 

4. Enfin, même dans les modèles exponentiels, la fonction de partition $Z(\theta)$ peut être inaccessible, en particulier lorsque la dimension de $\theta$ est grande comme en deep learning. L'estimation par maximum de vraisemblance sera alors quasiment infaisable. 



