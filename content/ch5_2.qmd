# Tests linéaires

Les modèles linéaires sont si riches, si puissants, et si fréquemment utilisés dans toutes les sciences quantitatives, que la question de *tester* si les paramètres estimés sont pertinents est rapidement devenue une discipline en elle-même, appelée *économétrie*.  

## Significativité d'un coefficient

Dans une régression de la forme $y_i = \theta_1 \bx_{i,1}+ \dotsb + \theta_d \bx_{i,d}$, si le $j$-ème coefficient $\theta_j$ est nul, alors cela veut dire que la $j$-ème variable explicative n'a *aucun effet* sur la variable expliquée : en effet, $\bx_{i,j}$ pourrait avoir une toute autre valeur sans modifier la sortie $y_i$. Pour cette raison, le test d'une hypothèse de type $\theta_j = 0$ s'appelle *test de significativité*. 

Dans un modèle gaussien comme @sec-mlg, nous savons que $\hat{\bt}\sim N(\bt, \sigma^2(X^\top X)^{-1})$. Notons $\ell_j^2$ le $j$-ème coefficient diagonal de la matrice $(X^\top X)^{-1}$ ; ce nombre est fréquemment appelé *levier*. Il est explicitement calculable, car il ne dépend que des données d'entrée $\bx_t$ ; de plus, $\hat{\theta}_j \sim N(\theta_j, \sigma^2 \ell_{j}^2)$, et l'on en déduit (comme dans un test de Student) que sous l'hypothèse nulle $\theta_j=0$, la statistique 
$$\frac{ \hat{\theta}_j}{\ell_j \hat{\sigma}_n} $$
suit une loi de Student $\mathscr{T}(n-d)$. Il est fréquent d'utiliser la notation 
$$ \hat{\sigma}(\hat{\theta}_j) = \ell_j \hat{\sigma}_n$$
car c'est un estimateur de la variance de $\hat{\theta}_j$. 

:::{#thm-teststudent}

Soit $t_{n-d, 1-\alpha}$ le quantile symétrique d'ordre $1-\alpha$ de la loi $\mathscr{T}(n-d)$. Dans un modèle gaussien, le test ayant pour région de rejet
$$\left\lbrace  \frac{|\hat{\theta_j}|}{\hat{\sigma}(\hat{\theta}_j)}> t_{n-d, 1-\alpha}\right\rbrace$$
est un test de significativité de $\theta_j$ au niveau $1-\alpha$. 

Lorsque le modèle n'est pas gaussien mais vérifie les conditions de @sec-mlgen, ce test est asymptotiquement de niveau $1-\alpha$. 
:::

La statistique $|\hat{\theta}_j|/\hat{\sigma}(\hat{\theta}_j)$ qui apparaît ci-dessus est appelée *$\mathsf{t}$ de Student*. Les outils usuels de statistique donnent fréquemment la valeur de cette statistique pour chaque coefficient d'une régression, ainsi que la $p$-valeur du test qui est égale à 
$$1-F_{n-d}(\mathsf{t}), $$
où $F_{n-d}$ est la fonction de répartition d'une loi $\mathscr{T}(n-d)$. Cette quantité est fréquemment notée `Prob>t`. 



## Test de contraintes linéaires

Les tests de contraintes linéaires consistent à tester si $\bt$ vérifie une équation linéaire. Le test de significativité est un test de contrainte linéaire : en notant $\delta_j$ le vecteur avec des zéro partout sauf en $j$, il s'agit du test de $\langle \delta_j, \bt\rangle = 0$. On pourrait cependant vouloir tester beaucoup d'autres contraintes de ce type : par exemple, savoir si l'influence de la variable $i$ et de la variable $j$ sont identiques se traduit par $\bt_i = \bt_j$, ou encore $\langle \delta_i - \delta_j, \bt\rangle = 0$. 

Formellement, un test de contrainte linéaire consiste à tester si $\bt$ vérifie l'identité
$$C\bt = c$$
où $C$ une matrice $r \times d$ et $c$ un vecteur de taille $r$. Comme $C$ possède $r$ lignes, cela signifie que l'on teste les $r$ contraintes $\langle C_i, \bt\rangle = c_i$, où $C_i$ est la $i$-ème ligne de $C$. 

Sous cette hypothèse nulle, 
$$C\hat{\bt}-c \sim N(0, \sigma^2 C(X^\top X)^{-1}C^\top).$$
En multipliant par la matrice $[\sigma^{-2}C(X^\top X)^{-1}C^\top]^{-1/2}$ puis en prenant la norme au carré et en simplifiant, on voit que l'expression
$$ \frac{1}{\sigma^2}\langle C\hat{\bt}-c, [C(X^\top X)^{-1}C^\top]^{-1} (C\hat{\bt}-c)\rangle$$
suit une loi $\chi_2(r)$. 
Maintenant, si l'on estime le terme $\sigma^2$ comme d'habitude et que l'on utilise le théorème @thm-glm, on obtient la loi de la statistique de test (une loi de Fisher), résumée dans le théorème suivant. 

:::{#thm-Wald}

## Test de contraintes linéaires

Sous l'hypothèse nulle $C\bt = c$, on a 
$$ \frac{\langle C\hat{\bt}-c, [C(X^\top X)^{-1}C^\top]^{-1} (C\hat{\bt}-c)\rangle /r}{\hat{\sigma}_n^2} \sim \mathscr{F}_{r, n-d}.$${#eq-wald}

:::

La statistique @eq-wald est appelée *statistique de Wald* associée au système linéaire $C\bt = c$. Formellement, la région de rejet du test de niveau $1-\alpha$ de ce l'hypothèse nulle $C\bt = c$ est donc donnée par 
$$\left\lbrace \frac{\langle C\hat{\bt}-c, [C(X^\top X)^{-1}C^\top]^{-1} (C\hat{\bt}-c)\rangle /r}{\hat{\sigma}_n^2}  > f \right\rbrace$$
où $f$ est le quantile d'ordre $1-\alpha$ de la loi $\mathscr{F}_{r,n-d}$. 

## Test de significativité globale de Fisher

La significativité *globale* de la régression consiste à tester si tous les coefficients sont significatifs, sauf la constante. Il s'agit donc du test de l'hypothèse nulle
$$ \theta_2 = \dotsb = \theta_d = 0.$$
Il s'agit d'un test de contraintes linéaires au sens du paragraphe précédent : il y a $d-1$ contraintes linéaires. En notant $C$ la matrice de taille $(d-1, d)$
$$C = \begin{pmatrix}0 & 1 && 0 \\ \vdots \\ 0 & \dots && 1 \end{pmatrix}, $$
on teste bien $C\bt = 0$. Dans la statistique de Wald associée à cette contrainte, la matrice $C(X^\top X)^{-1}C^\top$ est le bloc $B_X$ obtenu à partir de $(X^\top X)^{-1}$ en enlevant la première ligne et la première colonne (qui correspondent à la constante). Il se trouve que ce test possède une expression plus simple.



:::{#thm-fisher-sig}

$$\frac{R^2}{1-R^2}\frac{n-d}{d-1} \sim \mathscr{F}_{d-1, n-d}. $$

:::

:::{.proof}


Il existe une formule qui permet d'inverser des matrices par blocs : la formule de Schur (@thm-schur). Commençons par décomposer $X$ sous la forme[^1] $[\mathbf{1}, X_0]$, avec $X_0$ de taille $n \times (d-1)$ et $\mathbf{1}$ le vecteur constant égal à 1. La matrice $X^\top X$ vaut
$$\begin{bmatrix} n & u \\ v & X_0^\top X_0 \end{bmatrix}$$
où $u$ est le vecteur ligne $\mathbf{1}^\top X_0$ et $v$ est le vecteur colonne $X_0^\top \mathbf{1}$. La formule de Schur dit que l'inverse de cette matrice est
$$\begin{bmatrix}* & * \\ * & (X_0^\top X_0 - vu/n)^{-1}\end{bmatrix} $$
La matrice $uv/n$ s'écrit $X_0^\top E X_0$, où $E = \mathbf{1}\mathbf{1}^\top / n$ est la projection orthogonale sur le sous-espace vectoriel engendré par $\mathbf{1}$. Comme $(I-E)^2=I-E$, on peut écrire $(X_0^\top X_0 - vu/n)^{-1}$ sous la forme $X_1^\top X_1$, où $X_1 = (I-E)X_0$. La statistique de Wald s'écrit alors
$$\frac{\langle \hat{\bt}_1, X_1^\top X_1 \hat{\bt}_1\rangle}{(d-1)\hat{\sigma}^2_n} $${#eq-statfisher2}
où $\hat{\bt}_1 = C\hat{\bt}$ est composé des entrées de $\hat{\bt}$ sauf la première. On reconnaît, en haut, la norme $|X_1 \hat{\bt}_1|^2$. 


Il suffit de montrer que l'expression dans @eq-statfisher2 est égale à $(n-d)R^2/(d-1)(1-R^2)$. On rappelle que $R^2 = 1 - |\hat{Y} - Y|^2 / |\bar{y}\mathbf{1} - Y|^2$. L'expression $R^2/(1-R^2)$ devrait donc valoir 
$$\frac{|\bar{y}\mathbf{1} - Y|^2 - |\hat{Y} - Y|^2}{|\hat{Y} - Y|^2}.$$ 
Au vu de @eq-statfisher2 et du fait que $\hat{\sigma}^2_n = |\hat{Y} - Y|^2 / (n-d)$, il suffit de montrer que $|X_1 \hat{\bt}_1|^2$ et $|\bar{y}\mathbf{1} - Y|^2 - |\hat{Y} - Y|^2$ sont égales. 

Le vecteur $\bar{y}\mathbf{1}$ est la projection orthogonale de $Y$ sur $\mathbf{1}$. Ainsi, on a 
$$ Y = \hat{\varepsilon} + \bar{y}\mathbf{1} + (\hat{Y} - \bar{y}\mathbf{1})$$
et ces trois vecteurs sont orthogonaux. 
Le théorème de Pythagore dit que $|\bar{y}\mathbf{1} - Y|^2 - |\hat{Y} - Y|^2$ est égal à $|\hat{Y} - \bar{y}\mathbf{1}|^2$. On reconnaît la projection orthogonale de $Y - \bar{y}\mathbf{1} = (I-E)Y$ sur l'espace vectoriel engendré par les colonnes de la matrice $X_1 = (I-E)X_0$. Si l'on note $\tilde{\bt}_1$ l'estimateur des MCO de la régression de $(I-E)Y$ vers $(I-E)X_0$, il suffit donc de montrer que $\tilde{\bt}_1 = \hat{\bt}_1$ pour terminer la démonstration. C'est exactement le théorème de Frish-Waugh @thm-fw.

:::





[^1]: Ne pas oublier que sauf mention contraire, nous faisons des régressions avec constantes, d'où la présence de $\mathbf{1}$ en tant que colonne de $X$. 