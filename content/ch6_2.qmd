# L'information de Fisher


## Définitions


Nous avons vu apparaître naturellement la *variance* du moment dans un modèle exponentiel, à savoir $\mathrm{Var}_\theta(T)$. Cette quantité s'appelle *information de Fisher*, parce qu'elle quantifie l'information relative au paramètre $\theta$ qui est « contenue » dans la distribution $p_\theta$. 




:::{#def-information}

## Information de Fisher dans les modèles exponentiels

La matrice d'information de Fisher $I(\theta)$ est la matrice de covariance de $T$, 

$$E_\theta[T(X)T(X)^\top] - E_\theta[T(X)]E_\theta[T(X)]^\top.$$

:::


L'information de Fisher possède de nombreuses expressions alternatives. La plus importante, outre la définition, est qu'on peut interpréter $I(\theta)$ comme la matrice de covariance du *score* du modèle.  


:::{#def-score}

## Fonction de score

Le score est la dérivée de la log-vraisemblance : 
$$ s_\theta(x) = \nabla_\theta \ln p_\theta(x).$$

:::

Dans un modèle exponentiel $p_\theta(x) = \exp(\langle \theta, T(x)\rangle - \ln Z_\theta)$, nous avons déjà vu que 
$$ s_\theta(x) =  T(x) - \nabla \ln Z(\theta).$${#eq-score-expp}
Le score dépend des observations, et donc est une variable aléatoire. En fait, @eq-grad-exp montre que l'espérance du score, $E_\theta[T(X)] - \nabla \ln Z(\theta)$, vaut précisément zéro : le score est centré. Au vu de @eq-score-expp, il est donc clair que l'information de Fisher coïncide avec la variance du score. C'est cette dernière définition qu'on retient en général, car elle n'est pas propre aux modèles exponentiels.


:::{#def-information-gen}

## Information de Fisher dans les modèles généraux

Dans un modèle statistique général $(p_\theta)_{\theta \in \Theta}$, l'information de Fisher est définie comme la variance du score :

$$ I(\theta) = \mathrm{Var}_\theta(\nabla_\theta \ln p_\theta(X)), $$

pourvu que tout soit bien défini (la densité doit être dérivable, etc.).

:::


## Lien avec l'entropie


:::{#def-entropie}

## Entropie

L'entropie d'une loi de densité $f$ par rapport à la mesure de référence $\nu$ est donnée par 
$$\mathrm{Ent}(f) = -\int f(x)\ln f(x)\nu(dx). $$ 

:::

Nous reviendrons plus tard sur cette quantité. Dans le cas d'un modèle exponentiel, l'entropie est égale à $E_\theta[\ln p_\theta(X)]$ par définition. On peut interpréter $I(\theta)$ comme la *courbure moyenne de l'entropie*. 

:::{#prp-courbure}

$I(\theta) = - \nabla^2_\theta~ \mathrm{Ent}(p_\theta).$

:::

:::{.proof} 

Il suffit de dériver deux fois sous l'intégrale et d'utiliser @eq-hess-exp. 

:::


## Borne de Cramér-Rao



:::{#thm-cramer-rao}

## Borne de Cramér-Rao

Pour tout estimateur sans biais $\hat{\theta}$ de $\theta$, on a^[Rappelons que (cf @sec-defpos) lorsque $A,B$ sont des matrices symétriques, $A \preceq B$ équivaut à ce que $\langle y, Ay\rangle \leqslant \langle y, By\rangle$ pour tout $y$. ] $I(\theta)^{-1} \preceq \mathrm{Cov}_\theta(\hat{\theta})$. 

:::



Lorsque le paramètre $\theta$ est réel, la borne de Cramér-Rao dit que le risque quadratique de n'importe quel estimateur sans biais ne peut pas être plus petit que $1/I(\theta)$. Les estimateurs sans biais qui atteignent cette borne sont appelés *efficaces*, ou asymptotiquement efficaces si leur risque quadratique converge vers cette borne.  

:::{.proof}
Commençons par la dimension 1. Comme $T$ est sans biais, $\int p_\theta(x)T(x)dx=\theta$. Comme $\nabla p_\theta = p_\theta \nabla_\theta \ln p_\theta = p_\theta s_\theta$, en intervertissant intégrale et dérivée, on obtient donc $1 = \int p_\theta (x)s_\theta(x)T(x)dx = E_\theta[s_\theta(X)T(X)]$. Nous avons déjà vu que le score est centré : ainsi, ce dernier terme vaut aussi $E_\theta[s_\theta(X)(T(X) - \theta)]$. L'inégalité de Cauchy-Schwarz donne alors 
$$1 \leqslant \sqrt{E_\theta[|T(X)-\theta|^2]I(\theta)}, $$
qui est le résultat voulu. Pour la dimension supérieure, il suffit d'appliquer ce résultat à $\langle y, T(X)\rangle$, qui est un estimateur sans biais de $\langle y, \theta \rangle$ (ici, $y$ est n'importe quel vecteur de $\mathbb{R}^p$). L'inégalité ci-dessus, après quelques menues manipulations, devient 
$$\langle y, I(\theta)^{-1}y\rangle \leqslant \langle y, \mathrm{Cov}_\theta(T)y\rangle, $$
qui montre bien que $I(\theta)^{-1} \preceq \mathrm{Cov}_\theta(T)$. 
:::


## Interprétation

L'information de Fisher d'un modèle dépend crucialement de *comment* on paramétrise le modèle, et c'est ce qui a donné lieu à la terrible confusion qui m'a paralysé ce matin en cours. 


Prenons l'exemple de la loi de Bernoulli, et considérons d'abord sa paramétrisation usuelle, 
$$ p_\theta(x) = \theta^x(1-\theta)^{1-x}, \quad x \in \{0,1\}$$
où le paramètre $\theta$ est dans $]0,1[$. La fonction de score est donnée par $(x\ln \theta + (1-x)\ln(1-\theta))'$, soit $\frac{x}{\theta} - \frac{1-x}{1-\theta}$, ou encore 
$$ \frac{x}{\theta(1-\theta)} + \frac{1}{1-\theta}, $$
et donc l'information de Fisher de ce modèle est égale à la variance de $X/\theta(1-\theta) + 1/(1-\theta)$, qui vaut $1/\theta(1-\theta)$ : 
$$I(\theta) = \frac{1}{\theta(1-\theta)}.$${#eq-info-ber1}

Maintenant, nous allons paramétriser le modèle de Bernoulli avec les logits : plutôt que de considérer $p$, nous allons paramétriser $z = \ln(p/(1-p))$. La densité de Bernoulli s'écrit alors
$$p_z(x) = \frac{e^{zx}}{1+e^z}.$$
Comme $x$ vaut 0 ou 1, on voit que la probabilité d'obtenir 1 est égale à $\theta = \theta(z) = e^z/(1+e^z)$. Il s'agit donc d'une autre paramétrisation de $p$, mais celle-ci a l'avantage d'exprimer les lois de Bernoulli sous forme exponentielle, avec $T(x) = x$. L'information de Fisher est donc donnée par la formule 
$$ \mathrm{Var}_z(X) = \frac{e^z}{(1+e^z)^2} = \theta(1-\theta).$${#eq-info-ber2}
Les formules @eq-info-ber1 et @eq-info-ber2 (qui sont bien celles que j'ai obtenues en cours ce matin) sont toutes les deux correctes, mais elles ne coïncident pas. Pourquoi ?


La réponse est que $I(\theta)$ et $I(z)$ quantifient la quantité d'information **sur le paramètre** qui et contenue dans les observations, et que le paramètre n'est pas le même dans les deux cas : une variation infime de $\theta$ n'a pas la même influence sur $\mathrm{Ber}(\theta)$ qu'une variation infime de $z$ !

- pour le modèle @eq-info-ber1, le paramètre d'intérêt est $\theta$. Dans ce cas, l'information contenue dans les observations est maximale lorsque $\theta$ est proche de 0 ou 1.  
- pour le modèle @eq-info-ber2, le paramètre d'intérêt est $z$. Dans ce cas, c'est le contraire : on voit que l'information contenue dans les observations est maximale lorsque $z$ est proche de 0, ce qui correspond à $\theta = 1/2$.

Pour bien comprendre, imaginons la situation où $z$ est supposé être extrêmement grand, disons près de $1000$. Dans ce cas, $\theta$ sera quasiment indiscernable de 1. Nos observations devraient être toutes égales à 1. On se doutera donc que $z$ est tel que $\theta(z)$ est proche de 1, mais il sera impossible de trancher si $z$ est égal à 1000, 2000 ou 10000000. En revanche, si on s'intéresse à $\theta$ et pas à $p$, alors on pourra dire que $\theta$ est proche de 1 sans ambiguïté supplémentaire. 

Autrement dit, la paramétrisation a radicalement changé l'information, parce que le paramètre dans chacun des deux modèles n'est pas le même et ne représente pas la même chose. 