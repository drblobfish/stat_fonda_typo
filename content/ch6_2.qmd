# L'information de Fisher


## Définitions


Nous avons vu apparaître naturellement la *variance* du moment dans un modèle exponentiel, à savoir $\mathrm{Var}_\theta(T)$. Cette quantité s'appelle *information de Fisher*, parce qu'elle quantifie l'information relative au paramètre $\theta$ qui est « contenue » dans la distribution $p_\theta$. 




:::{#def-information}

## Information de Fisher

La matrice d'information de Fisher $I(\theta)$ est la matrice de covariance de $T$, 

$$E_\theta[T(X)T(X)^\top] - E_\theta[T(X)]E_\theta[T(X)]^\top.$$

:::


L'information de Fisher possède de nombreuses expressions alternatives. La plus importante, outre la définition, est qu'on peut interpréter $I(\theta)$ comme la matrice de covariance du *score* du modèle.  


:::{#def-score}

## Fonction de score

Le score est la dérivée de la log-vraisemblance : 
$$ s_\theta(x) = \nabla_\theta \ln p_\theta(x).$$

:::

Dans un modèle exponentiel $p_\theta(x) = \exp(\langle \theta, T(x)\rangle - \ln Z_\theta)$, nous avons déjà vu que 
$$ s_\theta(x) =  T(x) - \nabla \ln Z(\theta).$${#eq-score-expp}
Le score dépend des observations, et donc est une variable aléatoire. En fait, @eq-grad-exp montre que l'espérance du score, $E_\theta[T(X)] - \nabla \ln Z(\theta)$, vaut précisément zéro : le score est centré. Au vu de @eq-score-expp, el est donc clair que 
$$ I(\theta) = \mathrm{Var}_\theta(s_\theta(X)).$$

## Lien avec l'entropie


:::{#def-entropie}

## Entropie

L'entropie d'une loi de densité $f$ par rapport à la mesure de référence $\nu$ est donnée par 
$$\mathrm{Ent}(f) = -\int f(x)\ln f(x)\nu(dx). $$ 

:::

Nous reviendrons plus tard sur cette quantité. Dans le cas d'un modèle exponentiel, l'entropie est égale à $E_\theta[\ln p_\theta(X)]$ par définition. On peut interpréter $I(\theta)$ comme la *courbure moyenne de l'entropie*. 

:::{#prp-courbure}

$I(\theta) = - \nabla^2_\theta~ \mathrm{Ent}(p_\theta).$

:::

:::{.proof} 

Il suffit de dériver deux fois sous l'intégrale et d'utiliser @eq-hess-exp. 

:::


## Borne de Cramér-Rao



:::{#thm-cramer-rao}

## Borne de Cramér-Rao

Pour tout estimateur sans biais $\hat{\theta}$ de $\theta$, on a^[Rappelons que (cf @sec-defpos) lorsque $A,B$ sont des matrices symétriques, $A \preceq B$ équivaut à ce que $\langle y, Ay\rangle \leqslant \langle y, By\rangle$ pour tout $y$. ] $I(\theta)^{-1} \preceq \mathrm{Cov}_\theta(\hat{\theta})$. 

:::



Lorsque le paramètre $\theta$ est réel, la borne de Cramér-Rao dit que le risque quadratique de n'importe quel estimateur sans biais ne peut pas être plus petit que $1/I(\theta)$. Les estimateurs sans biais qui atteignent cette borne sont appelés *efficaces*, ou asymptotiquement efficaces si leur risque quadratique converge vers cette borne.  

:::{.proof}
Commençons par la dimension 1. Comme $T$ est sans biais, $\int p_\theta(x)T(x)dx=\theta$. Comme $\nabla p_\theta = p_\theta \nabla_\theta \ln p_\theta = p_\theta s_\theta$, en intervertissant intégrale et dérivée, on obtient donc $1 = \int p_\theta (x)s_\theta(x)T(x)dx = E_\theta[s_\theta(X)T(X)]$. Nous avons déjà vu que le score est centré : ainsi, ce dernier terme vaut aussi $E_\theta[s_\theta(X)(T(X) - \theta)]$. L'inégalité de Cauchy-Schwarz donne alors 
$$1 \leqslant \sqrt{E_\theta[|T(X)-\theta|^2]I(\theta)}, $$
qui est le résultat voulu. Pour la dimension supérieure, il suffit d'appliquer ce résultat à $\langle y, T(X)\rangle$, qui est un estimateur sans biais de $\langle y, \theta \rangle$ (ici, $y$ est n'importe quel vecteur de $\mathbb{R}^p$). L'inégalité ci-dessus, après quelques menues manipulations, devient 
$$\langle y, I(\theta)^{-1}y\rangle \leqslant \langle y, \mathrm{Cov}_\theta(T)y\rangle, $$
qui montre bien que $I(\theta)^{-1} \preceq \mathrm{Cov}_\theta(T)$. 
:::

## Tests fondés sur l'EMV

L'idée principale de l'EMV (maximiser la vraisemblance) est utilisable pour effectuer des tests. Typiquement, on peut vouloir tester l'hypothèse nulle 
$$ H_0 : \theta \in \Theta_0$$
contre l'hypothèse alternative 
$$ H_1 : \theta \in \Theta_1$$
où $\Theta_0,\Theta_1$ sont deux régions distinctes de l'espace des paramètres $\Theta$. Dans ces cas, nous pouvons définir deux maximums de vraisemblance, un par hypothèse : par exemple, 
$$ L_0 =  \sup_{\theta \in \Theta_0} p_\theta(x).$$

Dans le cas où les régions $\Theta_0,\Theta_1$ sont constituées d'un seul élément, disons $\theta_0,\theta_1$, ces maximums de vraisemblance sont simplement $p_{\theta_0}(x)$ et $p_{\theta_1}(x)$. Dans tous les cas, on peut associer à chaque hypothèse un EMV, par exemple 
$$ \hat{\theta}_0 = \arg \max_{\theta \in \Theta_0} p_\theta(x),$$
qui s'il existe vérifie $L_0 = p_{\hat{\theta}_0}(x)$. 


:::{#def-testRVgen}

Les *tests du rapport de vraisemblance* pour les hypothèses qui ne sont pas forcément simples sont les tests dont la région de rejet est de la forme 
$$ \left\lbrace\frac{\sup_{\theta \in \Theta_1}p_\theta(X)}{\sup_{\theta\in\Theta_0}p_\theta(X)} > z \right\rbrace. $$ 
:::

Lorsque les EMV pour chaque hypothèse existent, cette région de rejet s'écrit donc également 
$$ \left\lbrace \frac{p_{\hat{\theta}_0}(X)}{p_{\hat{\theta}_1}(X)} > z \right\rbrace. $$
Malheureusement, il n'y a pas d'équivalent du théorème de Neyman-Pearson (@thm-affinity, @thm-jerzy) lorsque les hypothèses ne sont pas simples. 



## Limitations

L'estimation par maximum de vraisemblance, par sa portée théorique autant que pratique, est une référence difficilement contournable. Au vu de son importance, il est de bon aloi d'en cerner les limites.

1. Si la loi $P$ qui a généré les observations n'appartient pas au modèle, l'estimateur n'a aucune chance d'être convergent, même s'il constitue quand même la meilleure estimation possible *dans ce modèle*. Le choix du modèle statistique reste donc un problème fondamental. 

2. L'apparente optimalité (au sens de Cramér-Rao) de l'EMV n'est qu'asymptotique. À distance finie, il peut y avoir des estimateurs biaisés ayant un meilleur risque quadratique. Pire, dans des modèles exponentiels élémentaires comme le modèle gaussien $N(\mu, I_p)$ où l'on cherche à estimer une moyenne $\mu \in \mathbb{R}^p$ à partir d'une réalisation, il existe un estimateur dont le risque quadratique est strictement meilleur que l'EMV **quel que soit $\mu$** : c'est le [paradoxe de James-Stein](https://fr.wikipedia.org/wiki/Paradoxe_de_Stein) sur lequel nous reviendrons peut-être. 

3. Tous les modèles ne sont pas exponentiels. Même si l'estimation par maximum de vraisemblance reste pertinente en général, elle peut aussi donner des résultats peu cohérents, surtout lorsqu'elle est utilisée pour faire des tests (voir par exemple @exr-testunif). 

4. Enfin, même dans les modèles exponentiels, la fonction de partition $Z(\theta)$ peut être inaccessible, en particulier lorsque la dimension de $\theta$ est grande comme en deep learning. L'estimation par maximum de vraisemblance sera alors quasiment infaisable. 


