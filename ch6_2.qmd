# L'information de Fisher

## Définitions


:::{#def-score}

## Fonction de score

Le score est la dérivée de la log-vraisemblance : 
$$ s_\theta(x) = \nabla_\theta \ln p_\theta(x).$$

:::

Dans un modèle exponentiel $p_\theta(x) = \exp(\langle \theta, T(x)\rangle - \ln Z_\theta)$, on voit immédiatement que 
$$ s_\theta(x) =  T(x).$$
Le score dépend des observations, et donc est une variable aléatoire. On peut calculer son espérance par rapport à $p_\theta$. Il s'avère que le score est centré : 
$$E_\theta[s_\theta(X)]=0.$$

:::{.proof}

L'espérance du score est $\int p_\theta(x) \nabla_\theta \ln p_\theta(x) \nu(dx)$, c'est-à-dire $\int \nabla_\theta p_\theta(x)\nu(dx)$. Si l'on peut intervertir l'intégrale et la dérivée, on obtient $\nabla_\theta \int p_\theta d\nu$, qui est le gradient de la fonction constante égale à 1 (puisque $p_\theta$ est une densité de probabilité par rapport à $\nu$) : c'est donc zéro. 
:::



:::{#def-information}

## Information de Fisher

L'information de Fisher est la matrice de covariance du score : 

$$I(\theta) = E_\theta[s_\theta(X) s_\theta(X)^\top].$$

:::

L'information de Fisher possède de nombreuses expressions alternatives. Parmi elles, la suivante permet d'interpréter $I(\theta)$ comme la *courbure moyenne de l'entropie*. On note $\nabla^2_\theta f$ la matrice hessienne^[On rappelle que la Hessienne d'une fonction $f:\mathbb{R}^d \to \mathbb{R}$ est la matrice de ses dérivées secondes $$\frac{\partial^2}{\partial x_i \partial x_j}f(x_1, \dotsc, x_d)$$] d'une fonction $f$. 

:::{#prp-courbure}

$I(\theta) = - \nabla^2_\theta E_\theta[ \ln p_\theta(X)].$

:::

:::{.proof}

L'intégrale en question est égale à $\int p_\theta \nabla_\theta^2\ln p_\theta$. Les règles de différentiation donnent 
$$\nabla^2_\theta \ln p_\theta = \frac{p_\theta\nabla^2_\theta p_\theta}{p_\theta^2} -  \frac{\nabla_\theta p_\theta (\nabla_\theta p_\theta)^\top}{p_\theta^2}$$
c'est-à-dire $\nabla^2_\theta p_\theta / p_\theta - s_\theta s_\theta^\top$. L'espérance de ceci par rapport à $p_\theta$ est donc égale à $\int \nabla^2_\theta p_\theta - I(\theta)$. Quant au premier terme, en intervertissant la dérivée seconde et l'intégrale, on voit qu'il est égal à la hessienne de la fonction constante égale à 1, donc il est nul. 
:::

## L'EMV est asymptotiquement normal



:::{#thm-emv-an}

## L'EMV est asymptotiquement normal

$\sqrt{n}(\emv - \theta)$ converge en loi vers $N(0,I(\theta)^{-1})$. 

:::

## Borne de Cramér-Rao



:::{#thm-cramer-rao}

## Borne de Cramér-Rao

Pour tout estimateur sans biais $\hat{\theta}$ de $\theta$, on a^[Rappelons que, lorsque $A,B$ sont des matrices définies positives, $A \preceq B$ équivaut à ce que $\langle y, Ay\rangle \leqslant \langle y, By\rangle$ pour tout $y$. ] $I(\theta)^{-1} \preceq \mathrm{Cov}_\theta(\hat{\theta})$. 

:::



Lorsque le paramètre $\theta$ est réel, la borne de Cramér-Rao dit que le risque quadratique de n'importe quel estimateur sans biais ne peut pas être plus petit que $1/I(\theta)$. Les estimateurs sans biais qui atteignent cette borne sont appelés *efficaces*, ou asymptotiquement efficaces si leur risque quadratique converge vers cette borne.  

:::{.proof}
Commençons par la dimension 1. Comme $T$ est sans biais, $\int p_\theta(x)T(x)dx=\theta$. Comme $\nabla p_\theta = p_\theta \nabla_\theta \ln p_\theta = p_\theta s_\theta$, en intervertissant intégrale et dérivée, on obtient donc $1 = \int p_\theta (x)s_\theta(x)T(x)dx = E_\theta[s_\theta(X)T(X)]$. Nous avons déjà vu que le score est centré : ainsi, ce dernier terme vaut aussi $E_\theta[s_\theta(X)(T(X) - \theta)]$. L'inégalité de Cauchy-Schwarz donne alors 
$$1 \leqslant \sqrt{E_\theta[|T(X)-\theta|^2]I(\theta)}, $$
qui est le résultat voulu. Pour la dimension supérieure, il suffit d'appliquer ce résultat à $\langle y, T(X)\rangle$, qui est un estimateur sans biais de $\langle y, \theta \rangle$ (ici, $y$ est n'importe quel vecteur de $\mathbb{R}^p$). L'inégalité ci-dessus, après quelques menues manipulations, devient 
$$\langle y, I(\theta)^{-1}y\rangle \leqslant \langle y, \mathrm{Cov}_\theta(T)y\rangle, $$
qui montre bien que $I(\theta)^{-1} \preceq \mathrm{Cov}_\theta(T)$. 
:::


