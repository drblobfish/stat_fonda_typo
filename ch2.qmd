# Estimation de paramètre


La plupart des expériences/modèles statistiques que nous rencontrerons dans ce cours, seront de nature paramétrique, autrement dit indexés par des parties de $\mathbb{R}^d$. Dans de nombreux développements des statistiques, par exemple en estimation de densité, on travaille sur des modèles plus riches qui n'admettent pas de paramétrisation naturelle par une partie d'un espace euclidien de dimension finie. On parle pourtant de paramètre d'une distribution pour désigner ce qui devrait plutôt s'appeler une fonctionnelle. Par exemple, la moyenne, la covariance  d'une   distribution sur $\mathbb{R}^d$ sont des paramètres de cette distribution. Les quantiles, l'asymétrie, la kurtosis sont d'autres paramètres.

## Précision d'un estimateur

On a fixé un modèle statistique $(\mathcal{X}, \mathscr{F}, (P_\theta))$, et l'on cherche à estimer $\theta$. 



:::{#def-biais}
Le biais de $\hat{\theta}$ est la quantité $\mathbb{E}_\theta[\hat\theta - \theta]$. 
L'estimateur est dit *sans biais* s'il est de biais nul.
:::





:::{#def-rq}
Le risque quadratique de $\hat\theta$ est la quantité $\mathbb{E}_{\theta}[ |\hat{\theta}- \theta|^2]$. 
:::

En pratique, on peut vouloir estimer non pas $\theta$ lui-même, mais un paramètre $\psi = \psi_\theta$ qui dépend de $\theta$, comme $\cos(\theta)$ ou $|\theta|$ par exemple. Dans ce cas, si $\hat{\psi}$ est un estimateur de $\psi$ alors le biais est défini par $\mathbb{E}_\theta[\hat{\psi} - \psi_\theta]$ et le risque quadratique par $\mathbb{E}_\theta [ |\psi^2 - \psi_\theta|^2]$. 



:::{#thm-biaisvar}
$$
\mathbb{E}_{\theta} [|\hat{\theta}-\theta|^2]
= \underbrace{\operatorname{Var}_{\theta} (\hat{\theta})}_{\text{variance}} +
\underbrace{\left(\mathbb{E}_{\theta}[\hat{\theta}]-\theta \right)^2}_{\text{carré du biais}} \, .
$$
:::
La dépendance du risque  quadratique vis à vis de la taille de l'échantillon est une question importante en statistique mathématique. Elle concerne la vitesse d'estimation (pour une suite d'expériences donnée, quelles sont les meilleures vitesses envisageables, et comment les obtenir ?).

Pour introduire la notion de consistance d'une suite d'estimateurs,  nous aurons besoin des notions de convergence en probabilité et de convergence presque sûre.

:::{#def-cv}
Une suite de variables aléatoires $X_n$ à valeurs dans
$\mathbb{R}^k$ converge en probabilité vers une
variable aléatoire $X$ à valeurs dans
$\mathbb{R}^k$, vivant sur cet espace probabilisé si et seulement
si, pour tout $\epsilon >0$
$$
\lim_{n\to\infty} \mathbb{P} (| X_n -X|> \epsilon ) = 0 \, .
$$
:::

:::{#def-consistance}

## consistance d'un estimateur
Une suite d'estimateurs $(\widehat{\theta}_n)$ est consistante pour l'estimation de $\theta$ lorsque pour n'importe quel $\theta \in \Theta$, 
si $$ \forall \varepsilon>0, \qquad \lim_n     P_\theta ( | \widehat{\theta}_n-\theta| > \varepsilon ) =0 \qquad\text{(convergence en probabilité).}
$$
La suite est fortement consistante si pour n'importe quel $\theta \in \Theta$, $$
\hat{\theta}_n \to \theta \quad P_\theta-\text{p.s.} \qquad\text{(convergence presque sûre).}
$$

:::



## Normalité asymptotique

::: {#def-normasymp}

## normalité asymtotique

Soit $\theta$ un paramètre à estimer, et $\hat{\theta}_n$ une suite d'estimateurs de $\theta$. On dit que ces estimateurs sont *asymptotiquement gaussiens* (ou *normaux*) si, après les avoir renormalisés convenablement, ils convergent en loi vers une loi gaussienne. Autrement dit, s'il existe une suite $a_n$ de nombres réels tels que 
$$ a_n(\hat{\theta}_n - \theta) \xrightarrow[n\to \infty]{\text{loi}} N(0,\Sigma)$$
où $\Sigma$ est une matrice de covariance qui dépend peut-être de $\theta$ --- pour éviter les cas dégénérés, on demande à ce que $\Sigma$ soit non-nulle. 
:::


La normalité asymptotique n'est pas intéressante en elle-même, l'idée est de chercher le comportement asymptotique de la statistique recentrée pour pouvoir en déduire des garanties en terme de risque asymptotique ou d'intervalle de confiance. Le théorème central limite indique que le comportement asymptotique normal est relativement fréquent.

## Deux outils sur la normalité asymptotique

:::{#thm-slutsky}

## Lemme de Slutsky

Soit $(X_n)$ une suite de variables aléatoire qui converge en loi vers $X$ et $(Y_n)$ une suite de variables aléatoires qui converge en probabilité (ou en loi) vers une constante $c$. Alors, le *couple* $(X_n, Y_n)$ converge en loi vers $(X,c)$ ; autrement dit, pour toute fonction continue bornée $\varphi$, 
$$\mathbb{E}[\varphi(X_n, Y_n)] \to \mathbb{E}[\varphi(X,c)].$$
:::

:::{#thm-deltamethod}

## Delta-méthode

Soit $(X_n)$ une suite de variables aléatoires réelles telle que $\sqrt{n}(X_n - \alpha)$ converge en loi vers $N(0,\sigma^2)$. Pour toute fonction $g : \mathbb{R} \to \mathbb{R}$ dérivable en $\alpha$ (de dérivée non nulle en $\alpha$), on a 
$$ \sqrt{n}(g(X_n) - g(\alpha)) \xrightarrow[n \to \infty]{\text{loi}} N(0, g'(\alpha)^2 \sigma^2).$$

:::

Plus généralement, si les $X_n$ sont à valeurs dans $\mathbb{R}^d$ et que $\sqrt{n}(X_n - \alpha) \to N(0,\Sigma)$, alors pour toute fonction $g:\mathbb{R}^d \to \mathbb{R}^k$, la suite $ \sqrt{n}(g(X_n) - g(\alpha)) $ converge en loi vers $$ N(0, Dg(\alpha)\Sigma Dg(\alpha)^\top)$$
où $Dg(x)$ est la [matrice jacobienne](https://fr.wikipedia.org/wiki/Matrice_jacobienne) de $g$ en $x$. 

## La méthode des moments

Dans un modèle statistique, supposons qu'on dispose d'une statistiques intégrable $T$ (pas forcément réelle). Supposons que sa moyenne soit, non pas le paramètre $\theta$ lui-même, mais plutôt une *fonction* de $\theta$ :  
$$\mathbb{E}_\theta[T(X)] = \varphi(\theta).$$ 
C'est cette fonction $\varphi$ qu'on appelle \emph{moment}. Typiquement, 
- la moyenne d'une loi $\mathscr{E}(\theta)$ n'est pas $\theta$ mais $1/\theta$. 
- la moyenne d'une loi log-normale de paramètres $(0, \sigma^2)$ est $e^{\sigma^2/2}$. 

Alors, par la loi des grands nombres, 
$$\bar{T}_n = \frac{1}{n}\sum_{i=1}^n T(X_i) \to \varphi(\theta), $$
ce qui permet d'estimer $\varphi(\theta)$. Peut-on alors estimer $\theta$ ? Si la fonction $\varphi$ est inversible et si $\bar{T}_n$ appartient presque sûrement à son ensemble de définition, alors $\varphi^{-1}(\bar{T_n})$ est bien définie. Pour qu'en plus cette quantité converge presque sûrement vers $\theta$, il faut s'assurer que $\varphi^{-1}$ est continue. C'est par exemple le cas lorsque l'ensemble des paramètres $\Theta$ est un ouvert, et que $\varphi$ est un difféomorphisme sur son image --- une situation si fréquente qu'elle mérite son propre théorème, et si agréable qu'elle garantit que l'esimateur est asymptotiquement normal. 

:::{#thm-mom}

## Estimation par moments

Sous l'hypothèse mentionnée ci-dessus (la fonction $\varphi$ est un difféomorphisme), l'estimateur $$\hat{\theta}_n = \varphi^{-1}(\bar{T}_n)$$ est presque sûrement bien défini pour tout $n$ suffisamment grand ; il est également consistant pour l'estimation de $\theta$. En outre, si $T$ est de carré intégrable, cet estimateur est asymptotiquement normal, au sens où $\sqrt{n}(\hat{\theta}_n - \theta)$ converge en loi vers une gaussienne centrée de matrice de covariance
$$ (D\varphi(\theta))^{-1}\mathrm{Cov}_\theta(T)(D\varphi(\theta)^\top)^{-1}.$$

:::

:::{.proof}
La première partie a essentiellement été démontrée un peu plus haut. Pour la seconde, il faut d'abord remarquer que si $T$ est de carré intégrable, alors $\sqrt{n}(\bar{T}_n - \varphi(\theta))$ converge vers une loi $N(0, \mathrm{Cov}_\theta(T))$ par le TCL. Une simple application de la delta-méthode (@thm-deltamethod) donne alors le résultat, puisque la matrice jacobienne de $\varphi^{-1}$ en $\varphi(\theta)$ n'est autre que l'inverse de la matrice jacobienne de $\varphi$ en $\theta$. 

:::

:::{#exr-yule}

## loi de Yule-Simon


Une variable aléatoire $X$ suit la loi de Yule-Simon de paramètre $\rho>0$ lorsque $\mathbb{P}(X = n) = \rho B(n, 1+\rho)$, où $n\geqslant 1$ et  $B(a,b) = \int_0^1 u^{a-1}(1-u)^{b-1}du$ est la [fonction beta](https://en.wikipedia.org/wiki/Beta_function). Montrer que si $\rho>1$, alors $\mathbb{E}[X] = \rho/(\rho-1)$, puis trouver un estimateur de $\rho$ par la méthode des moments. 
:::
