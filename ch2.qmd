# Estimation de paramètre


La plupart des expériences/modèles statistiques que nous rencontrerons dans ce cours, seront de nature paramétrique, autrement dit indexés par des parties de $\mathbb{R}^d$. Dans de nombreux développements des statistiques, par exemple en estimation de densité, on travaille sur des modèles plus riches qui n'admettent pas de paramétrisation naturelle par une partie d'un espace euclidien de dimension finie. On parle pourtant de paramètre d'une distribution pour désigner ce qui devrait plutôt s'appeler une fonctionnelle. Par exemple, la moyenne, la covariance  d'une   distribution sur $\mathbb{R}^d$ sont des paramètres de cette distribution. Les quantiles, l'asymétrie, la kurtosis sont d'autres paramètres.

## Précision d'un estimateur

On a fixé un modèle statistique $(\mathcal{X}, \mathscr{F}, (P_\theta))$, et l'on cherche à estimer $\theta$. 



:::{#def-biais}
Le biais de $\hat{\theta}$ est la quantité $\mathbb{E}_\theta[\hat\theta - \theta]$. 
L'estimateur est dit *sans biais* s'il est de biais nul.
:::





:::{#def-rq}
Le risque quadratique de $\hat\theta$ est la quantité $\mathbb{E}_{\theta}[ |\hat{\theta}- \theta|^2]$. 
:::

En pratique, on peut vouloir estimer non pas $\theta$ lui-même, mais un paramètre $\psi = \psi_\theta$ qui dépend de $\theta$, comme $\cos(\theta)$ ou $|\theta|$ par exemple. Dans ce cas, si $\hat{\psi}$ est un estimateur de $\psi$ alors le biais est défini par $\mathbb{E}_\theta[\hat{\psi} - \psi_\theta]$ et le risque quadratique par $\mathbb{E}_\theta [ |\psi^2 - \psi_\theta|^2]$. 



:::{#thm-biaisvar}
$$
\mathbb{E}_{\theta} [|\hat{\theta}-\theta|^2]
= \underbrace{\operatorname{Var}_{\theta} (\hat{\theta})}_{\text{variance}} +
\underbrace{\left(\mathbb{E}_{\theta}[\hat{\theta}]-\theta \right)^2}_{\text{carré du biais}} \, .
$$
:::
La dépendance du risque  quadratique vis à vis de la taille de l'échantillon est une question importante en statistique mathématique. Elle concerne la vitesse d'estimation (pour une suite d'expériences donnée, quelles sont les meilleures vitesses envisageables, et comment les obtenir ?).

Pour introduire la notion de consistance d'une suite d'estimateurs,  nous aurons besoin des notions de convergence en probabilité et de convergence presque sûre.

:::{#def-cv}
Une suite de variables aléatoires $X_n$ à valeurs dans
$\mathbb{R}^k$ converge en probabilité vers une
variable aléatoire $X$ à valeurs dans
$\mathbb{R}^k$, vivant sur cet espace probabilisé si et seulement
si, pour tout $\epsilon >0$
$$
\lim_{n\to\infty} \mathbb{P} (| X_n -X|> \epsilon ) = 0 \, .
$$
:::

:::{#def-consistance}

## consistance d'un estimateur
Une suite d'estimateurs $(\widehat{\theta}_n)$ est consistante pour l'estimation de $\theta$ lorsque pour n'importe quel $\theta \in \Theta$, 
si $$ \forall \varepsilon>0, \qquad \lim_n     P_\theta ( | \widehat{\theta}_n-\theta| > \varepsilon ) =0 \qquad\text{(convergence en probabilité).}
$$
La suite est fortement consistante si pour n'importe quel $\theta \in \Theta$, $$
\hat{\theta}_n \to \theta \quad P_\theta-\text{p.s.} \qquad\text{(convergence presque sûre).}
$$

:::



## Normalité asymptotique

::: {#def-normasymp}

## normalité asymtotique

Soit $\theta$ un paramètre à estimer, et $\hat{\theta}_n$ une suite d'estimateurs de $\theta$. On dit que ces estimateurs sont *asymptotiquement gaussiens* (ou *normaux*) si, après les avoir renormalisés convenablement, ils convergent en loi vers une loi gaussienne. Autrement dit, s'il existe une suite $a_n$ de nombres réels tels que 
$$ a_n(\hat{\theta}_n - \theta) \xrightarrow[n\to \infty]{\text{loi}} N(0,\Sigma)$$
où $\Sigma$ est une matrice de covariance qui dépend peut-être de $\theta$ --- pour éviter les cas dégénérés, on demande à ce que $\Sigma$ soit non-nulle. 
:::


La normalité asymptotique n'est pas intéressante en elle-même, l'idée est de chercher le comportement asymptotique de la statistique recentrée pour pouvoir en déduire des garanties en terme de risque asymptotique ou d'intervalle de confiance. Le théorème central limite indique que le comportement asymptotique normal est relativement fréquent.

## Deux outils sur la normalité asymptotique

:::{#thm-slutsky}

## Lemme de Slutsky

Soit $(X_n)$ une suite de variables aléatoire qui converge en loi vers $X$ et $(Y_n)$ une suite de variables aléatoires qui converge en probabilité (ou en loi) vers une constante $c$. Alors, le *couple* $(X_n, Y_n)$ converge en loi vers $(X,c)$ ; autrement dit, pour toute fonction continue bornée $\varphi$, 
$$\mathbb{E}[\varphi(X_n, Y_n)] \to \mathbb{E}[\varphi(X,c)].$$
:::

:::{#thm-deltamethod}

## Delta-méthode

Soit $(X_n)$ une suite de variables aléatoires réelles telle que $\sqrt{n}(X_n - \alpha)$ converge en loi vers $N(0,\sigma^2)$. Pour toute fonction $g : \mathbb{R} \to \mathbb{R}$ dérivable en $\alpha$ (de dérivée non nulle en $\alpha$), on a 
$$ \sqrt{n}(g(X_n) - g(\alpha)) \xrightarrow[n \to \infty]{\text{loi}} N(0, g'(\alpha)^2 \sigma^2).$$

:::

Plus généralement, si les $X_n$ sont à valeurs dans $\mathbb{R}^d$ et que $\sqrt{n}(X_n - \alpha) \to N(0,\Sigma)$, alors pour toute fonction $g:\mathbb{R}^d \to \mathbb{R}^k$ on a 
$$ \sqrt{n}(g(X_n) - g(\alpha)) \xrightarrow[n \to \infty]{\text{loi}} N(0, Dg(\alpha)\Sigma Dg(\alpha)^\top)$$
où $Dg(x)$ est la matrice jacobienne de $g$ en $x$. 
