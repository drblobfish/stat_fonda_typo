# Maximum de vraisemblance

Dans cette section, on a fixé un modèle exponentiel^[On se restreindra toujours aux modèles exponentiels qui satisfont les propriétés de la section précédente. ] associé au moment $T$, et l'on dispose d'observations indépendantes $x_1, \dotsc, x_n$ distribuées selon ce modèle. La densité de chaque observation par rapport à la mesure de référence $\nu$ est donc par $e^{\langle \theta, T(x_i)\rangle} / Z(\theta)$. En particulier, la densité de l'échantillon $x=(x_1, \dotsc, x_n)$ est $p_\theta(x):=p_\theta(x_1)\dotsb p_\theta(x_n)$, c'est-à-dire 
$$ \frac{e^{\langle \theta, \sum_{i=1}^n T(x_i)\rangle}}{Z(\theta)^n}.$${#eq-expo-prod}
Cela reste un modèle exponentiel associé à la fonction de moment $(x_1, \dotsc, x_n)\to T(x_1) + \dotsc + T(x_n)$ et à la fonction de partition $Z(\theta)^n$. 




## Définition

:::{#def-emv}

L'estimateur du maximum de vraisemblance (EMV) est le paramètre pour lequel la vraisemblance des observations est maximale : 
$$\emv = \arg \max_{\theta \in \Theta} p_\theta(x).$${#eq-MV}

:::

Il n'est pas évident que ce maximum existe, ni que le minimiseur est unique. Il existe un théorème général garantissant son existence et son unicité. 

:::{#prp-maxvraisex}

Dans un modèle exponentiel identifiable dont l'espace des paramètres $\Theta \subset \mathbb{R}^p$ est un ouvert convexe avec $\mathrm{Leb}(\partial \Theta)=0$,  l'estimateur @eq-MV existe et il est unique. 

:::

La démonstration de ce théorème général repose sur des outils analytiques simples : sous les hypothèses ci-dessus, la fonction de log-vraisemblance $\theta \mapsto \ln p_\theta(x)$ est presque sûrement strictement concave et tend vers $0$ au bord du domaine : son maximum $\hat{\theta}$ est donc bien défini, grâce aux résultats généraux de convexité. Je ne rentrerai pas dans les détails, qui peuvent être délicats. 

Trouver le maximum d'une fonction positive $f(x)$ et trouver le maximum de son logarithme $\ln f(x)$ reviennent au même : or, il est souvent plus facile dans les modèles exponentiels de maximiser le *logarithme* de la vraisemblance $\ell(\theta)$, qui dans un modèle de la forme @eq-expo-prod s'écrit 
$$\sum_{i=1}^n \langle \theta, T(x_i)\rangle - n\ln Z(\theta). $${#eq-logv-expo-prod}
On omettra presque systématiquement le fait que la log-vraisemblance dépend des observations $x_i$, mais **il faut garder en tête que la vraisemblance et la log-vraisemblance sont des variables aléatoires car elles dépendent de l'échantillon**. Parfois, pour indiquer quand même que l'échantillon comporte $n$ éléments, on notera $\ell_n(\theta)$. 
 En règle générale, @eq-MV est donc équivalent au problème du maximum de log-vraisemblance, 
 $$ \emv = \arg \max \ell(\theta).$$

## L'EMV et les moments

L'EMV maximise la log-vraisemblance. Lorsqu'il existe et qu'il est unique, il est donc l'unique solution de $\nabla_\theta \ell(\theta) = 0$. En dérivant @eq-logv-expo-prod, cette équation s'écrit encore 
$$\frac{1}{n}\sum_{i=1}^n T(x_i) = \nabla \ln Z(\theta).$$ 
Or, nous avons vu (@thm-iden-expo) que si le modèle est identifiable, le terme de droite, noté $\varphi(\theta)$, est un difféormorphisme. Le maximum de vraisemblance vérifie donc l'équation des moments, $\varphi(\emv) = \bar{T}_n$, où $\bar{T}_n = (T(x_1) + \dotsc + T(x_n))/n$. On peut donc appliquer le théorème des moments @thm-mom. L'hypothèse selon laquelle $T$ est de carré intégrable vient directement de @prp-reg. 

:::{#thm-emv-mom}

Dans un modèle iid, l'estimateur du maximum de vraisemblance vérifie 
$$\emv = \varphi^{-1}(\bar{T}_n)$$
où $\varphi(\theta) = \nabla \ln Z(\theta)= E_\theta[T(X)]$. Par ailleurs, cet estimateur est convergent et asymptotiquement normal : $\sqrt{n}(\emv - \theta)$ converge en loi vers $N(0,I(\theta)^{-1})$ où
$$I(\theta) = \mathrm{Var}_\theta(T).$$

:::


:::{.proof}

L'application du théorème des moments ayant été justifiée plus haut, il suffit de vérifier que l'expression de la variance asymptotique coïncide avec $I(\theta)^{-1}$. Le @thm-mom dit que $\sqrt{n}(\emv - \theta)$ converge vers une gaussienne centrée de variance 
$$D\varphi(\theta)^{-1}\mathrm{Var}_\theta(T)(D\varphi(\theta)^{-1})^\top.
$$
Or, @eq-hess-exp montre que $D\varphi(\theta) = \nabla^2 \ln Z(\theta)$ vaut également $\mathrm{Var}_\theta(T)$, d'où la simplification. 

:::

Il se trouve que la matrice $\mathrm{Var}_\theta(T)$ est centrale dans la théorie des statistiques : il s'agit de la *matrice d'information de Fisher*, que nous étudierons dans la prochaine section. 


## Problème d'optimisation

Dans les modèles exponentiels usuels où les paramètres ont peu de dimensions, il est aisé de maximiser la vraisemblance en résolvant l'équation $\nabla \ell(\theta)=0$ par des méthodes analytiques simples. Mais hors du giron des modèles classiques, on n'utilise presque jamais la formulation abstraite de @thm-emv-mom. La raison principale est que, *même dans les modèles exponentiels*, la fonction de partition $Z(\theta)$ peut être très difficile à inverser -- et parfois n'est même pas connue. Par exemple, un choix aussi simple que
$$T(x) = -\begin{bmatrix}x^2 \\ x^4 \end{bmatrix}$$
donne naissance à $Z(\theta) = \int e^{-\theta_1 x^2 - \theta_2 x^4}dx$ dont [la formule exacte](https://math.stackexchange.com/questions/471496/exponential-of-a-quartic) qui s'exprime via des fonctions hypergéométriques. Même si l'on accède à $\nabla \ln Z(\theta)$, il faut encore savoir en calculer l'inverse ! 


Dans ces cas, on maximise directement la vraisemblance en utilisant un algorithme d'optimisation, qui fournira donc une approximation de $\emv$ : typiquement, une variante des algorithmes de montée de gradient^[Le monde de l'optimisation ayant été habitué à minimiser des fonctions, les statisticiens ont pris l'habitude d'utiliser des *descentes* de gradient pour minimiser l'opposé de la log-vraisemblance. ], dont la version la plus simple est 
$$ \theta_{t+1} - \theta_t = \eta \nabla \ell(\theta_t)$$
où $\eta$ est le *pas* de la montée de gradient. 

## Exemple

Pour illustrer le propos, regardons l'exemple classique de l'estimation de $\mu$ dans un modèle $N(\mu, 1)$, à partir de $n$ observations indépendantes. La log-vraisemblance $\ell(\mu)$ du modèle est 
$$\sum_{i=1}^n -\frac{(x_i - \mu)^2}{2} - \frac{n}{2}\ln(2\pi).$$
Sa dérivée $\ell'(\mu)$ est égale à 
$$ \sum_{i=1}^n (x_i - \mu).$$
Le maximum de vraisemblance existe et il est unique, car le modèle est exponentiel et identifiable. Il n'y a donc qu'un seul point critique (qui vérifie $\ell'(\mu)=0$) et celui-ci est donné par 
$$ \hat{\mu}_{\mathrm{emv}} = \frac{\sum_{i=1}^n x_i}{n} = \bar{x}_n.$$
Sans surprise, l'EMV est donc bien la moyenne empirique. 


