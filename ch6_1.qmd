# Maximum de vraisemblance

Dans cette section, on a fixé un modèle exponentiel^[On se restreindra toujours aux modèles exponentiels qui satisfont les propriétés de la section précédente. ] associé au moment $T$, et l'on dispose d'observations $x = (x_1, \dotsc, x_n)$ distribuées selon ce modèle. La densité par rapport à une mesure de référence $\nu$ est donc donnée par $e^{\langle \theta, T(x_i)\rangle} / Z(\theta)$. En particulier, la loi du *modèle global* a pour densité $p_\theta(x_1)\dotsb p_\theta(x_n)$, c'est-à-dire 
$$ p_\theta(x) = e^{\langle \theta, \sum_{i=1}^n T(x_i)\rangle}/Z(\theta)^n.$${#eq-expo-prod}
Cela reste un modèle exponentiel associé à la fonction de moment $(x_1, \dotsc, x_n)\to T(x_1) + \dotsc + T(x_n)$ et à la fonction de partition $Z(\theta)^n$. 




## Définition

:::{#def-emv}

L'estimateur du maximum de vraisemblance (EMV) est le paramètre pour lequel la vraisemblance des observations est maximale : 
$$\emv = \arg \max_{\theta \in \Theta} p_\theta(x).$${#eq-MV}

:::

Il n'est pas évident que ce maximum existe, ni que le minimeur soit unique. C'est en règle générale le cas, à certaines exceptions près -- parfois même dans des modèles exponentiels. Il existe un théorème général garantissant son existence et son unicité. 

:::{#prp-maxvraisex}

Dans un modèle exponentiel identifiable dont l'espace des paramètres est un ouvert convexe avec $\nu(\partial \Theta)=0$,  l'estimateur @eq-MV existe et il est unique. 

:::

La démonstration de ce théorème général repose sur des outils purement analytiques relativement simples : en particulier, sous les hypothèses ci-dessus, la fonction de log-vraisemblance $\theta \mapsto \ln p_\theta(x)$ est presque sûrement strictement concave et tend vers $0$ au bord du domaine : son maximum $\hat{\theta}$ est donc bien défini, grâce aux résultats généraux de convexité. 

Trouver le maximum d'une fonction positive $f(x)$ et trouver le maximum de son logarithme $\ln f(x)$ reviennent au même : or, il est souvent bien plus facile dans les modèles exponentiels de maximiser le *logarithme* de la vraisemblance (la log-vraisemblance), qui dans un modèle exponentiel de la forme @eq-expo-prod s'écrit 
$$\ell(\theta) = \sum_{i=1}^n \langle \theta, T(x_i)\rangle - n\ln Z(\theta). $${#eq-logv-expo-prod}
On omettra presque systématiquement le fait que la log-vraisemblance dépend des observations $x_i$, mais **il faut garder en tête que la vraisemblance et la log-vraisemblance sont des variables aléatoires car elles dépendent de l'échantillon**. Parfois, pour indiquer quand même que l'échantillon comporte $n$ éléments, on notera $\ell_n(\theta)$. 
 En règle générale, @eq-MV est donc équivalent au problème du maximum de log-vraisemblance, 
 $$ \emv = \arg \max \ell(\theta).$$

## L'EMV et les moments

L'EMV maximise la log-vraisemblance. Lorsqu'il existe et qu'il est unique, il est donc l'unique solution de $\nabla_\theta \ell(\theta) = 0$. En dérivant @eq-logv-expo-prod, cette équation s'écrit encore 
$$\frac{1}{n}\sum_{i=1}^n T(x_i) = \nabla \ln Z(\theta).$$ 
Or, nous avons vu (@thm-iden-expo) que si le modèle est identifiable, le terme de droite, noté $\varphi(\theta)$, est un difféormorphisme. Le maximum de vraisemblance vérifie donc l'équation des moments, $\varphi(\emv) = \bar{T}_n(x)$. Dans le cas où les observations proviennent d'un échantillon iid, on peut donc appliquer le théorème des moments @thm-mom. L'hypothèse selon laquelle $T$ est de carré intégrable vient directement de @prp-reg. 

:::{#thm-emv-mom}

Dans un modèle iid, l'estimateur du maximum de vraisemblance vérifie 
$$\emv = \varphi^{-1}(\bar{T}_n)$$
où $\varphi(\theta) = \nabla \ln Z(\theta)= E_\theta[T(X)]$. Par ailleurs, cet estimateur est convergent et asymptotiquement normal : $\sqrt{n}(\emv - \theta)$ converge en loi vers $N(0,I(\theta)^{-1})$ où
$$I(\theta) = \mathrm{Var}_\theta(T).$$

:::


:::{.proof}

L'application du théorème des moments ayant été justifiée plus haut, il suffit de vérifier que l'expression de la variance asymptotique coïncide avec $I(\theta)^{-1}$. Le @thm-mom dit que $\sqrt{n}(\emv - \theta)$ converge vers une gaussienne centrée de variance 
$$D\varphi(\theta)^{-1}\mathrm{Var}_\theta(T)(D\varphi(\theta)^{-1})^\top.
$$
Or, @eq-hess-exp montre que $D\varphi(\theta) = \nabla^2 \ln Z(\theta)$ vaut également $\mathrm{Var}_\theta(T)$, d'où la simplification. 

:::

Il se trouve que la matrice $\mathrm{Var}_\theta(T)$ est centrale dans la théorie des statistiques : il s'agit de la *matrice d'information de Fisher*, que nous étudierons dans la prochaine section. 

En pratique, on n'utilise pas la formulation abstraite de @thm-emv-mom : il est souvent plus immédiat de maximiser la vraisemblance à la main (ce qui est théoriquement équivalent). Si le modèle est exponentiel, on n'aura pas à le mettre *explicitement* sous sa forme exponentielle. 

## Exemple

Pour illustrer le propos, regardons l'exemple classique de l'estimation de $\mu$ dans un modèle $N(\mu, 1)$, à partir de $n$ observations indépendantes. La log-vraisemblance $\ell(\mu)$ du modèle est 
$$\sum_{i=1}^n -\frac{(x_i - \mu)^2}{2} - \frac{n}{2}\ln(2\pi).$$
Sa dérivée $\ell'(\mu)$ est égale à 
$$ \sum_{i=1}^n (x_i - \mu).$$
Le maximum de vraisemblance existe et il est unique, car le modèle est exponentiel et identifiable. Il n'y a donc qu'un seul point critique (qui vérifie $\ell'(\mu)=0$) et celui-ci est donné par 
$$ \hat{\mu}_{\mathrm{emv}} = \frac{\sum_{i=1}^n x_i}{n} = \bar{x}_n.$$
Sans surprise, l'EMV est donc bien la moyenne empirique. 


